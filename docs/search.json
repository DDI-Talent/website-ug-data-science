[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Health and Biomedical Sciences",
    "section": "",
    "text": "Data science is revolutionising how medicine is understood, how biomedical research is conducted and how healthcare is delivered. Despite the widely-recognised opportunities that data can bring to biomedicine and healthcare, there is a shortage of data skills in the healthcare sector. This course aims to equip honours students with the key foundations and data skills that are needed for data-driven innovation.\n\n\nBIME10076 • Autumn 2025Deanery of Biomedical SciencesCollege of Medicine and Veterinary Medicine"
  },
  {
    "objectID": "course-information/timetable.html",
    "href": "course-information/timetable.html",
    "title": "Timetable",
    "section": "",
    "text": "You can find class times for this course via your University of Edinburgh calendar (links open in a new window):\n\nView your calendar in Office 365\nInformation about Student Timetables\n\nTODO: Add this years course timetable PDF"
  },
  {
    "objectID": "course-information/index.html",
    "href": "course-information/index.html",
    "title": "Course Information",
    "section": "",
    "text": "Course Name: Data Science for Health and Biomedical Sciences\nCourse Code: BIME10076\nSCQF Credits: 20\nLink to course on DRPS: http://www.drps.ed.ac.uk/24-25/dpt/cxbime10076.htm\n\nCourse Handbook\nData Science 2024-25 Handbook_FINAL.docx\n\n\nCourse Outline\nThe course aims to provide a broad introduction to data science in health and biomedical sciences, covering key concepts and principles, data analysis skills and implications of working with biomedical and healthcare data. Key topics in the course include: types of human health data; computational methods (e.g. process modelling and machine learning); data wrangling, analysis and reporting using the R programming language; legal considerations and bias in health data. This course is delivered in a flipped classroom format: it is based around short pre-recorded videos, which are complemented with readings and self-guided programming tasks. The students also have a weekly in-person tutorial, which provides an opportunity for further improvement of their programming practice and discussion of the core concepts. We will also offer weekly quizzes to provide the students with valuable formative feedback.\nLearning Outcomes\nOn completion of this course, the student will be able to:\n\nApply a range of specialised data science techniques to different medical and healthcare scenarios.\nAnalyse health and biomedical data with the use of the R programming language, including summarisation, visualisation and interpretation.\nCritically examine the ethical, societal and regulatory principles and implications of data science in health.\nExplain and critically discuss key concepts, principles and methods of data science in health.\n\nRecommended Reading & Resources\nThere is no compulsory course text.\nHowever, we recommended:\n\nR for Health Data Science (free online) by Ewen Harrison and Riinu Pius\nR for Data Science (free online) by Hadley Wickham, Mine Cetinkaya-Rundel and Garrett Grolemund\nValue Sensitive Design by Batya Friedman and David G. Hendry (available from the Library)\n\n\n\nTimetable\nYou can find class times for this course via your University of Edinburgh calendar (links open in a new window):\n\nView your calendar in Office 365\nInformation about Student Timetables\n\nEssential Course Guide\nThis is general information that applied to all BMTO courses.\nhttps://www.ed.ac.uk/biomedical-sciences/bmto/bmto-undergraduate-students/academic-guidance-and-support/academic-guidance/student-guide/biomedical-sciences-undergraduate-student-handbook\nCourse Handbook\nYou can find the course handbook here (PDF and docx versions) -&gt; https://github.com/DDI-Students/data-science-for-health-bms-24-25/blob/main/documents/. To download it, click “Download raw file” at the top right."
  },
  {
    "objectID": "course-information/gen-ai-guidance.html",
    "href": "course-information/gen-ai-guidance.html",
    "title": "ChatGPT (generative AI) guidance",
    "section": "",
    "text": "Generative AI tools are a form of AI that can generate content based on a prompt that you give it. It’s important to know that they attempt to create content based on data they have seen before, rather than having a database of “answers”. As such, they are not entirely reliable as you’ve no doubt noticed! They can however be useful for us as programmers to act as a pair programmer and personal tutor.\nHere is some guidance to bear in mind:\n\nTry Solving Problems Yourself First\nBefore going directly to a GenAI tool, try solving the problem yourself. This will strengthen your understanding and your learning muscles. Your attempted solution will likely be different from anything GenAI produces which will enhance learning even more if you ask it for an example after you’re finished.\n\n\nAsk For Explanations, Not Just Solutions\nAsk the AI for an explanation of what it has produced rather than just take it at face value. That way, next time you come across the same issue, you will have a better chance of success. You can also ask for explanations of error messages. Sometimes error messages can be a little vague so it can help to get a fuller explanation.\n\n\nAsk For Some Practice Exercises\nIf you want to practice a specific skill, ask for 10 exercises (without the solutions!) so you can practice. Once you’ve attempted each exercise, you can ask for a solution. Remember, sometimes it gets it wrong so check the code.\n\n\nBe Precise With Your Prompt\nTo get the best results, set the context and be precise. You will get much better results if the tool understands your learning stage and your context.\nGood example: “Hi! I’m learning data science using the tidyverse. So far, I’ve learned the basics like x, y, z. Please suggest 2 options with examples for what I should learn next.”\nBad example: “How do you use tidyverse for data science?”\n\n\nDon’t Just Immediately Ask For a Solution\nAsk for some hints rather than a full solution. Tell it what you’re stuck on and the code you’ve tried so far.\n\n\nAsk For a Code Review\nUsually done by a human, code reviews can help you gain a different perspective on your code and give you skills to improve. Paste in your code and ask for a code review. You might discover alternative ways to achieve the same result.\n\n\nAcademic Integrity\nAcknowledge your use of GenAI tools in any assessments you submit. Don’t submit any code that your don’t understand or wouldn’t be able to explain to someone. Remember, this is your learning and you’d only be doing yourself a disservice.\nAcknowledgment: This document was created based on ideas from ChatGPT\nUseful Links:\n\nAs well as ChatGPT, you can access Github Copilot for free as a student: https://education.github.com/discount_requests/application. Make sure the verified email address on your Github account is your University email address.\nYou can use Copilot and ChatGPT in RStudio. Watch this video for instructions on setting it up: https://youtu.be/t7NrkAeosog?si=QdY8cxbey_LlEzgz\nUniversity’s policy on GenAI: https://www.ed.ac.uk/sites/default/files/atoms/files/universityguidanceforstudentsonworkingwithgenerativeai.pdf\nBook on how to use GenAI for learning, including coding: https://psyteachr.github.io/AITutoR/"
  },
  {
    "objectID": "content/week-9/visibility-of-what-slides.html",
    "href": "content/week-9/visibility-of-what-slides.html",
    "title": "Visibility of what? slides",
    "section": "",
    "text": "Read here the guest lecture slides",
    "crumbs": [
      "Content",
      "Week 9: Data Ownership",
      "Visibility of What?"
    ]
  },
  {
    "objectID": "content/week-9/index.html",
    "href": "content/week-9/index.html",
    "title": "Overview",
    "section": "",
    "text": "This week we will delve into more societal issues around the gathering and analysis of health data.",
    "crumbs": [
      "Content",
      "Week 9: Data Ownership",
      "Overview"
    ]
  },
  {
    "objectID": "content/week-8/readings.html",
    "href": "content/week-8/readings.html",
    "title": "Data and Identity - readings",
    "section": "",
    "text": "The collection, analysis and use of data is political. The effects of knowledge, power and history on data practices are most apparent when they intersect with the lives and experiences of people from minoritised communities, such as LGBTQ communities.\nData practices that involve counting, categorising and managing identity categories (e.g. gender, race and sexuality) are increasingly common in health/science contexts. Numerical identity data provides an evidence base for targeted interventions, community engagement exercises, recruitment and training programmes, and other diversity, equity and inclusion interventions. Our lives are increasingly shaped by how it is defined, collected and used. But who counts in the collection, analysis and application of data?\nDrawing from ideas in critical race and gender studies, sociology, history, philosophy and science and technology studies, this session examines the intersection of data practices and identity categories in health/science contexts.\nRequired Readings\nGuyan, Kevin. Queer Data: Using Gender, Sex and Sexuality Data for Action. Bloomsbury Studies in Digital Cultures. London: Bloomsbury Academic, 2022. Introduction: Data and Difference.\nEpstein, Steven. Inclusion: The Politics of Difference in Medical Research. Chicago: University of Chicago Press, 2007. Introduction: Health Research and the Remaking of Common Sense\n Chapter by Kevin Guyan (PDF) \n Chapter by Steven Epstein (PDF) \nOptional readings:\nCross, Harry, Stephen Bremner, Catherine Meads, Alex Pollard, and Carrie Llewellyn. “Bisexual People Experience Worse Health Outcomes in England: Evidence from a Cross-Sectional Survey in Primary Care.” The Journal of Sex Research, July 24, 2023, 1–9. https://doi.org/10.1080/00224499.2023.2220680.\nGuyan, Kevin. “Constructing a Queer Population? Asking about Sexual Orientation in Scotland’s 2022 Census.” Journal of Gender Studies, January 4, 2021, 1–11. https://doi.org/10.1080/09589236.2020.1866513.\nLockhart, Jeffrey W. “Because the Machine Can Discriminate: How Machine Learning Serves and Transforms Biological Explanations of Human Difference.” Big Data & Society 10, no. 1 (January 2023): 205395172311550. https://doi.org/10.1177/20539517231155060.\nRitz, Stacey A., and Greaves, Lorraine. “More nuanced approaches to exploring sex and gender are warranted.” Nature, May 2, 2024, 34-36. https://doi-org.eux.idm.oclc.org/10.1038/d41586-024-01204-3\nUrwin, Sean, Thomas Mason, and William Whittaker. “Do Different Means of Recording Sexual Orientation Affect Its Relationship with Health and Wellbeing?” Health Economics 30, no. 12 (December 2021): 3106–22. https://doi.org/10.1002/hec.4422.",
    "crumbs": [
      "Content",
      "Week 8: Data and Identity",
      "Data and Identity - readings"
    ]
  },
  {
    "objectID": "content/week-8/guest-lecture-slides.html",
    "href": "content/week-8/guest-lecture-slides.html",
    "title": "Guest lecture slides",
    "section": "",
    "text": "Read here the guest lecture slides",
    "crumbs": [
      "Content",
      "Week 8: Data and Identity",
      "Guest lecture slides"
    ]
  },
  {
    "objectID": "content/week-7/topic-3.html",
    "href": "content/week-7/topic-3.html",
    "title": "Improving Healthcare Processes",
    "section": "",
    "text": "The Scottish Approach to Service Design (SAtSD) is based on a collaborative, inclusive and ethical approach. The resources provided by the Scottish Government state that:\n\nDoing research in an ethical manner ensures that:\n\nparticipants are safe\nresearchers are safe\nresearch is valid (objectivity and integrity)\nresearch is lawful and transparent\nresearch is inclusive and respectful’\n\n\nFurther information is available from the Digital Office again with the emphasis on ethical approaches which can include conducting a Data Protection Impact Assessment (DPIA) also encouraged by the Information Commissioner’s Office. An example of their Regulatory Sandbox activity with JISC looked at student support and wellbeing.\nDuring the Covid-19 pandemic, many countries rapidly developed contact tracing apps. The NHS Scotland Test and Protect app focused on Transparency – Evidence, Ethics and Impact. An article by Dr Pagliari in the Journal of Global Health considered the ethics and value of these apps.",
    "crumbs": [
      "Content",
      "Week 7: Improving Healthcare Processes & Integrating Health Data",
      "Topic 3: Improving Healthcare Processes"
    ]
  },
  {
    "objectID": "content/week-7/topic-1.html",
    "href": "content/week-7/topic-1.html",
    "title": "Computational Methods for Improving Healthcare Processes",
    "section": "",
    "text": "What is process modelling, and how can one use process modelling methods to gain insight into care pathways, hospital practices and other important processes in healthcare? In this topic you will be introduced to some of the most popular languages for modelling healthcare processes, you will learn how simulation and verification techniques can be used to inform healthcare process improvement initiatives, and you will find out about recent developments around mining health processes from data.",
    "crumbs": [
      "Content",
      "Week 7: Improving Healthcare Processes & Integrating Health Data",
      "Topic 1: Computational Methods for Improving Healthcare Processes"
    ]
  },
  {
    "objectID": "content/week-7/topic-1.html#overview",
    "href": "content/week-7/topic-1.html#overview",
    "title": "Computational Methods for Improving Healthcare Processes",
    "section": "",
    "text": "What is process modelling, and how can one use process modelling methods to gain insight into care pathways, hospital practices and other important processes in healthcare? In this topic you will be introduced to some of the most popular languages for modelling healthcare processes, you will learn how simulation and verification techniques can be used to inform healthcare process improvement initiatives, and you will find out about recent developments around mining health processes from data.",
    "crumbs": [
      "Content",
      "Week 7: Improving Healthcare Processes & Integrating Health Data",
      "Topic 1: Computational Methods for Improving Healthcare Processes"
    ]
  },
  {
    "objectID": "content/week-7/topic-1.html#modelling-analysing-and-discovering-healthcare-processes",
    "href": "content/week-7/topic-1.html#modelling-analysing-and-discovering-healthcare-processes",
    "title": "Computational Methods for Improving Healthcare Processes",
    "section": "Modelling, Analysing and Discovering Healthcare Processes",
    "text": "Modelling, Analysing and Discovering Healthcare Processes\nWatch the following 4 videos to find out what computational methods can be used to model healthcare processes, to analyse them and to automatically discovering them based on existing data.\nhttps://media.ed.ac.uk/media/1_eirigrks\nLink to the transcript\nhttps://media.ed.ac.uk/media/1_act9hmop\nLink to the transcript\nhttps://media.ed.ac.uk/media/1_egrcbvzy\nLink to the transcript\nhttps://media.ed.ac.uk/media/1_i881krdz\nLink to the transcript",
    "crumbs": [
      "Content",
      "Week 7: Improving Healthcare Processes & Integrating Health Data",
      "Topic 1: Computational Methods for Improving Healthcare Processes"
    ]
  },
  {
    "objectID": "content/week-7/index.html",
    "href": "content/week-7/index.html",
    "title": "Overview",
    "section": "",
    "text": "This week you will be introduced to process modelling methods and you will learn about the RDF approach to integrating healthcare data.",
    "crumbs": [
      "Content",
      "Week 7: Improving Healthcare Processes & Integrating Health Data",
      "Overview"
    ]
  },
  {
    "objectID": "content/week-7/index.html#learning-outcomes",
    "href": "content/week-7/index.html#learning-outcomes",
    "title": "Overview",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of the week you will be able to:\n\nExplain visual representations of healthcare processes, including BPMN and PROforma\nDiscuss computational methods for analysing and discovering processes based on existing data\nDescribe data captured in the RDF data model and discuss the advantages that it brings to merging biomedical datasets\nArgue about the value that ontologies bring in the precision medicine era",
    "crumbs": [
      "Content",
      "Week 7: Improving Healthcare Processes & Integrating Health Data",
      "Overview"
    ]
  },
  {
    "objectID": "content/week-6/topic-3.html",
    "href": "content/week-6/topic-3.html",
    "title": "Embedding Ethics into Data Sharing",
    "section": "",
    "text": "Health data is also being captured outside of the medical encounter, particularly from the digital exhaust trails we leave behind when we interact with online services and technologies. These may be harder to monitor and regulate. For now, pause and think about these questions.\n\nHow comfortable would you be with your medical data being shared with researchers, with government analysts, or with companies?\nWould you be willing to accept the risk that even anonymised data could in theory be used to identify you?\nWhat types of data use do you think should require your explicit consent?\nAnd when is it ethical to routinely capture health data for public benefit?",
    "crumbs": [
      "Content",
      "Week 6: Machine Learning & Analysing Clinical Text",
      "Topic 3: Embedding Ethics into Data Sharing"
    ]
  },
  {
    "objectID": "content/week-6/topic-1.html",
    "href": "content/week-6/topic-1.html",
    "title": "Machine Learning",
    "section": "",
    "text": "What is machine learning, what can it do and what can it not do for biomedicine and healthcare? In this topic you will be introduced to two main types of machine learning, i.e. supervised and unsupervised learning, you will find out about some recent success in using machine learning methods in health, and you will learn how it relates to artificial intelligence and other topics covered in this course.",
    "crumbs": [
      "Content",
      "Week 6: Machine Learning & Analysing Clinical Text",
      "Topic 1: Machine Learning"
    ]
  },
  {
    "objectID": "content/week-6/topic-1.html#overview",
    "href": "content/week-6/topic-1.html#overview",
    "title": "Machine Learning",
    "section": "",
    "text": "What is machine learning, what can it do and what can it not do for biomedicine and healthcare? In this topic you will be introduced to two main types of machine learning, i.e. supervised and unsupervised learning, you will find out about some recent success in using machine learning methods in health, and you will learn how it relates to artificial intelligence and other topics covered in this course.",
    "crumbs": [
      "Content",
      "Week 6: Machine Learning & Analysing Clinical Text",
      "Topic 1: Machine Learning"
    ]
  },
  {
    "objectID": "content/week-6/topic-1.html#supervised-and-unsupervised-learning",
    "href": "content/week-6/topic-1.html#supervised-and-unsupervised-learning",
    "title": "Machine Learning",
    "section": "Supervised and Unsupervised Learning",
    "text": "Supervised and Unsupervised Learning\nWatch the following 4 videos to find out what machine learning is, and what methods are used in supervised and unsupervised learning for health.\nhttps://media.ed.ac.uk/media/1_kd30g0to\nTranscript\nhttps://media.ed.ac.uk/media/0_0o96pa11\nTranscript\nhttps://media.ed.ac.uk/media/1_l42iag3e\nTranscript\nhttps://media.ed.ac.uk/media/1_w35wwarn\nTranscript",
    "crumbs": [
      "Content",
      "Week 6: Machine Learning & Analysing Clinical Text",
      "Topic 1: Machine Learning"
    ]
  },
  {
    "objectID": "content/week-6/topic-1.html#case-study-of-machine-learning-in-medicine",
    "href": "content/week-6/topic-1.html#case-study-of-machine-learning-in-medicine",
    "title": "Machine Learning",
    "section": "Case Study of Machine Learning in Medicine",
    "text": "Case Study of Machine Learning in Medicine\nOne of the biggest success stories of machine learning in healthcare involves the use of deep learning methods for diagnosis and referral in retinal disease, which was the result of a collaboration between Moorfields Eye Hospital NHS Foundation Trust, the UCL Institute of Ophthalmology and DeepMind Health. Researchers from these organisations worked together to apply a novel deep learning architecture to a set of thousands of historic de-personalised OCT eye scans to identify signs of eye disease and recommend how patients should be referred for care. The results, which were published online in Nature Medicine, show that the AI system can recommend the correct referral decision for over 50 eye diseases with 94% accuracy, matching world-leading eye experts. This high level of accuracy is a major breakthrough, and shows great potential for helping doctors spot conditions earlier and quickly prioritise patients who need urgent treatment.\nIt is still early days, however, and the next step is to go through clinical trials to explore how this technology might improve patient care in practice, and regulatory approval before it can be used in hospitals and other clinical settings.\nReading based on content on Moorfields Eye Hospital website, where one can find further information and videos.",
    "crumbs": [
      "Content",
      "Week 6: Machine Learning & Analysing Clinical Text",
      "Topic 1: Machine Learning"
    ]
  },
  {
    "objectID": "content/week-6/topic-1.html#artificial-intelligence-is-more-than-machine-learning",
    "href": "content/week-6/topic-1.html#artificial-intelligence-is-more-than-machine-learning",
    "title": "Machine Learning",
    "section": "Artificial Intelligence is more than Machine Learning",
    "text": "Artificial Intelligence is more than Machine Learning\nThe term “Artificial Intelligence” is sometimes used as a synonym for Machine Learning. However, Artificial Intelligence (AI) is a much broader field than Machine Learning (ML). This reading aims to demystify what AI is and how it relates to ML.\nIn their seminal book “Artificial Intelligence: A Modern Approach”, Russell and Norvig explain that there are four approaches or types of definitions for AI:\n\nSystems that act like humans: The authors describe this as “the Turing Test approach”, and they point out that a machine would need to possess a range of capabilities, including among others: natural language processing, to allow successful communication in English or some other human language; computer vision, to perceive objects; and robotics, to manipulate objects and move around.\nSystems that think like humans: This is the cognitive modelling approach to AI, where essentially we’re trying to get inside the actual working of human mind and then replicate this in a machine. In order to construct precise and testable theories of the workings of the human mind, the interdisciplinary field of cognitive science brings together computer models from AI and experimental techniques from psychology.\nSystems that think rationally: This is the “laws of thought” approach, where we create intelligent systems by making precise statements about all kinds of things in the world and about the relations among them. In order to make these statements and make inferences based on them, the field of logic is typically used, tracing all the way back to the Greek philosopher Aristotle.\nSystems that act rationally: This is the “rational agent” approach to AI. A computer agent is more than just a computer program, in the sense that it can operate under autonomous control, perceive its environment, adapt to changes, take on another’s goals, etc. A rational agent is one that acts with the objective to achieve the best possible outcome.\n\nWhen looking at the history of AI (it all started in the 50s!), one can identify two key eras or schools of thought. The first era (between the 1950s and late 1980s), which is also referred to as symbolic AI or “good old-fashioned AI”, was dominated by the logic-based or rule-based approach. Expert systems were a prominent technology at the time, capturing facts and rules about the world, and deducing new information. MYCIN was one of the most influential medical expert systems during that era, and it was designed to identify bacteria causing severe infections, such as bacteremia and meningitis, and to recommend antibiotics, with the dosage adjusted for the patient’s body weight. The first era of AI ended with a so-called “AI winter” around the early 90s.\nWe are currently undergoing the second era or “spring” of AI, where machine learning is the main area of focus. This is not only because of recent advances in computational methods and algorithms, but perhaps more importantly because of the vast amounts of data that are now available for training machine learning systems. Machine learning methods are used across different areas and topics discussed in this course, such as medical imaging and natural language processing.\n\nDistinguishing hype from reality – Or what history can teach us\nThe AI winter of the early 90s was not a result of slow research progress or poor results, but rather the result of hype and overselling of AI in popular media. The hype in the press in the 80s sparkled public curiosity but also led to unfounded and overly optimistic predictions by observers. AI researchers tried to warn the business community and the media about the over-inflated promises and high expectations, but the hype continued, leading to disappointment, disillusionment and ultimately reduced research funding for AI.\nToday, we are undergoing an AI spring, with increased AI funding, commercial uptake, public interest and enthusiasm in the media. AI researchers, including researchers in AI in Medicine, have raised concerns around unrealistic promises and expectations, and have warned that a new AI winter might be triggered. Let’s see what the future brings…",
    "crumbs": [
      "Content",
      "Week 6: Machine Learning & Analysing Clinical Text",
      "Topic 1: Machine Learning"
    ]
  },
  {
    "objectID": "content/week-6/topic-1.html#bonus-content-the-state-of-artificial-intelligence-in-medicine",
    "href": "content/week-6/topic-1.html#bonus-content-the-state-of-artificial-intelligence-in-medicine",
    "title": "Machine Learning",
    "section": "Bonus Content: The State of Artificial Intelligence in Medicine",
    "text": "Bonus Content: The State of Artificial Intelligence in Medicine\nWatch this optional video by Stanford Medicine to find out what experts in AI in Medicine think about the present and future of this field.",
    "crumbs": [
      "Content",
      "Week 6: Machine Learning & Analysing Clinical Text",
      "Topic 1: Machine Learning"
    ]
  },
  {
    "objectID": "content/week-5/writing-functions-in-R.html",
    "href": "content/week-5/writing-functions-in-R.html",
    "title": "Optional: Writing Functions in R",
    "section": "",
    "text": "Writing Functions\n\nObjectives\n\nUnderstand the structure and purpose of functions in R.\nDevelop skills in writing functions in R.\n\n\n\nWhy Do We Use Functions?\nFunctions are a way of writing code that reduces the repetition of your code. They are a way to avoid copying and pasting bits of code throughout your script. To quote Hadley Wickham (creator of tidyverse):\n“One of the best ways to improve your reach as a data scientist is to write functions. Functions allow you to automate common tasks in a more powerful and general way than copy-and-pasting. Writing a function has four big advantages over using copy-and-paste.”\nHis 4 big advantages are:\n\nYou can give a function an evocative name that makes your code easier to understand.\nAs requirements change, you only need to update code in one place, instead of many.\nYou eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another).\nIt makes it easier to reuse work from project-to-project, increasing your productivity over time.\n\n\n\nHow do I know when to write a function?\n“A good rule of thumb is to consider writing a function whenever you’ve copied and pasted a block of code more than twice (i.e. you now have three copies of the same code)”\nAttribution: “R for Data Science: Functions” by Hadley Wickham is licensed under CC BY-NC-ND 3.0 US\n\n\nAnatomy of a Function\nOk, but what does a function look like? Here is a simple example:\n\n# Basic function structure\n \nmy_function &lt;- function(arg1, arg2) {\n  result &lt;- arg1 + arg2\n  return(result)\n}\n\nLet’s break it down into the key parts:\n\nnameof the function. This is usually a verb, because functions ‘do’ things.\nfunctionis a keyword in R and is required to create a function.\narguments(also known as parameters but let’s not get too scientific) are values that are ‘passed in’ to the function.\ncurly bracketsdefine thebodyof the function. This is a black box world that has access to the arguments that are passed in.\nreturnkeyword defines what the output should be. (But as we’ll see, we don’t actually need it!)\n\n\n\n\n\n\n\n\nInput and Output\nSo the arguments of a function are the input and the return keyword defines the output.\nA washing machine is a good metaphor. Let’s say the washing machine is the function - a black box so to speak. The laundry and detergent are the inputs and the clean laundry is the output.\n\n\nImplicit Returns\nUsually in R, we skip the return keyword and the function knows to return the last thing that was defined within in. So our example above would become:\n\n# Implicit return\n \nmy_function &lt;- function(arg1, arg2) {\n  result &lt;- arg1 + arg2\n}\n\nWe could even skip the aassignemt to the result variable. Hence our function becomes:\n\n# Implicit return\n \nmy_function &lt;- function(arg1, arg2) {\n  arg1 + arg2\n}\n\nThis still behaves the same way but we have removed the return statement and the variable assignment. The result value will be returned because it is the last thing that is defined in the function body. This means less code. Yay!\n\n\nDefining & Running Functions\nWhen you define a function, you’re saying to R “please read this piece of code (the function) and store it for later use”.\nWhen you run a function, you are asking it to execute. Let’s write an example in RStudio.\nNOTE: “call”, “run”, “execute” and “invoke” are all words for the same thing.\nIf you haven’t already, create a new project or use the project you are currently in. Create a new R Markdown file and save it as my_functions.Rmd.\nWrite the following code in a code chunk:\n\ncalculate_area &lt;- function(length, width) {\n  length * width\n}\n\nWhat do you notice when you run this code?\nNothing happens! You will however, find our function definition in the Environment tab. That means, our function has been defined.\nThe key thing about functions is that we define them once but can call them many times.\nLet’s add some code to call our function.\n\n# AS BEFORE\ncalculate_area &lt;- function(length, width) {\n  length * width\n}\n \n# Run the function\ncalculate_area(50, 10) # ADDED\n\nIn this case, we ask R to call the function with the values 50 for length and 10 for width.\nExecute the code by clicking the “Source” button. What did you expect to happen? Given the arguments 50 and 10, you might have expected to see 500 output somewhere because the calculate_area function multiplies length and width and returns the area.\nIn fact, the function has done this but the return value (i.e. 500) has been lost because we haven’t done anything with the return value.\nLet’s fix this:\n\n# AS BEFORE\ncalculate_area &lt;- function(length, width) {\n  length * width\n}\n \n# Run the function\narea &lt;- calculate_area(50, 10) # NEW\narea # NEW\n\nSuccess! You will see that 500 has been printed in the console. Try adding some more function calls to your script:\n\n# ... AS BEFORE\n \nlarge_area &lt;- calculate_area(2020, 900) # NEW\nprint(large_area) # 1818000\n \nsmall_area &lt;- calculate_area(2, 2) # NEW\nsmall_area # 4\n\n\n\nPractice writing your own functions\n\nDefine a function called add_two_numbers() that takes two arguments, adds them together, and returns the result. Remember to call the function and print out the result!\nDefine a function called square_number() that takes a single argument and returns its square.\nDefine a function called is_even() that takes one argument, checks if the number is even or odd, and returns TRUE for even numbers and FALSE for odd numbers. HINT use the modulo operator %%.\n\n\n\nA note on default arguments\nConsider the following code:\n\nhappy_birthday &lt;- function(input = \"you\") {\n  paste0(\"Happy birthday to \", input)\n}\n \nhappy_birthday()\n\nWhat do you hypothesise this code will do when we run it? Run the code and see if you are right.\nNotice that when we called the function happy_birthday() we didn’t pass in any arguments. We can either pass in an argument or we can skip it. If we skip it, it will default to “you”. But if we pass in an argument, the input will be assigned that value instead.\n\nhappy_birthday(\"Zaphod\") # Happy birthday to Zaphod\n\n\n\nKey Takeaways\n\nFunctions in R help reduce code repetition, improve readability, and minimise errors by allowing you to encapsulate common tasks into reusable blocks of code.\nNaming functions descriptively (usually as verbs) makes the purpose of your code clearer. Well-named functions improve the understanding and maintainability of your code.\nInputs (arguments) are passed into functions, and outputs are returned. You can use a function like a “black box” that processes inputs and produces outputs, similar to how a washing machine takes in laundry and detergent and returns clean laundry.\nReturn values can be specified explicitly using return() but we usually omit the return keyword.\nFunction definitions store code to be run later, and once defined, the function can be called multiple times with different inputs.\nDefault arguments allow flexibility in function calls, providing default values when no input is given, but allowing custom inputs when needed.\n\n\n\n\nApplying Functions\n\nObjectives\n\nKnow how to apply functions in several contexts\nDevelop skills writing vector functions and data frame functions\n\nNow that we’ve understood the basic structure of functions, let’s apply this knowledge to more practical contexts.\n\n\nVector Functions\nVector functions take one or more vectors as arguments and return a vector. Let’s look at an example:\n\nadd_vectors &lt;- function(vec1, vec2) {\n  vec1 + vec2\n}\n \n# Example usage:\nv1 &lt;- c(1, 2, 3)\nv2 &lt;- c(4, 5, 6)\n \nsum_vector &lt;- add_vectors(v1, v2)\nsum_vector  # c(5, 7, 9)\n\nIn this example, the function is called add_vectors and it takes 2 arguments. The return value is the sum of the vectors.\nWe call the function and pass in v1 and v2. The result is c(5, 7, 9)\n\n\nA More Complex Example\nIn this example we’ll tackle the following problem:\nGiven a vector of peoples birthdates, write a function to compute their age in years.\nWhat steps do we need to create a function?\n\nA function name. It should be a verb so let’s call it get_ages\nDefine the arguments. In this case we will have one argument. It should be a noun so let’s call it birth_dates\nThe body of the function. This is the code between the curly brackets that does some work and returns a value. In this case, calculate ages.\n\n\nget_ages &lt;- function(birth_dates) {\n# TODO: calculate ages\n}\n\nTry defining and calling this function yourself. Give it a go! Then check the solution.\nHINT: Load the lubridate library so you can use the now() and interval() functions.\n\n\nSolution\n\nlibrary(lubridate)\nget_ages &lt;- function(birthdates) {\n  birthdates_as_dates &lt;- as.Date(birthdates)\n  todays_date &lt;- now()\n  ages &lt;- interval( birthdates_as_dates, todays_date ) / years(1)\n  return( floor(ages) ) \n}\n \nexample_birthdates &lt;- c(\"2001-02-19\", \"1972-02-18\")\n \n# run, execute, call\nget_ages(example_birthdates) # 23 52\n \n# Note: The body of your function might be slightly different to this solution but might achieve the same thing. There's no one answer in coding! The key thing is to understand how functions work.\n\nQuestion: Think about how you would explain the two variables example_birthdates and birthdates to another person.\nSolution: One possible way to explain them is: example_birthdates is the value that is passed in to the function get_ages. get_ages has an argument defined as birthdates.\nWatch an example below of defining and running a function.\n\n\nVideo: Defining and running an age vector function (7:09)\nWatch the video here\n\n\nDataframe functions\nOften, we find ourselves copying and pasting multiple dplyr verbs. For example, have you ever wanted to count() a variable and add a column to show the proportions?\n\n# E.G.\n \ndf %&gt;% \n  count(my_var) %&gt;% \n  mutate(prop = n / sum(n))\n\nLet’s take an example. Consider the following code:\n\n# https://x.com/Diabb6/status/1571635146658402309\n# https://r4ds.hadley.nz/functions#common-use-cases\nlibrary(tidyverse)\n \ncount_prop &lt;- function(df, var, sort = FALSE) {\n  df %&gt;%\n    count({{ var }}, sort = sort) %&gt;%\n    mutate(prop = n / sum(n))\n}\n \n# Call the function\ndiamonds %&gt;% count_prop(clarity)\nmpg %&gt;% count_prop(model)\n\nLet’s break it down:\n\nThe function name is count_prop\nThe arguments are df, var and sort\nThe body of the function pipes the given dataframe into count and mutate and returns a dataframe (tibble)\n\nWhen we call the function, we pipe in the dataframe (in this case diamonds and mpg) which will be the value of the df argument. clarity and model are variables that will become the value of var and sort is not passed into the function so the default value (FALSE) is used in the function body.\nBasically, we’ve created our own custom dplyr style function. Nice!\nDid you spot the weird curly braces around the var keyword? This is known as embracing. We need these in order to tell dplyr to use the value of the variable rather than the literal word ‘var’. In this example the values will be clarity and model. This can be a difficult concept to get your head around. You can read a further explanation here -&gt; https://r4ds.hadley.nz/functions#indirection-and-tidy-evaluation\nCan you think of other situations when you have thought “I wish there was a dplyr function that did x,y,z”? If so, try writing one!\nMore examples can be found here\n\n\nVideo: A closer look at “embracing”\nWatch the video here\n\n\nA note on style\nThinking of good function names is not easy but it’s worth taking a few minutes to think about it before jumping to a name.\nfunc1 or func2 are not good names because:\n\nThey are not descriptive of what they do.\nThey are not verbs.\n\nThere’s little, if any, value in making function (or indeed) variable names as short as possible. Longer and more descriptive is always better.\nExamples of better names would be:\nstrip_white_space()\ncalculate_mean()\nplot_histogram()\nfilter_data()\nThese are good examples because they are descriptive and action-oriented, telling you exactly what the function is intended to do.\n\n\nKey Takeaways\n\nFunctions help you organize reusable code.\nVector functions: Operate on vectors and return vector outputs (e.g., add_vectors()).\nData frame functions: Combining dplyr functions (like count() and mutate()) into custom functions saves time and reduces redundancy (e.g., count_prop()).\nEmbracing: {{ }} is shorthand for embracing arguments in tidyverse functions. It allows you to pass column names or expressions as arguments in functions.\nGood naming matters: Use descriptive, action-oriented names for functions to improve code readability and understanding.\nPractice makes perfect: Developing your own functions, especially with clear names and focused tasks, makes your R coding more efficient and scalable.\n\n\n\n\nWriting Custom Plotting Functions with ggplot2\n\nObjectives\n\nLearn how to encapsulate repetitive plotting tasks into functions.\nUnderstand how to pass data and aesthetic mappings into custom ggplot2 functions.\nWrite flexible functions that allow for optional customisation of plots.\n\n\n\nPlotting Functions with ggplot2\nIn ggplot2, you often find yourself writing similar code multiple times with only slight variations, such as changing the data or the variables being plotted. This can lead to copying and pasting code, which makes your script harder to manage and debug. Functions to the rescue! Can you see how functions can help you simplify your workflow, reduce errors and make your code easier to maintain?\nLet’s look at an example. You can code along with these examples in a new R or markdown file, or use the file you created previously.\n\n\nA Simple Scatter Plot Function\nLet’s start by writing a function that returns a plot, given a dataset and any two variables.\n\n# Load the ggplot2 library (or tidyverse)\nlibrary(ggplot2)\n \n# Define a custom plotting function for scatter plots\nscatter_plot &lt;- function(df, x_var, y_var) {\n  df %&gt;% \n    ggplot( aes(x = {{ x_var }}, y = {{ y_var }})) +\n    geom_point() +\n    theme_minimal() +\n    labs(title = paste(\"Scatter plot of\", deparse(substitute(x_var)), \"vs\", deparse(substitute(y_var))))\n}\n \n# Example usage with built-in dataset\ndiamonds %&gt;% scatter_plot(price, carat)\n\n(diamonds is a built-in dataset containing the prices and other attributes of almost 54,000 diamonds).\nRemember, the double curly brackets around the x_var and y_var are to tell R to use the value of the variable. In this example the values will be price and carat.\n\nFunction Arguments:\ndf: The dataset to use.\nx_var and y_var: The variables to plot on the x and y axes.\nggplot2 Aesthetic Mapping (aes): We use curly brackets {{ }} to pass the variables to the aes function, allowing us to pass column names directly.\nTitle Creation:\nThe deparse(substitute(x_var)) captures the variable name for the plot title.\n\nTry using this function on another dataset of your choice. NOTE: You’ll have to load in the dataset or use a built-in one!\n\n\nCustomising our plot further\nWe can enhance the function by allowing more customisation, such as the colour of points and the size of the title.\n\n# Load the ggplot2 library (or tidyverse)\nlibrary(ggplot2)\n \nscatter_plot_custom &lt;- function(df, x_var, y_var, colour = \"blue\", title_size = 14) {\n  \n  df %&gt;% \n    ggplot(aes(x = {{ x_var }}, y = {{ y_var }})) +\n    geom_point(colour = colour) +\n    theme_minimal() +\n    labs(title = paste(\"Scatter plot of\", deparse(substitute(x_var)), \"vs\", deparse(substitute(y_var)))) +\n    theme(plot.title = element_text(size = title_size))\n}\n \n# Example usage with custom parameters\nmtcars %&gt;% scatter_plot_custom( wt, mpg, colour = \"darkgrey\", title_size = 16)\n\n\nDefault Arguments: The colour and title_size arguments let the user modify the appearance of the plot without changing the core structure.\nDefault Values: If the user doesn’t specify a colour or title size, the function uses default values (“blue” for colour and 14 for title size).\n\n\n\nA Histogram example\n\n# Attribution: https://r4ds.hadley.nz/functions.html#plot-functions\nhistogram &lt;- function(data, var, binwidth = NULL) {\n  data %&gt;% \n    ggplot(aes(x = {{ var }})) + \n    geom_histogram(binwidth = binwidth)\n}\n \ndiamonds %&gt;% histogram(carat, 0.1)\n\nFurther Examples: https://r4ds.hadley.nz/functions.html#plot-functions\n\n\nKey Takeaways\nBy writing custom plotting functions:\n\nYou can reuse code, avoid repetitive work, and make your scripts cleaner and more flexible.\nPlotting functions can be made more customisable by allowing arguments for anything you want to be made more flexible.",
    "crumbs": [
      "Content",
      "Week 5: Making Maps and Tables in R",
      "Optional: Writing Functions in R"
    ]
  },
  {
    "objectID": "content/week-5/topic-2.html",
    "href": "content/week-5/topic-2.html",
    "title": "Creating more exciting tables",
    "section": "",
    "text": "There are many packages which can be used to create tables in R. We have already looked at knitr::kable() and KableExtra in the course. Another popular table package is flextable, which is particularly powerful for making tables in Word. If you are interested, you can find a nice overview of packages for tables in R here.\ngt is the table compliment to the ggplot2 package for making plots - the first g in both cases meaning “grammar”. gt meaning “grammar of tables” and the gg in ggplot2 meaning “grammar of graphics”. This underlying general philosophy of tables makes the gt package easy to use (with some practice) and extremely flexible and customisable.\nJust as we build our plots in ggplot2 with layers, we can build gt tables by editing each part or layer of the table. As the gt developers outline: “The gt philosophy: we can construct a wide variety of useful tables with a cohesive set of table parts. These include the table header, the stub, the column labels and spanner column labels, the table body, and the table footer.”\n\n\n\nParts of a gt table\n\n\nThe gt package webpages have an excellent introduction to greating gt Tables, which you can find here.\nThomas Mock has some excellent resources on his blog as well which you may find helpful in getting accustomed to the gt package and its wide-ranging functionalities: gt cookbook as well as the advanced gt cookbook and more advanced making beautiful tables with gtExtra.\nNote: You are not expected to master gt nor the advanced elements of gt for this course. If you are interested, some advanced resources are included here. The gt introduction and gt cookbook would are a good place to start if you wish to explore the package and creating tables over and above what is covered in this optional Topic.",
    "crumbs": [
      "Content",
      "Week 5: Making Maps and Tables in R",
      "Topic 2: Creating more exciting tables"
    ]
  },
  {
    "objectID": "content/week-5/topic-2.html#the-gt-package",
    "href": "content/week-5/topic-2.html#the-gt-package",
    "title": "Creating more exciting tables",
    "section": "",
    "text": "There are many packages which can be used to create tables in R. We have already looked at knitr::kable() and KableExtra in the course. Another popular table package is flextable, which is particularly powerful for making tables in Word. If you are interested, you can find a nice overview of packages for tables in R here.\ngt is the table compliment to the ggplot2 package for making plots - the first g in both cases meaning “grammar”. gt meaning “grammar of tables” and the gg in ggplot2 meaning “grammar of graphics”. This underlying general philosophy of tables makes the gt package easy to use (with some practice) and extremely flexible and customisable.\nJust as we build our plots in ggplot2 with layers, we can build gt tables by editing each part or layer of the table. As the gt developers outline: “The gt philosophy: we can construct a wide variety of useful tables with a cohesive set of table parts. These include the table header, the stub, the column labels and spanner column labels, the table body, and the table footer.”\n\n\n\nParts of a gt table\n\n\nThe gt package webpages have an excellent introduction to greating gt Tables, which you can find here.\nThomas Mock has some excellent resources on his blog as well which you may find helpful in getting accustomed to the gt package and its wide-ranging functionalities: gt cookbook as well as the advanced gt cookbook and more advanced making beautiful tables with gtExtra.\nNote: You are not expected to master gt nor the advanced elements of gt for this course. If you are interested, some advanced resources are included here. The gt introduction and gt cookbook would are a good place to start if you wish to explore the package and creating tables over and above what is covered in this optional Topic.",
    "crumbs": [
      "Content",
      "Week 5: Making Maps and Tables in R",
      "Topic 2: Creating more exciting tables"
    ]
  },
  {
    "objectID": "content/week-5/topic-2.html#the-gt-table-workflow",
    "href": "content/week-5/topic-2.html#the-gt-table-workflow",
    "title": "Creating more exciting tables",
    "section": "The gt table workflow",
    "text": "The gt table workflow\nNot only is gt like ggplot2 but for tables, but it also follows tidyverse conventions! This means you can pipe your wrangled data into the gt() function seemlessly.\nA typical gt Table workflow is visualized below:\n\n\n\n\n\nYou begin with a preprocessed tabular data, such as a tibble. Next you create your gt table and customize it to your needs. Finally, the table is rendered by printing it at the console, including it in an R Markdown document, or exporting to a file using the gtsave() function.\nThe code can look a bit scary, but do not fear! Think about it as writing down in code all of the edits that you would make to a table in Word - only now it is reproducible as you have written this in code!",
    "crumbs": [
      "Content",
      "Week 5: Making Maps and Tables in R",
      "Topic 2: Creating more exciting tables"
    ]
  },
  {
    "objectID": "content/week-5/topic-2.html#an-example-of-gt-workflow",
    "href": "content/week-5/topic-2.html#an-example-of-gt-workflow",
    "title": "Creating more exciting tables",
    "section": "An example of gt workflow",
    "text": "An example of gt workflow\n\nThe data\nAs with everything we have learned about in programming in this course, we must first start with the data.\nWe will be using a new dataset from Public Health Scotland:\n\nStroke Mortality by Health Board\n\nLet’s start out with some data wrangling to get the data ready for presentation in a table\n\n# load libraries \n library(tidyverse)  \n library(gt) # remember to install gt in the first instance with install.packages(\"gt\")\n \n # import data\n stroke_mortality_raw &lt;- read_csv(\"https://www.opendata.nhs.scot/dataset/f5dcf382-e6ca-49f6-b807-4f9cc29555bc/resource/19c01b59-6cf7-42a9-876a-b07b9b92d6eb/download/stroke_mortalitybyhbr.csv\")\n \n hb &lt;- read_csv(\"https://www.opendata.nhs.scot/dataset/9f942fdb-e59e-44f5-b534-d6e17229cc7b/resource/652ff726-e676-4a20-abda-435b98dd7bdc/download/hb14_hb19.csv\")\n \n stroke_mortality &lt;- stroke_mortality_raw %&gt;%\n # Join cancelled to hb\n   left_join(hb, join_by(Hbr == HB)) %&gt;%\n # select the variables we are interested in \n   select(Year, HBName, AgeGroup, Sex, Diagnosis, NumberOfDeaths, CrudeRate, EASR) %&gt;% \n # filter out aggregate levels of the variables \n   filter(Sex != \"All\" & AgeGroup != \"All\")",
    "crumbs": [
      "Content",
      "Week 5: Making Maps and Tables in R",
      "Topic 2: Creating more exciting tables"
    ]
  },
  {
    "objectID": "content/week-5/topic-2.html#create-a-gt-table-with-gt",
    "href": "content/week-5/topic-2.html#create-a-gt-table-with-gt",
    "title": "Creating more exciting tables",
    "section": "Create a gt table with gt()",
    "text": "Create a gt table with gt()\nFor sake of simplicity, let’s say that we are specifically interested in the year 2020, adults 75 years old or older, and in 2 Health Boards: NHS Borders and NHS Fife.\nTo create a gt table object, all you need to do is pass your dataset, plus any data wrangling, to the gt() function. Because the gt package follows tidyverse conventions, our good friend the pipe (%&gt;%) will continue to be useful to us here to use the gt functions to modify the gt table object!\nstroke_mortality %&gt;%\n   filter(Year == 2020,\n          AgeGroup == \"75plus years\",\n          HBName %in% c(\"NHS Borders\", \"NHS Fife\")) %&gt;% \n   gt()\n\n\n\n\n\n\nYear\nHBName\nAgeGroup\nSex\nDiagnosis\nNumberOfDeaths\nCrudeRate\nEASR\n\n\n\n\n2020\nNHS Borders\n75plus years\nMale\nCerebrovascular Disease\n23\n393.633407\n424.306091\n\n\n2020\nNHS Borders\n75plus years\nFemale\nCerebrovascular Disease\n46\n617.947340\n609.187745\n\n\n2020\nNHS Fife\n75plus years\nMale\nCerebrovascular Disease\n98\n667.393081\n757.712097\n\n\n2020\nNHS Fife\n75plus years\nFemale\nCerebrovascular Disease\n151\n761.818274\n742.539160\n\n\n2020\nNHS Borders\n75plus years\nMale\nstroke\n17\n290.946432\n314.000526\n\n\n2020\nNHS Borders\n75plus years\nFemale\nstroke\n35\n470.177324\n463.058843\n\n\n2020\nNHS Fife\n75plus years\nMale\nstroke\n52\n354.126941\n396.146064\n\n\n2020\nNHS Fife\n75plus years\nFemale\nstroke\n87\n438.928409\n428.374773\n\n\n2020\nNHS Borders\n75plus years\nMale\nSubaracahnoid Haemorrhage\n1\n17.114496\n16.835017\n\n\n2020\nNHS Borders\n75plus years\nFemale\nSubaracahnoid Haemorrhage\n1\n13.433638\n14.804945\n\n\n2020\nNHS Fife\n75plus years\nMale\nSubaracahnoid Haemorrhage\n1\n6.810133\n10.797970\n\n\n2020\nNHS Fife\n75plus years\nFemale\nSubaracahnoid Haemorrhage\n1\n5.045154\n5.460676\n\n\n2020\nNHS Borders\n75plus years\nMale\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\n2020\nNHS Borders\n75plus years\nFemale\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\n2020\nNHS Fife\n75plus years\nMale\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\n2020\nNHS Fife\n75plus years\nFemale\nTIAs and related syndromes\n1\n5.045154\n5.059705\n\n\n\n\n\nBecause we have filtered the data to only be for one Year and one AgeGroup, it is not necessarily relevant to include them in the table as we can highlight what data is presented in the table title (covered later in this tutorial). In this case, you can first select only the variables you are interested in showing in the table before creating the gt object.\n\nstroke_mortality %&gt;%\n   filter(Year == 2020,\n          AgeGroup == \"75plus years\",\n          HBName %in% c(\"NHS Borders\", \"NHS Fife\")) %&gt;% \n   select(HBName, \n          Sex, \n          Diagnosis, \n          NumberOfDeaths, \n          CrudeRate, \n          EASR) %&gt;% \n   gt()\n\n\n\n\n\n\n\nHBName\nSex\nDiagnosis\nNumberOfDeaths\nCrudeRate\nEASR\n\n\n\n\nNHS Borders\nMale\nCerebrovascular Disease\n23\n393.633407\n424.306091\n\n\nNHS Borders\nFemale\nCerebrovascular Disease\n46\n617.947340\n609.187745\n\n\nNHS Fife\nMale\nCerebrovascular Disease\n98\n667.393081\n757.712097\n\n\nNHS Fife\nFemale\nCerebrovascular Disease\n151\n761.818274\n742.539160\n\n\nNHS Borders\nMale\nstroke\n17\n290.946432\n314.000526\n\n\nNHS Borders\nFemale\nstroke\n35\n470.177324\n463.058843\n\n\nNHS Fife\nMale\nstroke\n52\n354.126941\n396.146064\n\n\nNHS Fife\nFemale\nstroke\n87\n438.928409\n428.374773\n\n\nNHS Borders\nMale\nSubaracahnoid Haemorrhage\n1\n17.114496\n16.835017\n\n\nNHS Borders\nFemale\nSubaracahnoid Haemorrhage\n1\n13.433638\n14.804945\n\n\nNHS Fife\nMale\nSubaracahnoid Haemorrhage\n1\n6.810133\n10.797970\n\n\nNHS Fife\nFemale\nSubaracahnoid Haemorrhage\n1\n5.045154\n5.460676\n\n\nNHS Borders\nMale\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\nNHS Borders\nFemale\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\nNHS Fife\nMale\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\nNHS Fife\nFemale\nTIAs and related syndromes\n1\n5.045154\n5.059705",
    "crumbs": [
      "Content",
      "Week 5: Making Maps and Tables in R",
      "Topic 2: Creating more exciting tables"
    ]
  },
  {
    "objectID": "content/week-5/topic-2.html#modify-columns-with-the-cols_-functions",
    "href": "content/week-5/topic-2.html#modify-columns-with-the-cols_-functions",
    "title": "Creating more exciting tables",
    "section": "Modify columns with the cols_*() functions",
    "text": "Modify columns with the cols_*() functions\nColumn labels, cell alignment, column width, and placement as well as combine multiple columns with the range of cols_*() functions.\nFor example, cols_label() is particularly useful to rename columns to more informative names than what the variable is called in the dataset. This changes the labels of the columns, as per the function name, not the underlying column names (which remain the name of the variable in your dataset). Tables should be able to be understood by people who are not familiar with your dataset, so it is important for your variables to have informative names.\n\nstroke_mortality %&gt;%\n   filter(Year == 2020,\n          AgeGroup == \"75plus years\",\n          HBName %in% c(\"NHS Borders\", \"NHS Fife\")) %&gt;% \n   select(HBName, \n          Sex, \n          Diagnosis, \n          NumberOfDeaths, \n          CrudeRate, \n          EASR) %&gt;% \n   gt() %&gt;% \n   cols_label(HBName = \"Health Board\",\n              NumberOfDeaths = \"Number of Deaths\",\n              CrudeRate = \"Crude Rate\", \n              EASR = \"European Age-Sex Standardised Rate\") \n\n\n\n\n\n\n\nHealth Board\nSex\nDiagnosis\nNumber of Deaths\nCrude Rate\nEuropean Age-Sex Standardised Rate\n\n\n\n\nNHS Borders\nMale\nCerebrovascular Disease\n23\n393.633407\n424.306091\n\n\nNHS Borders\nFemale\nCerebrovascular Disease\n46\n617.947340\n609.187745\n\n\nNHS Fife\nMale\nCerebrovascular Disease\n98\n667.393081\n757.712097\n\n\nNHS Fife\nFemale\nCerebrovascular Disease\n151\n761.818274\n742.539160\n\n\nNHS Borders\nMale\nstroke\n17\n290.946432\n314.000526\n\n\nNHS Borders\nFemale\nstroke\n35\n470.177324\n463.058843\n\n\nNHS Fife\nMale\nstroke\n52\n354.126941\n396.146064\n\n\nNHS Fife\nFemale\nstroke\n87\n438.928409\n428.374773\n\n\nNHS Borders\nMale\nSubaracahnoid Haemorrhage\n1\n17.114496\n16.835017\n\n\nNHS Borders\nFemale\nSubaracahnoid Haemorrhage\n1\n13.433638\n14.804945\n\n\nNHS Fife\nMale\nSubaracahnoid Haemorrhage\n1\n6.810133\n10.797970\n\n\nNHS Fife\nFemale\nSubaracahnoid Haemorrhage\n1\n5.045154\n5.460676\n\n\nNHS Borders\nMale\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\nNHS Borders\nFemale\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\nNHS Fife\nMale\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\nNHS Fife\nFemale\nTIAs and related syndromes\n1\n5.045154\n5.059705\n\n\n\n\n\n\n\ncols_align() aligns all text within a column. You can specify which columns to align using vars() - notice that the arguments are the original variable names. Commonly, the convention is to left-align text with varying length and right-align numbers. You can align different columns different ways by adding multiple cols_align() layers.\n\nstroke_mortality %&gt;%\n   filter(Year == 2020,\n          AgeGroup == \"75plus years\",\n          HBName %in% c(\"NHS Borders\", \"NHS Fife\")) %&gt;% \n   select(HBName, \n          Sex, \n          Diagnosis,\n          NumberOfDeaths,\n          CrudeRate, \n          EASR) %&gt;% \n   gt() %&gt;% \n   cols_label(HBName = \"Health Board\",\n              NumberOfDeaths = \"Number of Deaths\",\n              CrudeRate = \"Crude Rate\", \n              EASR = \"European Age-Sex Standardised Rate\") %&gt;% \n   cols_align(align = \"center\",\n              columns = NumberOfDeaths)\n\n\n\n\n\n\n\nHealth Board\nSex\nDiagnosis\nNumber of Deaths\nCrude Rate\nEuropean Age-Sex Standardised Rate\n\n\n\n\nNHS Borders\nMale\nCerebrovascular Disease\n23\n393.633407\n424.306091\n\n\nNHS Borders\nFemale\nCerebrovascular Disease\n46\n617.947340\n609.187745\n\n\nNHS Fife\nMale\nCerebrovascular Disease\n98\n667.393081\n757.712097\n\n\nNHS Fife\nFemale\nCerebrovascular Disease\n151\n761.818274\n742.539160\n\n\nNHS Borders\nMale\nstroke\n17\n290.946432\n314.000526\n\n\nNHS Borders\nFemale\nstroke\n35\n470.177324\n463.058843\n\n\nNHS Fife\nMale\nstroke\n52\n354.126941\n396.146064\n\n\nNHS Fife\nFemale\nstroke\n87\n438.928409\n428.374773\n\n\nNHS Borders\nMale\nSubaracahnoid Haemorrhage\n1\n17.114496\n16.835017\n\n\nNHS Borders\nFemale\nSubaracahnoid Haemorrhage\n1\n13.433638\n14.804945\n\n\nNHS Fife\nMale\nSubaracahnoid Haemorrhage\n1\n6.810133\n10.797970\n\n\nNHS Fife\nFemale\nSubaracahnoid Haemorrhage\n1\n5.045154\n5.460676\n\n\nNHS Borders\nMale\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\nNHS Borders\nFemale\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\nNHS Fife\nMale\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\nNHS Fife\nFemale\nTIAs and related syndromes\n1\n5.045154\n5.059705\n\n\n\n\n\n\n\ncols_move_*() set of functions allows you to move columns to the start or end (or wherever you want!) in your table. To move a column to the start we use cols_move_to_start() and to move a column to the end, the function is cols_move_to_end().\n\nstroke_mortality %&gt;%\n  filter(Year == 2020,\n         AgeGroup == \"75plus years\",\n         HBName %in% c(\"NHS Borders\", \"NHS Fife\")) %&gt;% \n  select(HBName, \n         Sex, \n         Diagnosis, \n         NumberOfDeaths, \n         CrudeRate, \n         EASR) %&gt;% \n  gt() %&gt;% \n# to move the diagnosis and Sex columns to the start \n  cols_move_to_start(columns = c(Diagnosis, Sex)) %&gt;% \n# to move the HBName after Number of Deaths \n  cols_move(columns = HBName, after = NumberOfDeaths) %&gt;% \n  cols_label(HBName = \"Health Board\",\n             NumberOfDeaths = \"Number of Deaths\",\n             CrudeRate = \"Crude Rate\", \n             EASR = \"European Age-Sex Standardised Rate\") %&gt;% \n cols_align(align = \"center\", columns = NumberOfDeaths)\n\n\n\n\n\n\n\nDiagnosis\nSex\nNumber of Deaths\nHealth Board\nCrude Rate\nEuropean Age-Sex Standardised Rate\n\n\n\n\nCerebrovascular Disease\nMale\n23\nNHS Borders\n393.633407\n424.306091\n\n\nCerebrovascular Disease\nFemale\n46\nNHS Borders\n617.947340\n609.187745\n\n\nCerebrovascular Disease\nMale\n98\nNHS Fife\n667.393081\n757.712097\n\n\nCerebrovascular Disease\nFemale\n151\nNHS Fife\n761.818274\n742.539160\n\n\nstroke\nMale\n17\nNHS Borders\n290.946432\n314.000526\n\n\nstroke\nFemale\n35\nNHS Borders\n470.177324\n463.058843\n\n\nstroke\nMale\n52\nNHS Fife\n354.126941\n396.146064\n\n\nstroke\nFemale\n87\nNHS Fife\n438.928409\n428.374773\n\n\nSubaracahnoid Haemorrhage\nMale\n1\nNHS Borders\n17.114496\n16.835017\n\n\nSubaracahnoid Haemorrhage\nFemale\n1\nNHS Borders\n13.433638\n14.804945\n\n\nSubaracahnoid Haemorrhage\nMale\n1\nNHS Fife\n6.810133\n10.797970\n\n\nSubaracahnoid Haemorrhage\nFemale\n1\nNHS Fife\n5.045154\n5.460676\n\n\nTIAs and related syndromes\nMale\n0\nNHS Borders\n0.000000\n0.000000\n\n\nTIAs and related syndromes\nFemale\n0\nNHS Borders\n0.000000\n0.000000\n\n\nTIAs and related syndromes\nMale\n0\nNHS Fife\n0.000000\n0.000000\n\n\nTIAs and related syndromes\nFemale\n1\nNHS Fife\n5.045154\n5.059705",
    "crumbs": [
      "Content",
      "Week 5: Making Maps and Tables in R",
      "Topic 2: Creating more exciting tables"
    ]
  },
  {
    "objectID": "content/week-5/topic-2.html#format-columns-with-the-fmt_-functions",
    "href": "content/week-5/topic-2.html#format-columns-with-the-fmt_-functions",
    "title": "Creating more exciting tables",
    "section": "Format columns with the fmt_*() functions",
    "text": "Format columns with the fmt_*() functions\nTo format values within columns, you can use the range of fmt_*() functions.\nfmt_number() formats numeric (number-based) columns. For example, using the argument decimal = 2 you can set a decimal place or add a suffix such as K for thousands to large numbers using suffixing = TRUE.\n\nstroke_mortality %&gt;%\n   filter(Year == 2020,\n          AgeGroup == \"75plus years\",\n          HBName %in% c(\"NHS Borders\", \"NHS Fife\")) %&gt;% \n   select(HBName, Sex, Diagnosis, NumberOfDeaths, CrudeRate, EASR) %&gt;% \n   gt() %&gt;%\n   fmt_number(columns = c(CrudeRate, EASR), decimals = 3)\n\n\n\n\n\n\n\nHBName\nSex\nDiagnosis\nNumberOfDeaths\nCrudeRate\nEASR\n\n\n\n\nNHS Borders\nMale\nCerebrovascular Disease\n23\n393.633\n424.306\n\n\nNHS Borders\nFemale\nCerebrovascular Disease\n46\n617.947\n609.188\n\n\nNHS Fife\nMale\nCerebrovascular Disease\n98\n667.393\n757.712\n\n\nNHS Fife\nFemale\nCerebrovascular Disease\n151\n761.818\n742.539\n\n\nNHS Borders\nMale\nstroke\n17\n290.946\n314.001\n\n\nNHS Borders\nFemale\nstroke\n35\n470.177\n463.059\n\n\nNHS Fife\nMale\nstroke\n52\n354.127\n396.146\n\n\nNHS Fife\nFemale\nstroke\n87\n438.928\n428.375\n\n\nNHS Borders\nMale\nSubaracahnoid Haemorrhage\n1\n17.114\n16.835\n\n\nNHS Borders\nFemale\nSubaracahnoid Haemorrhage\n1\n13.434\n14.805\n\n\nNHS Fife\nMale\nSubaracahnoid Haemorrhage\n1\n6.810\n10.798\n\n\nNHS Fife\nFemale\nSubaracahnoid Haemorrhage\n1\n5.045\n5.461\n\n\nNHS Borders\nMale\nTIAs and related syndromes\n0\n0.000\n0.000\n\n\nNHS Borders\nFemale\nTIAs and related syndromes\n0\n0.000\n0.000\n\n\nNHS Fife\nMale\nTIAs and related syndromes\n0\n0.000\n0.000\n\n\nNHS Fife\nFemale\nTIAs and related syndromes\n1\n5.045\n5.060\n\n\n\n\n\n\n\nfmt_percent() formats numbers into percents and includes the % symbol for us. By default, the values are multiplied by 100 before adding the % symbol. So, if your data is already multiplied by 100, use the scale_value = FALSE argument to tell R that the input is not a proportion.\n\nstroke_mortality %&gt;%\n  filter(Year == 2020,\n          AgeGroup == \"75plus years\",\n          HBName %in% c(\"NHS Borders\", \"NHS Fife\")) %&gt;% \n  select(HBName,\n         Sex, \n         Diagnosis,\n         NumberOfDeaths, \n         CrudeRate, \n         EASR) %&gt;%\n  gt() %&gt;%\n  fmt_percent(columns = c(CrudeRate, EASR), decimals = 0, scale_value = FALSE)\n\n\n\n\n\n\n\nHBName\nSex\nDiagnosis\nNumberOfDeaths\nCrudeRate\nEASR\n\n\n\n\nNHS Borders\nMale\nCerebrovascular Disease\n23\n394%\n424%\n\n\nNHS Borders\nFemale\nCerebrovascular Disease\n46\n618%\n609%\n\n\nNHS Fife\nMale\nCerebrovascular Disease\n98\n667%\n758%\n\n\nNHS Fife\nFemale\nCerebrovascular Disease\n151\n762%\n743%\n\n\nNHS Borders\nMale\nstroke\n17\n291%\n314%\n\n\nNHS Borders\nFemale\nstroke\n35\n470%\n463%\n\n\nNHS Fife\nMale\nstroke\n52\n354%\n396%\n\n\nNHS Fife\nFemale\nstroke\n87\n439%\n428%\n\n\nNHS Borders\nMale\nSubaracahnoid Haemorrhage\n1\n17%\n17%\n\n\nNHS Borders\nFemale\nSubaracahnoid Haemorrhage\n1\n13%\n15%\n\n\nNHS Fife\nMale\nSubaracahnoid Haemorrhage\n1\n7%\n11%\n\n\nNHS Fife\nFemale\nSubaracahnoid Haemorrhage\n1\n5%\n5%\n\n\nNHS Borders\nMale\nTIAs and related syndromes\n0\n0%\n0%\n\n\nNHS Borders\nFemale\nTIAs and related syndromes\n0\n0%\n0%\n\n\nNHS Fife\nMale\nTIAs and related syndromes\n0\n0%\n0%\n\n\nNHS Fife\nFemale\nTIAs and related syndromes\n1\n5%\n5%\n\n\n\n\n\n\n\nsub_missing() formats the default NA missing values in R to be a dash instead. As the subset of data we have been using does not have any NAs, we need to first change the data filtering a bit of our code to include NA values.\n\nstroke_mortality %&gt;% \n  filter(Year == 2012,\n         AgeGroup == \"75plus years\",\n         HBName %in% c(\"NHS Orkney\", \"NHS Grampian\")) %&gt;% \n  select(-Year, -AgeGroup) %&gt;%\n  gt() %&gt;%\n  sub_missing(columns = c(NumberOfDeaths, CrudeRate))\n\n\n\n\n\n\n\nHBName\nSex\nDiagnosis\nNumberOfDeaths\nCrudeRate\nEASR",
    "crumbs": [
      "Content",
      "Week 5: Making Maps and Tables in R",
      "Topic 2: Creating more exciting tables"
    ]
  },
  {
    "objectID": "content/week-5/topic-2.html#and-now-to-the-stub-or-rows",
    "href": "content/week-5/topic-2.html#and-now-to-the-stub-or-rows",
    "title": "Creating more exciting tables",
    "section": "…and now to the stub (or rows)!",
    "text": "…and now to the stub (or rows)!\nThe stub is typically a column of row labels that does not have or need a column label.\n\nAdd a grouping using the rowname_col argument\nThere are two ways in which you can add a grouping variable to your table, which differ in aesthetics\n\nby passing grouped data to the gt() function\n\n\n stroke_mortality %&gt;%\n  filter(Year == 2020,\n         AgeGroup == \"75plus years\",\n         HBName %in% c(\"NHS Borders\", \"NHS Fife\")) %&gt;% \n  select(-Year, - AgeGroup) %&gt;%\n  group_by(Sex) %&gt;%\n  gt()\n\n\n\n\n\n\n\nHBName\nDiagnosis\nNumberOfDeaths\nCrudeRate\nEASR\n\n\n\n\nMale\n\n\nNHS Borders\nCerebrovascular Disease\n23\n393.633407\n424.306091\n\n\nNHS Fife\nCerebrovascular Disease\n98\n667.393081\n757.712097\n\n\nNHS Borders\nstroke\n17\n290.946432\n314.000526\n\n\nNHS Fife\nstroke\n52\n354.126941\n396.146064\n\n\nNHS Borders\nSubaracahnoid Haemorrhage\n1\n17.114496\n16.835017\n\n\nNHS Fife\nSubaracahnoid Haemorrhage\n1\n6.810133\n10.797970\n\n\nNHS Borders\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\nNHS Fife\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\nFemale\n\n\nNHS Borders\nCerebrovascular Disease\n46\n617.947340\n609.187745\n\n\nNHS Fife\nCerebrovascular Disease\n151\n761.818274\n742.539160\n\n\nNHS Borders\nstroke\n35\n470.177324\n463.058843\n\n\nNHS Fife\nstroke\n87\n438.928409\n428.374773\n\n\nNHS Borders\nSubaracahnoid Haemorrhage\n1\n13.433638\n14.804945\n\n\nNHS Fife\nSubaracahnoid Haemorrhage\n1\n5.045154\n5.460676\n\n\nNHS Borders\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\nNHS Fife\nTIAs and related syndromes\n1\n5.045154\n5.059705\n\n\n\n\n\n\n\n\nspecify rowname_col within gt()\n\n\nstroke_mortality %&gt;%\n  filter(Year == 2020,\n         AgeGroup == \"75plus years\",\n         HBName %in% c(\"NHS Borders\", \"NHS Fife\")) %&gt;% \n  select(-Year, - AgeGroup) %&gt;%\n  gt(rowname_col = \"Sex\")\n\n\n\n\n\n\n\n\nHBName\nDiagnosis\nNumberOfDeaths\nCrudeRate\nEASR\n\n\n\n\nMale\nNHS Borders\nCerebrovascular Disease\n23\n393.633407\n424.306091\n\n\nFemale\nNHS Borders\nCerebrovascular Disease\n46\n617.947340\n609.187745\n\n\nMale\nNHS Fife\nCerebrovascular Disease\n98\n667.393081\n757.712097\n\n\nFemale\nNHS Fife\nCerebrovascular Disease\n151\n761.818274\n742.539160\n\n\nMale\nNHS Borders\nstroke\n17\n290.946432\n314.000526\n\n\nFemale\nNHS Borders\nstroke\n35\n470.177324\n463.058843\n\n\nMale\nNHS Fife\nstroke\n52\n354.126941\n396.146064\n\n\nFemale\nNHS Fife\nstroke\n87\n438.928409\n428.374773\n\n\nMale\nNHS Borders\nSubaracahnoid Haemorrhage\n1\n17.114496\n16.835017\n\n\nFemale\nNHS Borders\nSubaracahnoid Haemorrhage\n1\n13.433638\n14.804945\n\n\nMale\nNHS Fife\nSubaracahnoid Haemorrhage\n1\n6.810133\n10.797970\n\n\nFemale\nNHS Fife\nSubaracahnoid Haemorrhage\n1\n5.045154\n5.460676\n\n\nMale\nNHS Borders\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\nFemale\nNHS Borders\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\nMale\nNHS Fife\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\nFemale\nNHS Fife\nTIAs and related syndromes\n1\n5.045154\n5.059705\n\n\n\n\n\n\n\nTo add a column name to the stub, if desired, you can use tab_stubhead().\n\nstroke_mortality %&gt;%\n  filter(Year == 2020,\n         AgeGroup == \"75plus years\",\n         HBName %in% c(\"NHS Borders\", \"NHS Fife\")) %&gt;% \n  #select(-Year, - AgeGroup) %&gt;%\n  group_by(Sex) %&gt;% \n  gt() %&gt;% \n  summary_rows(columns = NumberOfDeaths, \n               fns = list(\"Average\" = ~mean(., na.rm = TRUE)))\n\n\n\n\n\n\n\n\nYear\nHBName\nAgeGroup\nDiagnosis\nNumberOfDeaths\nCrudeRate\nEASR\n\n\n\n\nMale\n\n\n\n2020\nNHS Borders\n75plus years\nCerebrovascular Disease\n23\n393.633407\n424.306091\n\n\n\n2020\nNHS Fife\n75plus years\nCerebrovascular Disease\n98\n667.393081\n757.712097\n\n\n\n2020\nNHS Borders\n75plus years\nstroke\n17\n290.946432\n314.000526\n\n\n\n2020\nNHS Fife\n75plus years\nstroke\n52\n354.126941\n396.146064\n\n\n\n2020\nNHS Borders\n75plus years\nSubaracahnoid Haemorrhage\n1\n17.114496\n16.835017\n\n\n\n2020\nNHS Fife\n75plus years\nSubaracahnoid Haemorrhage\n1\n6.810133\n10.797970\n\n\n\n2020\nNHS Borders\n75plus years\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\n\n2020\nNHS Fife\n75plus years\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\nAverage\n—\n—\n—\n—\n24.00\n—\n—\n\n\nFemale\n\n\n\n2020\nNHS Borders\n75plus years\nCerebrovascular Disease\n46\n617.947340\n609.187745\n\n\n\n2020\nNHS Fife\n75plus years\nCerebrovascular Disease\n151\n761.818274\n742.539160\n\n\n\n2020\nNHS Borders\n75plus years\nstroke\n35\n470.177324\n463.058843\n\n\n\n2020\nNHS Fife\n75plus years\nstroke\n87\n438.928409\n428.374773\n\n\n\n2020\nNHS Borders\n75plus years\nSubaracahnoid Haemorrhage\n1\n13.433638\n14.804945\n\n\n\n2020\nNHS Fife\n75plus years\nSubaracahnoid Haemorrhage\n1\n5.045154\n5.460676\n\n\n\n2020\nNHS Borders\n75plus years\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\n\n2020\nNHS Fife\n75plus years\nTIAs and related syndromes\n1\n5.045154\n5.059705\n\n\nAverage\n—\n—\n—\n—\n40.25\n—\n—\n\n\n\n\n\n\n\nYou can also add grand summaries to grouped data using grand_summary_rows().\n\nstroke_mortality %&gt;%\n  filter(Year == 2020,\n         AgeGroup == \"75plus years\",\n         HBName %in% c(\"NHS Borders\", \"NHS Fife\")) %&gt;% \n  select(-Year, - AgeGroup) %&gt;%   \n  group_by(Sex) %&gt;% \n  gt() %&gt;% \n  summary_rows(columns = c(NumberOfDeaths, CrudeRate, EASR), \n               fns = list(Average = ~mean(., na.rm = TRUE))) %&gt;% \n  grand_summary_rows(columns = c(NumberOfDeaths, CrudeRate, EASR), \n                     fns = list(\"Overall Average\" = ~mean(., na.rm = TRUE)))\n\n\n\n\n\n\n\n\nHBName\nDiagnosis\nNumberOfDeaths\nCrudeRate\nEASR\n\n\n\n\nMale\n\n\n\nNHS Borders\nCerebrovascular Disease\n23\n393.633407\n424.306091\n\n\n\nNHS Fife\nCerebrovascular Disease\n98\n667.393081\n757.712097\n\n\n\nNHS Borders\nstroke\n17\n290.946432\n314.000526\n\n\n\nNHS Fife\nstroke\n52\n354.126941\n396.146064\n\n\n\nNHS Borders\nSubaracahnoid Haemorrhage\n1\n17.114496\n16.835017\n\n\n\nNHS Fife\nSubaracahnoid Haemorrhage\n1\n6.810133\n10.797970\n\n\n\nNHS Borders\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\n\nNHS Fife\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\nAverage\n—\n—\n24.00\n216.2531\n239.9747\n\n\nFemale\n\n\n\nNHS Borders\nCerebrovascular Disease\n46\n617.947340\n609.187745\n\n\n\nNHS Fife\nCerebrovascular Disease\n151\n761.818274\n742.539160\n\n\n\nNHS Borders\nstroke\n35\n470.177324\n463.058843\n\n\n\nNHS Fife\nstroke\n87\n438.928409\n428.374773\n\n\n\nNHS Borders\nSubaracahnoid Haemorrhage\n1\n13.433638\n14.804945\n\n\n\nNHS Fife\nSubaracahnoid Haemorrhage\n1\n5.045154\n5.460676\n\n\n\nNHS Borders\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\n\nNHS Fife\nTIAs and related syndromes\n1\n5.045154\n5.059705\n\n\nAverage\n—\n—\n40.25\n289.0494\n283.5607\n\n\nOverall Average\n—\n—\n32.125\n252.6512\n261.7677\n\n\n\n\n\n\n\nYou may have noticed that the decimal places are all of the place in the above table. It is good practice to have a consistent reporting of numbers to the same decimal place. We can sort out the columns with fmt_number() which have have seen previously and the summary rows with the argument fmt.\n\nstroke_mortality %&gt;%\n  filter(Year == 2020,\n         AgeGroup == \"75plus years\",\n         HBName %in% c(\"NHS Borders\", \"NHS Fife\")) %&gt;% \n  select(-Year, - AgeGroup) %&gt;%   \n  group_by(Sex) %&gt;% \n  gt() %&gt;% \n  # format numbers in columns \n  fmt_number(columns = c(CrudeRate, EASR), decimals = 3) %&gt;% \n  summary_rows(columns = c(NumberOfDeaths, CrudeRate, EASR), \n               fns = list(Average = ~mean(., na.rm = TRUE)),\n               # format number in grouped summary rows \n               fmt = list(~ fmt_number(., decimals = 3))) %&gt;% \n  grand_summary_rows(columns = c(NumberOfDeaths, CrudeRate, EASR), \n                     fns = list(\"Overall Average\" = ~mean(., na.rm = TRUE)),\n                     # and format number in grand summary row \n                     fmt = list(~ fmt_number(., decimals = 3)))\n\n\n\n\n\n\n\n\nHBName\nDiagnosis\nNumberOfDeaths\nCrudeRate\nEASR\n\n\n\n\nMale\n\n\n\nNHS Borders\nCerebrovascular Disease\n23\n393.633\n424.306\n\n\n\nNHS Fife\nCerebrovascular Disease\n98\n667.393\n757.712\n\n\n\nNHS Borders\nstroke\n17\n290.946\n314.001\n\n\n\nNHS Fife\nstroke\n52\n354.127\n396.146\n\n\n\nNHS Borders\nSubaracahnoid Haemorrhage\n1\n17.114\n16.835\n\n\n\nNHS Fife\nSubaracahnoid Haemorrhage\n1\n6.810\n10.798\n\n\n\nNHS Borders\nTIAs and related syndromes\n0\n0.000\n0.000\n\n\n\nNHS Fife\nTIAs and related syndromes\n0\n0.000\n0.000\n\n\nAverage\n—\n—\n24.000\n216.253\n239.975\n\n\nFemale\n\n\n\nNHS Borders\nCerebrovascular Disease\n46\n617.947\n609.188\n\n\n\nNHS Fife\nCerebrovascular Disease\n151\n761.818\n742.539\n\n\n\nNHS Borders\nstroke\n35\n470.177\n463.059\n\n\n\nNHS Fife\nstroke\n87\n438.928\n428.375\n\n\n\nNHS Borders\nSubaracahnoid Haemorrhage\n1\n13.434\n14.805\n\n\n\nNHS Fife\nSubaracahnoid Haemorrhage\n1\n5.045\n5.461\n\n\n\nNHS Borders\nTIAs and related syndromes\n0\n0.000\n0.000\n\n\n\nNHS Fife\nTIAs and related syndromes\n1\n5.045\n5.060\n\n\nAverage\n—\n—\n40.250\n289.049\n283.561\n\n\nOverall Average\n—\n—\n32.125\n252.651\n261.768",
    "crumbs": [
      "Content",
      "Week 5: Making Maps and Tables in R",
      "Topic 2: Creating more exciting tables"
    ]
  },
  {
    "objectID": "content/week-5/topic-2.html#add-the-final-touches",
    "href": "content/week-5/topic-2.html#add-the-final-touches",
    "title": "Creating more exciting tables",
    "section": "Add the final touches",
    "text": "Add the final touches\nOnce you have your table formatted as you would like, it is important to add the final touches to make it formative for your target audience, including a header (title) and any notes.\nAdd a title and/or subtitle using tab_header().\n\nstroke_mortality %&gt;%\n  filter(Year == 2020,\n         AgeGroup == \"75plus years\",\n         HBName %in% c(\"NHS Borders\", \"NHS Fife\")) %&gt;% \n  select(-Year, - AgeGroup) %&gt;%   \n  gt() %&gt;% \n  tab_header(title = \"Stroke Mortality in 2020 in Adults Aged 75+\",\n             subtitle = \"Data from NHS Borders and NHS Fife\")\n\n\n\n\n\n\n\nStroke Mortality in 2020 in Adults Aged 75+\n\n\nData from NHS Borders and NHS Fife\n\n\nHBName\nSex\nDiagnosis\nNumberOfDeaths\nCrudeRate\nEASR\n\n\n\n\nNHS Borders\nMale\nCerebrovascular Disease\n23\n393.633407\n424.306091\n\n\nNHS Borders\nFemale\nCerebrovascular Disease\n46\n617.947340\n609.187745\n\n\nNHS Fife\nMale\nCerebrovascular Disease\n98\n667.393081\n757.712097\n\n\nNHS Fife\nFemale\nCerebrovascular Disease\n151\n761.818274\n742.539160\n\n\nNHS Borders\nMale\nstroke\n17\n290.946432\n314.000526\n\n\nNHS Borders\nFemale\nstroke\n35\n470.177324\n463.058843\n\n\nNHS Fife\nMale\nstroke\n52\n354.126941\n396.146064\n\n\nNHS Fife\nFemale\nstroke\n87\n438.928409\n428.374773\n\n\nNHS Borders\nMale\nSubaracahnoid Haemorrhage\n1\n17.114496\n16.835017\n\n\nNHS Borders\nFemale\nSubaracahnoid Haemorrhage\n1\n13.433638\n14.804945\n\n\nNHS Fife\nMale\nSubaracahnoid Haemorrhage\n1\n6.810133\n10.797970\n\n\nNHS Fife\nFemale\nSubaracahnoid Haemorrhage\n1\n5.045154\n5.460676\n\n\nNHS Borders\nMale\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\nNHS Borders\nFemale\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\nNHS Fife\nMale\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\nNHS Fife\nFemale\nTIAs and related syndromes\n1\n5.045154\n5.059705\n\n\n\n\n\n\n\nYou can also add spanner column labels if you have multiple similar variables and would like to draw attention to the grouping using tab_spanner(). You can use multiple tab_spanner() layers to add multiple spanners highlighting different groups of columns.\n\nstroke_mortality %&gt;%\n  filter(Year == 2020,\n         AgeGroup == \"75plus years\",\n         HBName %in% c(\"NHS Borders\", \"NHS Fife\")) %&gt;% \n  select(-Year, - AgeGroup) %&gt;%  \n  gt() %&gt;%\n  tab_header(title = \"Stroke Mortality in 2020 in Adults Aged 75+\",\n             subtitle = \"Data from NHS Borders and NHS Fife\") %&gt;% \n  tab_spanner(label = \"Rate per 100k population\",\n              columns = c(CrudeRate, EASR))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStroke Mortality in 2020 in Adults Aged 75+\n\n\nData from NHS Borders and NHS Fife\n\n\nHBName\nSex\nDiagnosis\nNumberOfDeaths\n\nRate per 100k population\n\n\n\nCrudeRate\nEASR\n\n\n\n\nNHS Borders\nMale\nCerebrovascular Disease\n23\n393.633407\n424.306091\n\n\nNHS Borders\nFemale\nCerebrovascular Disease\n46\n617.947340\n609.187745\n\n\nNHS Fife\nMale\nCerebrovascular Disease\n98\n667.393081\n757.712097\n\n\nNHS Fife\nFemale\nCerebrovascular Disease\n151\n761.818274\n742.539160\n\n\nNHS Borders\nMale\nstroke\n17\n290.946432\n314.000526\n\n\nNHS Borders\nFemale\nstroke\n35\n470.177324\n463.058843\n\n\nNHS Fife\nMale\nstroke\n52\n354.126941\n396.146064\n\n\nNHS Fife\nFemale\nstroke\n87\n438.928409\n428.374773\n\n\nNHS Borders\nMale\nSubaracahnoid Haemorrhage\n1\n17.114496\n16.835017\n\n\nNHS Borders\nFemale\nSubaracahnoid Haemorrhage\n1\n13.433638\n14.804945\n\n\nNHS Fife\nMale\nSubaracahnoid Haemorrhage\n1\n6.810133\n10.797970\n\n\nNHS Fife\nFemale\nSubaracahnoid Haemorrhage\n1\n5.045154\n5.460676\n\n\nNHS Borders\nMale\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\nNHS Borders\nFemale\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\nNHS Fife\nMale\nTIAs and related syndromes\n0\n0.000000\n0.000000\n\n\nNHS Fife\nFemale\nTIAs and related syndromes\n1\n5.045154\n5.059705",
    "crumbs": [
      "Content",
      "Week 5: Making Maps and Tables in R",
      "Topic 2: Creating more exciting tables"
    ]
  },
  {
    "objectID": "content/week-5/topic-2.html#bonus---introduction-video-from-gt-package-developers",
    "href": "content/week-5/topic-2.html#bonus---introduction-video-from-gt-package-developers",
    "title": "Creating more exciting tables",
    "section": "Bonus - Introduction video from gt package developers",
    "text": "Bonus - Introduction video from gt package developers\nRich Iannone is one of the developers of the gt package. You can watch this video where he introduces the package and does a live walk through of creating gt tables.",
    "crumbs": [
      "Content",
      "Week 5: Making Maps and Tables in R",
      "Topic 2: Creating more exciting tables"
    ]
  },
  {
    "objectID": "content/week-5/solutions-to-practice-exercises.html",
    "href": "content/week-5/solutions-to-practice-exercises.html",
    "title": "Solutions to practice exercises",
    "section": "",
    "text": "Define a function called add_two_numbers() that takes two arguments, adds them together, and returns the result.\n\nadd_two_numbers &lt;- function(num_1, num_2) {\n  num_1 + num_2\n}\n\nsum_3_4 &lt;- add_two_numbers(3, 4)\nsum_3_4\n\n[1] 7\n\nsum_5_5 &lt;- add_two_numbers(5, 5)\nsum_5_5\n\n[1] 10\n\n\nDefine a function called square_number() that takes a single argument and returns its square.\n\nsquare_number &lt;- function(num) {\n  num**2\n}\n\nsquare_3 &lt;- square_number(3)\nsquare_3\n\n[1] 9\n\nsquare_5 &lt;- square_number(5)\nsquare_5\n\n[1] 25\n\n\nDefine a function called is_even() that takes one argument, checks if the number is even or odd, and returns TRUE for even numbers and FALSE for odd numbers. HINT use the modulo operator %%.\n\nis_even &lt;- function(x) {\n  x %% 2 == 0\n}\n\neven_num_3 &lt;- is_even(3)\neven_num_3\n\n[1] FALSE\n\neven_num_4 &lt;- is_even(4)\neven_num_4\n\n[1] TRUE"
  },
  {
    "objectID": "content/week-4/topic-2.html",
    "href": "content/week-4/topic-2.html",
    "title": "Fine-tuning plots",
    "section": "",
    "text": "The purpose of this topic is to introduce you to some of the finer points of plotting in ggplot2. The icing on the cake. You’ve mastered the basics but perhaps you want more flexibility over the axes ranges, or you want to add in an annotation, or zoom into sections of your plot, or use more aesthetically pleasing colour palettes.\nThe exercises you’ll be working through are included in an R Markdown document which you can download and open in RStudio, similar to the sessions in previous weeks.",
    "crumbs": [
      "Content",
      "Week 4: Analysing and Presenting Data in R",
      "Topic 2: Fine-tuning plots"
    ]
  },
  {
    "objectID": "content/week-4/topic-2.html#overview",
    "href": "content/week-4/topic-2.html#overview",
    "title": "Fine-tuning plots",
    "section": "",
    "text": "The purpose of this topic is to introduce you to some of the finer points of plotting in ggplot2. The icing on the cake. You’ve mastered the basics but perhaps you want more flexibility over the axes ranges, or you want to add in an annotation, or zoom into sections of your plot, or use more aesthetically pleasing colour palettes.\nThe exercises you’ll be working through are included in an R Markdown document which you can download and open in RStudio, similar to the sessions in previous weeks.",
    "crumbs": [
      "Content",
      "Week 4: Analysing and Presenting Data in R",
      "Topic 2: Fine-tuning plots"
    ]
  },
  {
    "objectID": "content/week-4/topic-2.html#plotting-practice",
    "href": "content/week-4/topic-2.html#plotting-practice",
    "title": "Fine-tuning plots",
    "section": "Plotting Practice",
    "text": "Plotting Practice\n\nRemember to begin by creating a new project in RStudio.\nThen download the R Markdown document using this Link, navigate to your newly created project folder, and save.\nHave fun!\n\n\n\n\n\n\n\n\n\n\nartwork by Allison Horst",
    "crumbs": [
      "Content",
      "Week 4: Analysing and Presenting Data in R",
      "Topic 2: Fine-tuning plots"
    ]
  },
  {
    "objectID": "content/week-4/index.html",
    "href": "content/week-4/index.html",
    "title": "Overview",
    "section": "",
    "text": "This week we will be learning more about factors and joins, how to fine-tune our plots, and taking an overview of what an analysis project in R might look like Learning Outcomes",
    "crumbs": [
      "Content",
      "Week 4: Analysing and Presenting Data in R",
      "Overview"
    ]
  },
  {
    "objectID": "content/week-4/index.html#by-the-end-of-the-week-you-will-be-able-to",
    "href": "content/week-4/index.html#by-the-end-of-the-week-you-will-be-able-to",
    "title": "Overview",
    "section": "By the end of the week you will be able to:",
    "text": "By the end of the week you will be able to:\n\nExplain the difference between the data types factor and character\nJoin two datasets together using various joins\nCarry out fine-tuning of your plots using functions in ggplot2\nDemonstrate the steps required to start analysing and reporting on your own data",
    "crumbs": [
      "Content",
      "Week 4: Analysing and Presenting Data in R",
      "Overview"
    ]
  },
  {
    "objectID": "content/week-3/topic-5.html",
    "href": "content/week-3/topic-5.html",
    "title": "Going Further with R Markdown",
    "section": "",
    "text": "You need certain R packages for using R Markdown and also for carrying out this tutorial. The following instructions will help to get you set up and ready to go.\n\nInstall packages\nOpen RStudio and copy-paste the lines below into the console and press Enter:\n\nmarkdown_practice &lt;- c( \n    \"rmarkdown\",\n    \"tinytex\",\n    \"tidyverse\",\n    \"knitr\",\n    \"remotes\",\n    \"gapminder\",\n    \"shiny\",\n    \"flexdashboard\",\n    \"here\") \ninstall.packages(markdown_practice)\nNote: The steps above might take a wee while to run, so don’t worry if it looks like it has momentarily stopped, just give it a minute.\nIf you are asked whether you would like to restart R prior to installing, click on Yes. (If it immediately asks again, press No.)\nIf a message pops up saying you can ignore warning messages, click on OK.\nOnce all packages have been installed, restart R again.\n\n\nInstall Tex\nThen run the following line by copy-pasting into the console and Enter:\ntinytex::install_tinytex()\nIf you are asked whether you would like to restart R prior to installing, click on Yes. (If it immediately asks again, press No).\nWhat’s going on here? We’ve installed the package called  tinytex , but we also needed to run its function install_tinytex() to install Tex on your computer. So the tinytex package helps us install Tex, which is required in order to produce PDF outputs.  \nNote: Don’t worry if you have difficulties with the tinytex line or are getting error messages, this is common. You will still be able to produce outputs in the other formats and can have a go at troubleshooting later. Click here for a link to some troubleshooting suggestions.\nOnce this has run, you might see a message to reopen the R session and check if  tinytex:::is_tinytex()  is TRUE. Run this bit of code in the Console to check and hopefully it will return TRUE.\ntinytex:::is_tinytex() \nNote: Notice the triple colon, this is because it’s an internal variable name.\nHooray! You have installed all the relevant packages.\n\n\nDownload R Markdown Document into an RStudio Project\nFirst, create a new project in RStudio, e.g., you can call it rmd_practice.\nThen download the R Markdown file by right clicking on the link below and choosing, Save Link As, navigate to your newly created project folder, and save as  working_document.Rmd \nYou can also right-click the button and choose “Save Link As…” to download.\n⬇ Download working_document.Rmd\n In RStudio, navigate to your newly created project folder and click to open your newly saved R Markdown document,  working_document.Rmd  (you can use the  …  in the Files tab to browse for your folder).\nOpen it and you are ready to go!\nKnitting to 3 Different Formats We can Knit working_document.Rmd into HTML, PDF, and Word documents, either using Control+Shift+K to Knit the whole Rmd (RMarkdown) document, or the Knit button which has the following options:\n\n\n\nworking_document_kint_to_htms\n\n\nYou can set the output format of working_document.Rmd back to an html document by either:\n\nPressing on the small arrow next to the Knit button and selecting “Knit to HTML” again (this moves html_document to the top of output:)\n\nor\n\nEditing the YAML header and moving html_document to the top:",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 5: Going Further with R Markdown"
    ]
  },
  {
    "objectID": "content/week-3/topic-3.html",
    "href": "content/week-3/topic-3.html",
    "title": "Data Storytelling",
    "section": "",
    "text": "Stories are an effective way of presenting your ideas to other people. Stories have the power to inspire, influence, and change views. Thus, storytelling is one of the most efficient ways of communication. Data storytelling is the practice of building a narrative around data and associated visualisations to convey actionable insights in a commanding and engaging fashion. Data storytelling differs from data visualisation because it requires communicators to consider and provide a broad and holistic view of the idea you are trying to pitch. Storytelling with data first takes the audience, and narrative structure into consideration before any visuals are created. Therefore, before you start the storytelling process, you need to identify:\n\nWho is the audience?\nWhat does the audience care about?\nWhat level of technical detail will the audience expect or appreciate?\nWhat does the audience need to know about the data being presented?\nHow will the narrative be structured to generate the desired action?\nHow does the data being presented drive the decision-making process?\n\nWith these questions answered, you have got the road map in place to identify the technical level of your pitch and what information must be included in your narrative or left out entirely.\n\n\n\n\n\n\n\n\nKeep visuals simple. This draws actionable insights into focus.\nCreate a headline for each slide. This articulates your actionable insights and connects your slides in an easily recognisable pattern.\nUse photographs and icons. This will break up your text, making it more visually pleasing and memorable.\nDo not present superfluous data. Only present the data that directly supports your pitch.\n\nAs with any good story, a data story needs a beginning, a middle, and an end, and some actionable insights. A good data story leverages three key components: data, visuals, and narrative. As data scientists, the data component enables you to obtain and communicate actionable insights. The visual component enables you to identify and translate patterns and trends in data. Finally, the narrative component allows you to tell the data story. All combined, narrative, data and visuals generate data stories that inspire, influence, change views and drive data-driven innovation.\n\n\n\nWhen creating a plot or data visualisation which you will use in data storytelling, there are a few specific type of questions you need to ask which can guide your reader to understand the story you are trying to tell with the data and your visualisation.\n\nWhat is your story? What is the point? This determines your headline or title.\nNote: when creating a data visualisation not intended for data storytelling, the title will be more objective and lacking any interpretation, perhaps explaining what variables within the dataset are shown.\nHow can you emphasise your point in your data visualisation? Answering this question determines your decisions around plot type, specific variables used, colors, highlights, annotations, etc.\nWhat does the final plot show exactly? This is where you add things like a description, any legend or keys, and data sources.\n\nYou can read more about this approach on Lisa Charlotte Muth’s post “What Questions to Ask When Creating Charts: The Attempt of a Data Vis Worflow”\n…..",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 3: Data Storytelling"
    ]
  },
  {
    "objectID": "content/week-3/topic-3.html#what-is-storytelling-with-data-and-why-is-it-important",
    "href": "content/week-3/topic-3.html#what-is-storytelling-with-data-and-why-is-it-important",
    "title": "Data Storytelling",
    "section": "",
    "text": "Stories are an effective way of presenting your ideas to other people. Stories have the power to inspire, influence, and change views. Thus, storytelling is one of the most efficient ways of communication. Data storytelling is the practice of building a narrative around data and associated visualisations to convey actionable insights in a commanding and engaging fashion. Data storytelling differs from data visualisation because it requires communicators to consider and provide a broad and holistic view of the idea you are trying to pitch. Storytelling with data first takes the audience, and narrative structure into consideration before any visuals are created. Therefore, before you start the storytelling process, you need to identify:\n\nWho is the audience?\nWhat does the audience care about?\nWhat level of technical detail will the audience expect or appreciate?\nWhat does the audience need to know about the data being presented?\nHow will the narrative be structured to generate the desired action?\nHow does the data being presented drive the decision-making process?\n\nWith these questions answered, you have got the road map in place to identify the technical level of your pitch and what information must be included in your narrative or left out entirely.\n\n\n\n\n\n\n\n\nKeep visuals simple. This draws actionable insights into focus.\nCreate a headline for each slide. This articulates your actionable insights and connects your slides in an easily recognisable pattern.\nUse photographs and icons. This will break up your text, making it more visually pleasing and memorable.\nDo not present superfluous data. Only present the data that directly supports your pitch.\n\nAs with any good story, a data story needs a beginning, a middle, and an end, and some actionable insights. A good data story leverages three key components: data, visuals, and narrative. As data scientists, the data component enables you to obtain and communicate actionable insights. The visual component enables you to identify and translate patterns and trends in data. Finally, the narrative component allows you to tell the data story. All combined, narrative, data and visuals generate data stories that inspire, influence, change views and drive data-driven innovation.\n\n\n\nWhen creating a plot or data visualisation which you will use in data storytelling, there are a few specific type of questions you need to ask which can guide your reader to understand the story you are trying to tell with the data and your visualisation.\n\nWhat is your story? What is the point? This determines your headline or title.\nNote: when creating a data visualisation not intended for data storytelling, the title will be more objective and lacking any interpretation, perhaps explaining what variables within the dataset are shown.\nHow can you emphasise your point in your data visualisation? Answering this question determines your decisions around plot type, specific variables used, colors, highlights, annotations, etc.\nWhat does the final plot show exactly? This is where you add things like a description, any legend or keys, and data sources.\n\nYou can read more about this approach on Lisa Charlotte Muth’s post “What Questions to Ask When Creating Charts: The Attempt of a Data Vis Worflow”\n…..",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 3: Data Storytelling"
    ]
  },
  {
    "objectID": "content/week-3/topic-3.html#examples-of-good-data-storytelling-in-health-and-social-care",
    "href": "content/week-3/topic-3.html#examples-of-good-data-storytelling-in-health-and-social-care",
    "title": "Data Storytelling",
    "section": "Examples of good data storytelling in health and social care",
    "text": "Examples of good data storytelling in health and social care\n\nOxfam Ireland\nOxfam Ireland is part of a global movement for change, one that empowers people to create a future that is secure, just and free from poverty. Oxfam Ireland is a not-for-profit organisation (NGO) with charitable status and is a public benefit entity. Oxfam Ireland not only collects data, but uses it to tell a story of both urgent need and impressive impact. In their annual impact report for stakeholders and the public, the team at Oxfam showcase their impact using data-driven maps and infographics, presented in an sophisticated and engaging way:\nView Oxfam Impact Report ↗\n\n\nRadio NZ\nRadio NZ is New Zealand’s public broadcaster providing comprehensive news and current affairs, specialist audio features and documentaries. One such story came from a team in Radio NZ who published and continued to update a timeline on the impact of COVID-19 in Aotearoa New Zealand. Rather than use scroll-based animation, the team embedded rich visualisations from data visualisation platform Tableau resulting in a spectacular, well-told story, interspersed with striking interactive charts and maps",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 3: Data Storytelling"
    ]
  },
  {
    "objectID": "content/week-3/topic-1.html#what-is-data-visualisation",
    "href": "content/week-3/topic-1.html#what-is-data-visualisation",
    "title": "Data Visualisation",
    "section": "What is data visualisation?",
    "text": "What is data visualisation?\nData visualisation is an interdisciplinary field that deals with the visual representation of data. The primary goal of data visualisation is to make it easier to assess smaller facets of the larger picture, such as identifying outliers, patterns, and trends in complex data.\nData visualisation is one of the fundamental steps of the data science process; it happens after data has been collected and processed, and before and after data is modelled. Data must be visualised and interpreted for conclusions to be made. In health and social care, data visualisation is essential for analysing massive amounts of service users’ and administrative data and for data-driven decision-making within and across health and social care organisations.",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 1: Data Visualisation"
    ]
  },
  {
    "objectID": "content/week-3/topic-1.html#data-visualisation-provides",
    "href": "content/week-3/topic-1.html#data-visualisation-provides",
    "title": "Data Visualisation",
    "section": "Data visualisation provides:",
    "text": "Data visualisation provides:\nan effective means of communicating data to non-technical (and technical) audiences using visual information a way to better engage audiences with crucial and understandable information a mechanism to present data so that actionable data can be absorbed quickly for greater insights and faster decision-making Different types of data visualisation\nCharts can be considered a catch-all term for data visualisations of all varieties, such as those shown above.\nFigures are visuals that use varied forms and dimensions, including shapes, colours, sizes, and locations. Graphs are a common type of figure where information is presented in two dimensions: the x-axis (horizontal) and y-axis (vertical). Figures which are not graphs may be referred to as plots and include maps and Venn diagrams.\nWhile tables tend not to be as colourful or eye-catching as other visuals, they can be used to clearly present large quantities of summary information. They are particularly useful when precision is required, such as when determining whether one value is higher than another.\n\n\n\n\n\nAn infographic is a combination of text, tables, and/or figures, is explicitly designed to guide the audience to a particular conclusion (the art known as graphical storytelling). They are often used as posters in marketing and research outreach to summarise the key themes of a suite of analyses. On the other hand, dashboards present data as objectively as possible to allow the audience to overview information at a glance.",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 1: Data Visualisation"
    ]
  },
  {
    "objectID": "content/week-3/topic-1.html#data-visualisation-tools-in-data-science",
    "href": "content/week-3/topic-1.html#data-visualisation-tools-in-data-science",
    "title": "Data Visualisation",
    "section": "Data visualisation tools in data science",
    "text": "Data visualisation tools in data science\nNot meant to be an exhaustive list in any way, some of the most popular data visualisation tools are outlined below.\n\nMicrosoft Excel\nThe most simple tool for data visualisation for many of you will be using a piece of software you should all be familiar with: Microsoft Excel. Excel originally offered nine simple plot types: bar graphs, line graphs, pie charts, hierarchy charts, histograms, scatter plots, waterfall plots (usually used in financial analyses), maps, and tables (or pivot charts). Newer versions of Excel (2016 onward) offer a wider variety of chart types and templates.\n\n\nTableau\nTableau is a visual analytics platform that has drawn real traction in the last few years. Tableau has become very popular in industry because it is intuitive with a point-and-click interface and looks professional. Although the cost is relatively high, there is a 1-year free trial available for students, so this is a great time to get familiar with the software if you are interested. Here are a couple of graphs produced in under ten minutes. Tableau also has a gallery to browse, including Viz of the day.\n\n\nPython\nPython is a free, open-source programming language that can be used to conduct more complex analyses than Excel, and to create more customised tables and figures. matplotlib and seaborn are packages within Python that are designed to create nice-looking plots using default settings to minimise the necessary code. The matplotlib examples gallery and seaborn examples gallery show some of the plots you can make using the packages and provide the code to allow you to replicate it. Additionally, the Python Graph Gallery is an excellent resource with a collection of 100s of charts made with Python, as well as sections dedicated to more general topics like matplotlib or seaborn packages.\n\n\nR\nR, like Python, is another free, open-source programming language with great graphing abilities and will be the primary language used for this course. The package ggplot2 can be used to make most plot types within the core package. There are over 136 extension packages which have been developed to date, meaning the plot options are essentially endless! R Graph Gallery and the newer R Charts are both great websites to get some inspiration for what is possible with R and with ggplot2 and show both examples and code for various plots.",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 1: Data Visualisation"
    ]
  },
  {
    "objectID": "content/week-3/topic-1.html#examples-of-good-data-visualisation-in-health-and-social-care",
    "href": "content/week-3/topic-1.html#examples-of-good-data-visualisation-in-health-and-social-care",
    "title": "Data Visualisation",
    "section": "Examples of good data visualisation in health and social care",
    "text": "Examples of good data visualisation in health and social care\nData visualisation is storytelling with a purpose. Good data visualisation tells a story by removing the noise from data and highlighting valuable information: it’s a delicate balancing act between form and function. The plainest graph could be too dull to catch any notice, or the most beautiful visualisation could utterly fail because it conveys too much information or fails to make a point. For good data visualisation, data and visuals need to work in tandem. Data visualisation needs to provide context, draw attention to key insights, and support the decision-making process. However, the real magic happens when data visualisation is driven by storytelling.\n\nLouisiana vaccine rate versus infection\nTo facilitate timely and informative research during the COVID-19 pandemic, vast quantities of data were made freely and easily available for download. While this data was primarily used by news crews, policy and decision-makers, and researchers to provide the necessary context to a rapidly evolving situation, it was also used by many amateur data analysts to hone their skills and share their insights.\n\n\n\n\n\nThis graph made by Reddit user zkanalog highlights the states with low vaccination rates and how much of an outlier Florida was, with its particularly large outbreak (almost 1% of the state population infected in that month alone). There are a few factors that make this a great graph. First, the graph is really well labelled: the axes, the title (typo aside), the captions, and even the annotations on specific points of the scatter plot. The data source is also clearly explained in detail in the caption on the left. While it has not been listed in any of the captions, which is not ideal, the use of colour to mirror the x-axis, and size to mirror the y-axis, adds some visual appeal to the plot. Second, the trend line has been appropriately fitted: a linear (straight line) would not be right here as neither the vaccination rate nor infection rate could ever go below zero.\n\n\nThe infant mortality rate in Japan\nJapan is one of the countries with the lowest infant mortality rate in the world.  This graph shows the mortality rate by year from 1899 to 2017, with annotations for the peak of the Spanish flu (which we can see did not have a massive effect on infant mortality) and highlights the cause of the missingness during the second world war. While I would typically argue that rates should not be represented by bars (ideally used exclusively for counts), it makes the plot very easy to read and visually appealing. The biggest strength of this plot is simplicity – it is presenting a lot of data but only contains two dimensions (year and rate) which mean that patterns can be easily identified and inferences can be made quickly.\n\n\nBe fruitful and multiply: Association of China’s universal two-child policy with birth rates\nThis infographic published in the British Medical Journal describes factors influencing birth rates over time in China, with contextual annotations such as the date of the introduction of the two-child policy, the Chinese zodiac year, and annual celebrations. The infographic displays text, a combination of line and area graphs and a selection of pie charts. There is a good mix between white and colour, the font size is appropriate, and the text is well-placed.",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 1: Data Visualisation"
    ]
  },
  {
    "objectID": "content/week-3/git-from-rstudio.html",
    "href": "content/week-3/git-from-rstudio.html",
    "title": "Creating a Version-Controlled Project in RStudio",
    "section": "",
    "text": "This guide demonstrates how to create a version-controlled project in RStudio using Git. Each step is paired with an image that shows precisely where to click and what to expect in the interface.\n\n\nOpen RStudio and begin by creating a new project:\n\nClick File &gt; New Project…\nAlternatively, you can click the project dropdown at the top right of your RStudio window.\n\n\n\n\n\n\n\nFigure 1: Start a new project\n\n\n\n\n\n\n\nSelect “New Directory” to start a fresh project folder.\n\n\n\n\n\n\n\nFigure 2: Choose New Directory\n\n\n\n\n\n\n\nSelect “New Project” as the project type.\n\n\n\n\n\n\n\nFigure 3: Choose R Project\n\n\n\n\n\n\n\nEnter a Directory Name.\n\nMake sure you give the project a meaningful name.\nE.g. something that will make it easy to remember what the project does.\n\nChoose where to save the project.\n\nA good place for the purposes of this course might be in Documents/ or a new directory therein perhaps called, something like, Documents/data-science-tutorials/\n\nTick “Create a git repository”.\n\nThis step is important.\nIf you forget to click this, everything else in this doc will fail 😱\nClick Create Project.\n\nYour new Git enabled project will open automatically.\n\n\n\n\n\n\n\n\n\nFigure 4: Set directory and enable Git\n\n\n\n\n\n\nLooks like any other, except there will be a new file that was never present in any of your previous projects: .gitignore. You should also see a Git interface tab…\n\n\n\nAfter creating the project, the Git tab should appear somewhere. If your RStudio is still in its default configuration, that is usually in the top right pane.\n\n\n\n\n\n\nFigure 5: Git pane appears\n\n\n\nIf you have changed things around you may have to find it elsewhere\n\nIf you don’t see it anywhere, go to Tools &gt; Global Options &gt; Git/SVN, and tick the box that says “Enable version control interface”.\n\n\n\n\n\nSince your project is brand new, you will have only two files.\n\n.gitignore\nproj-name.Rproj\n\nIn the Git tab, tick both files to “stage” it (Figure 5).\n\nThis means tag it to tell Git you want to keep a track of its version history.\n\nClick Commit.\n\nStaging the files is the first step of telling git you want to track changes to it. Making a commit means actually tracking it. It is necessary to provide a message that describes the cntent of the commit.\n\n\n\n\n\n\nFigure 6: Write commit message\n\n\n\nIt is common to leave a message like “First commit” for the first commit, but not a good pattern to follow beyond that (“One-hundred and twenty-first commit” anyone!?) Good practice for writing commit messages is to complete the following sentance…\n\nThis commit will…\n\nSo you might have a commit message like\n\nUpdate tutorial script with changes to answer questions 4, 5 an 6\n\n\n\n\n\n\n\nGit Gud\n\n\n\n\n\nHonestly, don’t be lax with the commit messages. Conscientious messages will pay you back!\n\n\n\n\n\n\nIf Git or RStudionis not correctly configured, e.g. missing user name/email or missing credetials, the commit will fail.\n\nUse the Terminal to run:\n\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"you@example.com\"\n\n\n\n\n\n\nFigure 7: Commit failed\n\n\n\n\n\n\nOnce Git is configured, repeat the commit:\n\nStage the file.\nAdd a commit message.\nClick Commit again.\n\n \n\n\n\nYou can check the commit history for your project by clicking History in the Git tab to view previous commits.\n\n\n\n\n\n\nFigure 8: Git commit history\n\n\n\nAt the moment you will see only the single commit you have made, but as you add more commits, you will see them all with a summary of your commit messages allowing an at a glance overview of the changes you have made in your repo.\n\n\n\n\n\n\nSummary\n\n\n\n\n\nIn this tutorial we have:\n\nCreated a new RStudio project.\nEnabled Git version control.\nMade and reviewed our first commits.\n\nFollow this precess each week in the course, and for setting up all your future projects."
  },
  {
    "objectID": "content/week-3/git-from-rstudio.html#step-1-start-a-new-project",
    "href": "content/week-3/git-from-rstudio.html#step-1-start-a-new-project",
    "title": "Creating a Version-Controlled Project in RStudio",
    "section": "",
    "text": "Open RStudio and begin by creating a new project:\n\nClick File &gt; New Project…\nAlternatively, you can click the project dropdown at the top right of your RStudio window.\n\n\n\n\n\n\n\nFigure 1: Start a new project"
  },
  {
    "objectID": "content/week-3/git-from-rstudio.html#step-2-choose-to-create-a-project-in-a-new-directory",
    "href": "content/week-3/git-from-rstudio.html#step-2-choose-to-create-a-project-in-a-new-directory",
    "title": "Creating a Version-Controlled Project in RStudio",
    "section": "",
    "text": "Select “New Directory” to start a fresh project folder.\n\n\n\n\n\n\n\nFigure 2: Choose New Directory"
  },
  {
    "objectID": "content/week-3/git-from-rstudio.html#step-3-choose-project-type",
    "href": "content/week-3/git-from-rstudio.html#step-3-choose-project-type",
    "title": "Creating a Version-Controlled Project in RStudio",
    "section": "",
    "text": "Select “New Project” as the project type.\n\n\n\n\n\n\n\nFigure 3: Choose R Project"
  },
  {
    "objectID": "content/week-3/git-from-rstudio.html#step-4-choose-project-directory-and-enable-git",
    "href": "content/week-3/git-from-rstudio.html#step-4-choose-project-directory-and-enable-git",
    "title": "Creating a Version-Controlled Project in RStudio",
    "section": "",
    "text": "Enter a Directory Name.\n\nMake sure you give the project a meaningful name.\nE.g. something that will make it easy to remember what the project does.\n\nChoose where to save the project.\n\nA good place for the purposes of this course might be in Documents/ or a new directory therein perhaps called, something like, Documents/data-science-tutorials/\n\nTick “Create a git repository”.\n\nThis step is important.\nIf you forget to click this, everything else in this doc will fail 😱\nClick Create Project.\n\nYour new Git enabled project will open automatically.\n\n\n\n\n\n\n\n\n\nFigure 4: Set directory and enable Git"
  },
  {
    "objectID": "content/week-3/git-from-rstudio.html#your-new-git-enabled-project",
    "href": "content/week-3/git-from-rstudio.html#your-new-git-enabled-project",
    "title": "Creating a Version-Controlled Project in RStudio",
    "section": "",
    "text": "Looks like any other, except there will be a new file that was never present in any of your previous projects: .gitignore. You should also see a Git interface tab…"
  },
  {
    "objectID": "content/week-3/git-from-rstudio.html#step-5-confirm-git-pane-appears",
    "href": "content/week-3/git-from-rstudio.html#step-5-confirm-git-pane-appears",
    "title": "Creating a Version-Controlled Project in RStudio",
    "section": "",
    "text": "After creating the project, the Git tab should appear somewhere. If your RStudio is still in its default configuration, that is usually in the top right pane.\n\n\n\n\n\n\nFigure 5: Git pane appears\n\n\n\nIf you have changed things around you may have to find it elsewhere\n\nIf you don’t see it anywhere, go to Tools &gt; Global Options &gt; Git/SVN, and tick the box that says “Enable version control interface”."
  },
  {
    "objectID": "content/week-3/git-from-rstudio.html#step-6-make-a-commit",
    "href": "content/week-3/git-from-rstudio.html#step-6-make-a-commit",
    "title": "Creating a Version-Controlled Project in RStudio",
    "section": "",
    "text": "Since your project is brand new, you will have only two files.\n\n.gitignore\nproj-name.Rproj\n\nIn the Git tab, tick both files to “stage” it (Figure 5).\n\nThis means tag it to tell Git you want to keep a track of its version history.\n\nClick Commit.\n\nStaging the files is the first step of telling git you want to track changes to it. Making a commit means actually tracking it. It is necessary to provide a message that describes the cntent of the commit.\n\n\n\n\n\n\nFigure 6: Write commit message\n\n\n\nIt is common to leave a message like “First commit” for the first commit, but not a good pattern to follow beyond that (“One-hundred and twenty-first commit” anyone!?) Good practice for writing commit messages is to complete the following sentance…\n\nThis commit will…\n\nSo you might have a commit message like\n\nUpdate tutorial script with changes to answer questions 4, 5 an 6\n\n\n\n\n\n\n\nGit Gud\n\n\n\n\n\nHonestly, don’t be lax with the commit messages. Conscientious messages will pay you back!"
  },
  {
    "objectID": "content/week-3/git-from-rstudio.html#step-7-handle-commit-errors-if-any",
    "href": "content/week-3/git-from-rstudio.html#step-7-handle-commit-errors-if-any",
    "title": "Creating a Version-Controlled Project in RStudio",
    "section": "",
    "text": "If Git or RStudionis not correctly configured, e.g. missing user name/email or missing credetials, the commit will fail.\n\nUse the Terminal to run:\n\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"you@example.com\"\n\n\n\n\n\n\nFigure 7: Commit failed"
  },
  {
    "objectID": "content/week-3/git-from-rstudio.html#step-8-commit-successfully",
    "href": "content/week-3/git-from-rstudio.html#step-8-commit-successfully",
    "title": "Creating a Version-Controlled Project in RStudio",
    "section": "",
    "text": "Once Git is configured, repeat the commit:\n\nStage the file.\nAdd a commit message.\nClick Commit again."
  },
  {
    "objectID": "content/week-3/git-from-rstudio.html#step-9-view-git-history",
    "href": "content/week-3/git-from-rstudio.html#step-9-view-git-history",
    "title": "Creating a Version-Controlled Project in RStudio",
    "section": "",
    "text": "You can check the commit history for your project by clicking History in the Git tab to view previous commits.\n\n\n\n\n\n\nFigure 8: Git commit history\n\n\n\nAt the moment you will see only the single commit you have made, but as you add more commits, you will see them all with a summary of your commit messages allowing an at a glance overview of the changes you have made in your repo.\n\n\n\n\n\n\nSummary\n\n\n\n\n\nIn this tutorial we have:\n\nCreated a new RStudio project.\nEnabled Git version control.\nMade and reviewed our first commits.\n\nFollow this precess each week in the course, and for setting up all your future projects."
  },
  {
    "objectID": "content/week-2/topic-3.html",
    "href": "content/week-2/topic-3.html",
    "title": "How to explore data?",
    "section": "",
    "text": "In this topic, we are focusing on getting our data ready for analysis and reporting. We will cover how to:\n\nInterrogate and check the data for inaccuracies (explore)\nChange the format and transform the data (tidy)\nVisualise and clean the data (prepare).\n\nYou might also find this type of task referred to as data wrangling. You can expect to spend as much as 80% of your time on this part of the process, sometimes more!\n\nartwork by Allison Horst\nWe are continuing to use one of the datasets available from Public Health Scotland:\n\nCancelled Planned Operations by Health Board\n\nWe’re also going to be using the Global Burden of Disease (GBD, or gbd in lowercase), which includes information on the causes of death globally. What we will be using is the number of deaths for the three broad cause categories - Injuries, Communicable diseases, and Non-communicable diseases. But the project and its freely available datasets also include much more detailed breakdowns, both in the types of diseases, countries, as well as different metrics. It is freely available from here: \n\nGlobal Burden of Disease data\n\nHere are the functions we’ll be covering in the topic videos:\n\nbase::library() we’re already familiar with for loading packages\nbase::names() allows us to view all the column names or variables in our dataet\ndplyr::distinct() for exploring the unique values in our columns or variables\ndplyr::filter() allows us to subset data by filtering out rows based on variable criteria\ndplyr::select() allows us to subset data by selecting specific columns or variables\ndplyr::glimpse() provides information on dimensions, data types and a small preview\ndplyr::arrange() & dplyr::desc() lets you sort your data (default is ascending order)\ntidyr::pivot_longer() lengthens the data, increasing rows and decreasing columns\ntidyr::pivot_wider() widens the data, increasing columns and decreasing rows\ndplyr::mutate() for changing or adding columns\nstringr::str_replace() for finding and replacing strings (text) in your data\nggplot2::ggplot() & ggplot2::aes() lets R know you are about to make a plot and what you’re going to plot\nggplot2::geom_point() lets R know you want to make a scatter plot\nbase::replace() for finding and replacing values in your data\n\n\n\nartwork by Allison Horst\n\nExplore Data\nThe main package we will be using to explore the data is dplyr, which is part of the tidyverse. Watch the following video to learn about some of the functions for carrying out your initial exploration of your data.\nIf you want to recreate the following session on your own computer, you can download the cancelled_operations project folder here: cancelled_operations.zip\nNote: To open the project, make sure to “unzip” or “extract all” from the folder, before clicking on the cancelled_operations.Rproj file to open the project in RStudio.\nAlternatively, here is the individual cancelled_messy file: phs_cancelled_messy.Rda Note: To load the data, find the file on your computer, double click to open (select RStudio if prompted, and select “yes” to loading into your Global Environment).\n\n\n\nTidy Data\nWatch the following video to learn about some of the functions for reshaping your data so that you can get it into a format which makes it easy to plot.\nIf you want to recreate the following session on your own computer, you can download the cancelled_operations project folder here: cancelled_operations.zip Note: To open the project, make sure to “unzip” or “extract all” from the folder, before clicking on the cancelled_operations.Rproj file to open the project in RStudio.\nAlternatively, here is the individual cancelled_messy file: phs_cancelled_messy.Rda Note: To load the data, find the file on your computer, double click to open (select RStudio if prompted, and select “yes” to loading into your Global Environment).\n\nDownload the video transcript link here\nAs described in the video, and visualized below, in a tidy dataset (a) each variable forms a column, (b) each observation forms a row, and (c) and each cell is a single measurement.\n\n\n\n\n\nartwork by Allison Horst\n\n\nPrepare Data\nIn this video we’re going to take a first look at visualising our data as a way of exploring it and looking for outliers, or mistakes.\nIf you want to recreate the following session on your own computer, you can download the cancelled_operations project folder here: cancelled_operations.zip Note: To open the project, make sure to “unzip” or “extract all” from the folder, before clicking on the cancelled_operations.Rproj file to open the project in RStudio.\nAlternatively, is the individual cancelled_tidy file: phs_cancelled_tidy.Rda Note: To load the data, find the file on your computer, double click to open (select RStudio if prompted, and select “yes” to loading into your Global Environment).\n\nDownload the video transcript link here\n\n\nOther Useful Functions and Operations\nHere are some other useful functions and operators you might find useful when wrangling with data.\n\n\nThe c() combine function and the %in% operator\nThe combine function, as you may well have guessed, is useful for combining several values: c(). We do this by listing the different elements between the brackets, separated by a comma. Text has to be within inverted commas.\nThe c() function crops up all over the place, for example, in functions where we want to list multiple values as the input to one argument. But it is particularly useful for filtering data.\nWhen combined with the special type of operator, the %in% operator, it can save us extra typing and make our code more readable. We use the %in% operator to identify if a value or set of values appears within another set of values.\nLet’s quickly recap how we might filter data if we didn’t know about %in% operator.\nIf we were interested in filtering our GBD dataset to only include “Communicable” and “Non-communicable” diseases, we might use the following code:\n\ngbd_full %&gt;%\n  # also filtering for a single year to keep the result concise \n  filter(year == 1990) %&gt;%\n  filter(cause == \"Communicable diseases\" | cause == \"Non-communicable diseases\")\n\nWhich produces the following output:\n\nThis works perfectly well, but what if we had a column where we wanted to look at data on a whole list of values. Perhaps we have a column with Health Boards and we want to look at data on all those on the East of Scotland. We would have to repeat | HBName == for each value.\nThis is where the %in% operator and c() function can save us some typing, see the following code which gives exactly the same output as our previous code:\n\ngbd_full %&gt;%\n  # also filtering for a single year to keep the result concise \n  filter(year == 1990) %&gt;%\n  filter(cause %in% c(\"Communicable diseases\", \"Non-communicable diseases\"))\n\nWhich produces the following output:\n\n\n\nThe paste() function for joining values together\nWe’ve just explored the c() function which allows values to be “combined” but they are not actually joined to make a new variable, they remain separate.\nIf we want to make a new character variable by joining other variables together, we need to use the paste()function.\nThe paste() function allows us to paste several variables (e.g. words, numbers, or dates) into one character variable or sentence.\nFor example, we might want to create a new column with text which could be used as a label:\n\n# create a new column\ngbd_full %&gt;%\n  mutate(year_label_paste = paste(\"Year is\", year))\n\nWhich produces the following output:\n\nIn the paste() function we need to specify each element we would like to combine, separated by a comma. By default, the separation between the elements is a space, but we can change this using the sep = argument within the paste function.\nNote: For more informtion on this, try looking the function up in the help tab in RStudio, either by pressing F1 when the cursor is within the function name, or by search in the Help tab search box.\n\n\nThe pull() function or the extract operator\nWe’ve seen how to use the select() function to select a specific column or columns from a dataset. The type of object we get out is the same as the type we put in, just smaller, so if we pipe a tibble (the name for a tidyverse data frame) into select(), we get a smaller tibble out.\nSometimes however we might need our output to be in a slightly different format, we might need it to be a vector, which we can think of as a simple list of values.\nWe can get a vector, either by using the base R extract operator $, or we can use the pull() function from the dplyr package.\nLet’s see how both of these work:\n\n# $ operator \ngbd_full$deaths_millions \n \n# pull() function \ngbd_full %&gt;%\n  pull(deaths_millions)\n\nR will give us all the data for that variable - as seen in the following output:\n\n\n\nThe round() function\nNot surprisingly, we can use the round() function from base R to round variables to create integers.\nFor example, in the GBD dataset we might want to round the total deaths column (deaths_millions) to no decimal places.\n\n# round to whole number \nround(gbd_full$deaths_millions) \n\nWhich produces the following output:\n\nNote: Here we use the $ sign to specify which column from the gbd_full dataset we wanted. We did not pipe (%&gt;%) gbd_full into round() because the first argument in this base R function is looking for a numeric vector (a list of numbers) rather than the name of a tibble or data frame, which is what we are more used to seeing in the tidyverse functions.\n\n\nOther Useful Packages\n\nlubridate\nDates can often be a bit of a nightmare. They can come in many different formats and are prone to error and so can be awkward to work with. In the “Import Excel” video in the last topic we saw that R wasn’t able to recognise the date column when we used the Import Wizard.\nThankfully, like many problems in R, there is a package for just this issue and it’s called the lubridate package. It’s a tidyverse package which makes it easy to work with dates.\nHere is a quick taster, you can:\n\nparse (read in) a very wide range of formats with the ymd() function\nextract the relevant parts from your dates with year() and month() functions\neasily calculate time intervals with the interval() function\n\n\n\nartwork by Allison Horst\n\n\njanitor\nCleaning and tidying our data takes up a considerable amount of valuable time, but there’s one function which can help to make our lives a little bit easier when we’re going round the bend trying to fix badly named variables. It’s the clean_names() function from the janitor package.\nThe clean_names() function converts your variables into snake case style. This just means that it replaces all capital letters, punctuation and spaces with underscores to make the variables easier to work with because R doesn’t like your column headers to have spaces in them or weird characters and symbols.\n\nartwork by Allison Horst",
    "crumbs": [
      "Content",
      "Week 2: Data Tidying and Wrangling",
      "Topic 3: Exploring Data"
    ]
  },
  {
    "objectID": "content/week-1/tutorial.html",
    "href": "content/week-1/tutorial.html",
    "title": "Tutorial Materials",
    "section": "",
    "text": "During this course, it’s recommended that you create a directory somewhere on your computer to store all the tutorial materials. You can call it whatever you like, but for the sake of simplicity, we recommend you create the directory on your Desktop and call it data_science (no spaces in the name!).",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Tutorial materials"
    ]
  },
  {
    "objectID": "content/week-1/tutorial.html#thursday",
    "href": "content/week-1/tutorial.html#thursday",
    "title": "Tutorial Materials",
    "section": "Thursday",
    "text": "Thursday\n\n1. Bingo!\n\n\n2. Introduction (introduction.pptx)\n\n\n3. Assessment brief\n\n\n4. Pair programming demo\n\n\n5. Quiz\n\n\n6. BRFSS Analysis (Behavioral Risk Factor Surveillance System)\nThe goals of this exercise are:\n\nPractice pair programming\nSolve some problems using your current R knowledge (no Tidyverse!)\n\nCreate a New project called brfss_analysis in your data_science directory. Save the starter file provided below in brfss_analysis.\nRemember, you can use the R documentation to help you solve the exercises or Google it!\n\n\nWarning: package 'downloadthis' was built under R version 4.4.3\n\n\n\n Download the starter file",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Tutorial materials"
    ]
  },
  {
    "objectID": "content/week-1/tutorial.html#friday",
    "href": "content/week-1/tutorial.html#friday",
    "title": "Tutorial Materials",
    "section": "Friday",
    "text": "Friday\n\n1. Updating R, RStudio, installing Tidyverse\n\n\n2. RMarkdown Exercise\n\n\n\n Download the starter file",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Tutorial materials"
    ]
  },
  {
    "objectID": "content/week-1/topic-5.html",
    "href": "content/week-1/topic-5.html",
    "title": "Topic 5: Guide to R Markdown",
    "section": "",
    "text": "R Markdown allows us to turn the findings of our analysis in R into well presented and high quality reports in various formats. In this topic you will learn to navigate the different elements in an R Markdown file and we’ll introduce you to some of the main features which make these documents so attractive.",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 5: Guide to R Markdown"
    ]
  },
  {
    "objectID": "content/week-1/topic-5.html#what-is-r-markdown",
    "href": "content/week-1/topic-5.html#what-is-r-markdown",
    "title": "Topic 5: Guide to R Markdown",
    "section": "What is R Markdown?",
    "text": "What is R Markdown?\nR Markdown is a file format and a tool which combines both R code and Markdown, meaning you can produce documents with text and code fully integrated. We can use R Markdown to create documents which are fully reproducible and easy to share.\n\n\n\nReproducibility court\n\n\nartwork by Allison Horst\nSo far we have been working entirely with R Scripts, so why would we want to switch to using R Markdown instead?\nR Markdown contains lots of “mini” R scripts, called code chunks, but in combining these with text and commentary in Markdown we can:\n\nCommunicate our analysis and conclusions more effectively to those who are not interested in the code\nCollaborate more easily with other R users who are interested in the workings of the R code\nCapture not just our analysis but our ideas and thoughts, just like using a notebook",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 5: Guide to R Markdown"
    ]
  },
  {
    "objectID": "content/week-1/topic-5.html#i-know-what-r-is-but-what-exactly-is-markdown",
    "href": "content/week-1/topic-5.html#i-know-what-r-is-but-what-exactly-is-markdown",
    "title": "Topic 5: Guide to R Markdown",
    "section": "I know what R is but what exactly is Markdown?",
    "text": "I know what R is but what exactly is Markdown?\nMarkdown is a lightweight markup language which we can use to add formatting and styling easily to plaintext documents (e.g. **bold**, *italic*). It is one of the most popular markup languages because it was designed to be “easy-to-read, easy-to-write”.\nMarkdown is very powerful because it enables us to convert “plaintext” into various formats such as:\n\nPDF\nWord\nHTML\n\n… at the click of a button!\nAnd then when we add in R to turn it into an R Markdown document, it becomes even more powerful still.",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 5: Guide to R Markdown"
    ]
  },
  {
    "objectID": "content/week-1/topic-5.html#dissecting-our-document",
    "href": "content/week-1/topic-5.html#dissecting-our-document",
    "title": "Topic 5: Guide to R Markdown",
    "section": "Dissecting our Document",
    "text": "Dissecting our Document\nThere are three basic elements which make up our R Markdown document:\n\nthe metadata in YAML\nthe code chunks in R\nthe text in Markdown\n\nAnd when combined, these allow us to produce the outputs in PDF or HTML or Word… or many more!",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 5: Guide to R Markdown"
    ]
  },
  {
    "objectID": "content/week-1/topic-5.html#hang-on-whats-yaml",
    "href": "content/week-1/topic-5.html#hang-on-whats-yaml",
    "title": "Topic 5: Guide to R Markdown",
    "section": "Hang on, what’s YAML?",
    "text": "Hang on, what’s YAML?\nIs this another language you’ve sneaked in, I hear you say?\nNo need to panic, YAML is only used in a small section of our document, in the header. But yes, it is another language and Wikipedia helpfully informs us that:\n“Originally YAML was said to mean Yet Another Markup Language, referencing its purpose as a markup language with the yet another construct, but it was then repurposed as YAML Ain’t Markup Language, a recursive acronym, to distinguish its purpose as data-oriented, rather than document markup.”\nThe YAML helps us to create meaningful metadata. This is where we can add in options for how we want our outputs to be displayed.\nFor example we can specify whether we want a table of contents (toc) to be displayed by including this in our YAML:\n\nThis is also where we can specify that we want our table of contents to be floating (this means it is always visible on the screen), where we can add themes, include instructions for a “code download” button to be added, and much more:\n\nAs well as editing the YAML directly we can control what appears in here by going to the settings cog at the top of the document and selecting Output Options…\n\n\nHere we can also see options to add in section numbers and set the default figure size. When we change these settings we can see that the code in the YAML is automatically updated.\nBe aware that not all output options are available for all formats.",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 5: Guide to R Markdown"
    ]
  },
  {
    "objectID": "content/week-1/topic-5.html#where-does-the-r-code-go",
    "href": "content/week-1/topic-5.html#where-does-the-r-code-go",
    "title": "Topic 5: Guide to R Markdown",
    "section": "Where does the R code go?",
    "text": "Where does the R code go?\nThe R code is interspersed throughout our document in small sections called code chunks which look like this:\n\nThis back ticks ``` and curly braces {r} combination lets Markdown know it’s going to run some R code. Most of your R code will appear between two sets of back ticks like the example above.\nWatch out for unpaired or rogue sets of back ticks as these can cause errors or strange behaviour and can sometimes be hard to spot.\nWe can run each individual chunk of code by clicking on the green arrow at the top-right of the code chunk. However, the usual Keyboard Shortcut of Control+Enter to run a line is still essential too. It is faster than moving your hand back from the keyboard to the mouse and navigating to this button. Furthermore, this means that if you have several lines in your code chunk, you can run each line separately which helps with fixing errors. The keyboard shortcut for running all lines in a code chunk is Control+Shift+Enter.\n\nBut what about the output? Where does it appear?\nSimilar to when we were working in an R Script, if we save our output as an object it isn’t displayed until we “print” it, but if we only “print” it and don’t assign our code to an object, then our output is only displayed.\nWhen we run code in R Markdown, instead of our output appearing in the console or plotting tab, it appears directly below the code chunk. Our saved objects still appear in the Environment tab.\nR Markdown behaviour is slightly more complicated however, because as well as having the option to run our the code in the ways shown, we can also “knit” our document to see what our finished report will look like. When this happens, the “knitting” process essentially creates a new self-contained environment, separate to the Environment tab we see in RStudio, so any objects assigned will not appear in our Environment tab unless we run them separately. More on this shortly.\nTo add an R chunk, rather than manually entering back ticks and curly brackets, the easiest way is simply to click on the Insert shortcut at the top of your document and select R.\n\nTo run an R chunk, we’ve already mentioned the green arrow within each code chunk. There are other ways to run your code and these can be found in the Run shortcut at the top of your document.",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 5: Guide to R Markdown"
    ]
  },
  {
    "objectID": "content/week-1/topic-5.html#how-do-i-write-in-markdown",
    "href": "content/week-1/topic-5.html#how-do-i-write-in-markdown",
    "title": "Topic 5: Guide to R Markdown",
    "section": "How do I write in Markdown?",
    "text": "How do I write in Markdown?\nMarkdown is very simple to learn and you may even have been using it without knowing if you have ever formatted your text in a WhatsApp message. For example, we can put an underscore (_) on either side of a word to italicise it.\nHere is a link to an online tutorial which provides a very nice introduction to using Markdown and will take about 10 minutes to complete: Markdown tutorial. The RStudio website also has a self-paced online tutorial introducing RMarkdown as well as an (recently updated) handy RMarkdown Cheatsheet and an older (slightly less overwhelming) version of the cheatsheet.\nThere is also a very handy Markdown Quick Reference guide in the Help menu in RStudio if you need a reminder. I use this a lot.\nSome aspects of formatting we can control with Markdown include:\n\nHeadings\nText emphasis (bold or italic)\nLists\nImages\nLinks\n\nIt’s worth noting a slight quirk when writing in Markdown that a new line does not output a new line - to create a new line you have to leave an empty line.",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 5: Guide to R Markdown"
    ]
  },
  {
    "objectID": "content/week-1/topic-5.html#organising-the-output",
    "href": "content/week-1/topic-5.html#organising-the-output",
    "title": "Topic 5: Guide to R Markdown",
    "section": "Organising the Output",
    "text": "Organising the Output\nThere are various settings we can alter to control and organise the output of our document. For example, depending on who your intended audience is, you may or may not want to show lots of R code.\nLet’s look at how we can do this.",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 5: Guide to R Markdown"
    ]
  },
  {
    "objectID": "content/week-1/topic-5.html#chunk-options",
    "href": "content/week-1/topic-5.html#chunk-options",
    "title": "Topic 5: Guide to R Markdown",
    "section": "Chunk Options",
    "text": "Chunk Options\nEach code chunk has a Modify Chunk Options cog shortcut which you can click on:\n\nThis allows us to alter settings such as:\n\necho = FALSE only the output gets printed - useful for including figures\neval = FALSE the code gets printed but it isn’t run - useful for displaying partial code to learners to complete\ninclude = FALSE the code is run but nothing gets printed - useful for loading data other chunks might use\n\nWhen we edit these settings in the Modify Chunk Options, the code for these options appear at the top of our code chunk:\n\nYou will see that multiple code chunk options are separated by commas, but there is no comma after the first letter “r”. This letter “r” is important as this signifies what language the code chunk is in. Other languages are available including SQL and Python.",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 5: Guide to R Markdown"
    ]
  },
  {
    "objectID": "content/week-1/topic-5.html#the-setup-chunk",
    "href": "content/week-1/topic-5.html#the-setup-chunk",
    "title": "Topic 5: Guide to R Markdown",
    "section": "The Setup Chunk",
    "text": "The Setup Chunk\nUsually the very first code chunk in our document will be the setup chunk. This is essentially a normal code chunk but it has the label setup indicated after the letter r and all following code chunks will use the options specified here (unless otherwise specified on an individual basis).\n\nWe set the option include = FALSE to this code chunk as we want the code to run, but we don’t need this code to be displayed in our report.",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 5: Guide to R Markdown"
    ]
  },
  {
    "objectID": "content/week-1/topic-5.html#parameters",
    "href": "content/week-1/topic-5.html#parameters",
    "title": "Topic 5: Guide to R Markdown",
    "section": "Parameters",
    "text": "Parameters\nConsider the following scenario: You’ve created a beautiful report with R Markdown, presenting data drawn from various hospitals or Health Boards across the country. But individual hospitals have been in touch and have asked for a report with only their data.\nR Markdown to the rescue! It is possible to produce this in an R Markdown document with very little effort thanks to the use of parameters.\nInstead of manually editing each report, we can use parameters to automatically summarise the data for different hospitals or NHS Health Boards.\nThe input (parameter variable) is included in the YAML header by adding in params: then underneath is information about the name and input value of our parameter.\n\nHere we’re showing the YAML (header) from an example of a life expectancy report on the gapminder dataset. We’ve created a parameter to enable us to switch between reports for different continents easily.\nThe indentations are very important in the YAML, unlike in R code, so be careful with this. Also check that your colons are in the right place too ( : ).\nIf we find ourselves parameterising a document we have already created which was based on one continent’s data, we can simply search (Ctrl + F works here) the document for any mention of the continent and update.\nFor object names we can change the text to make it more generic.\nWhere we have referred to the value of a specific continent in our R code, for example in a filter, we need to update with params$continent where params is letting R know to look in the YAML for a parameter, and continent is telling R to use whatever value appears in this parameter.\nHere is a code chunk from our document before parameterisation.\n\nHere is the edited code chunk after an update to refer to the parameter listed in the YAML.\n\nHere is some descriptive text before parameterisation:\n\nHere is the edited descriptive text after an update to refer to the parameter listed in the YAML:\n\nNote the use of the r here to let R know the following text is R code. This is because these lines don’t appear within a code chunk (between the two sets of ```).",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 5: Guide to R Markdown"
    ]
  },
  {
    "objectID": "content/week-1/topic-5.html#knitting-the-document",
    "href": "content/week-1/topic-5.html#knitting-the-document",
    "title": "Topic 5: Guide to R Markdown",
    "section": "Knitting the document",
    "text": "Knitting the document\nThroughout the process of modifying our document we should constantly be “knitting” the report to see what the finished output will look like.\nKnitting is the process where R Markdown runs the entire document, including chunks of code and accompanying text, and creates the output in whatever format you have specified.\nThe knitting icon can be found at the top of your document:\n\nNote that although Knit and Run All both run all code chunks within the document, as mentioned previously, Knit doesn’t save any objects to our environment, but Run does.",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 5: Guide to R Markdown"
    ]
  },
  {
    "objectID": "content/week-1/topic-5.html#whats-the-difference-between-r-notebook-and-r-markdown",
    "href": "content/week-1/topic-5.html#whats-the-difference-between-r-notebook-and-r-markdown",
    "title": "Topic 5: Guide to R Markdown",
    "section": "What’s the difference between R Notebook and R Markdown?",
    "text": "What’s the difference between R Notebook and R Markdown?\nYou may come across R Notebooks. Both R Notebook and R Markdown are options we can select from the File -&gt; New file menu, and often people ask what the difference is between the two as they have the same file format. The answer is very little!\nR Notebook is one of the many output types that an R Markdown file can produce, so when you select R Notebook, the output part of the YAML will already say output: html_notebook.\nInteracting with an R Notebook is slightly different to that of an R Markdown document. When executing chunks in an R Markdown document, all the code is sent to the console at once, but in an R Notebook, only one line at a time is sent. This allows execution to stop if a line raises an error.",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 5: Guide to R Markdown"
    ]
  },
  {
    "objectID": "content/week-1/topic-5.html#top-tip",
    "href": "content/week-1/topic-5.html#top-tip",
    "title": "Topic 5: Guide to R Markdown",
    "section": "Top Tip",
    "text": "Top Tip\nOnce your R Markdown document starts to get quite long (you’ve been working hard!), with several different headings and perhaps various subheadings too, it can start to get slightly tricky to quickly scroll and find the various sections in your raw document.\nTo help with this, RStudio have a helpful tool, the Show document outline option which you can find near the Run menu.\n\nThis lets you quickly jump between sections in your document and see the outline of your structure at a glance. A similar outline feature can be found at the bottom of your document, and code chunks are included here too.\n\nThat’s all you need to know (and more) to get up and running in R Markdown, now have fun in the next Topic exploring all of these options for yourself.\n\nartwork by Allison Horst",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 5: Guide to R Markdown"
    ]
  },
  {
    "objectID": "content/week-1/topic-3.html",
    "href": "content/week-1/topic-3.html",
    "title": "Topic 3: Getting Started with R",
    "section": "",
    "text": "In case you don’t already have R and RStudio installed, follow the instructions here to get set up. If you already have R and RStudio installed, you can skip this.",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 3: Getting started with R"
    ]
  },
  {
    "objectID": "content/week-1/topic-3.html#installing-r-rstudio",
    "href": "content/week-1/topic-3.html#installing-r-rstudio",
    "title": "Topic 3: Getting Started with R",
    "section": "",
    "text": "In case you don’t already have R and RStudio installed, follow the instructions here to get set up. If you already have R and RStudio installed, you can skip this.",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 3: Getting started with R"
    ]
  },
  {
    "objectID": "content/week-1/topic-3.html#r-and-rstudio-recap",
    "href": "content/week-1/topic-3.html#r-and-rstudio-recap",
    "title": "Topic 3: Getting Started with R",
    "section": "R and RStudio recap",
    "text": "R and RStudio recap\nNow that you have access to RStudio you can start using it to experiment with the exercises we cover this week. Throughout the materials, there are chunks of code for you to run. Try copying the code from these chunks directly into RStudio to reproduce the examples and continue your explorations. It is through experimentation that we learn most effectively.\nAs a quick reminder, the image below shows you where to go to open up a new script in RStudio. Then you can start writing your own code and running it.\n\n\n\nOpen a new script in RStudio",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 3: Getting started with R"
    ]
  },
  {
    "objectID": "content/week-1/topic-3.html#getting-help",
    "href": "content/week-1/topic-3.html#getting-help",
    "title": "Topic 3: Getting Started with R",
    "section": "Getting Help",
    "text": "Getting Help\nRStudio has a built-in Help tab. To use the Help tab, move your cursor to something in your code and press F1 - this will show you the definition and some examples.\nThe Help tab is only really useful if you already know what you are looking for but can’t remember how it worked exactly. For finding help on things you have not used before, it is best to Google it. R has about 2 million users so someone somewhere will have had the same question or problem.",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 3: Getting started with R"
    ]
  },
  {
    "objectID": "content/week-1/topic-3.html#starting-with-a-blank-canvas",
    "href": "content/week-1/topic-3.html#starting-with-a-blank-canvas",
    "title": "Topic 3: Getting Started with R",
    "section": "Starting with a Blank Canvas",
    "text": "Starting with a Blank Canvas\nWhen we import data, R remembers the data and stores it in the Environment tab. But this is not really where our data lives. For anything important that we want to load in and save, we write the code in our scripts so that we can bring back the data and objects into our environment whenever we want to by rerunning it - reproducibility in action!\nIt’s good practice to clear out our environment frequently, and always before starting new work (a bit like spring cleaning) to make sure there are no old objects we no longer need cluttering up space. If we didn’t clear the environment, we might accidentally end up using older data we thought had been updated.\nTo clear our environment we Restart R and it only takes a second!\nTo restart R you can do one of the following:\n\nUse the keyboard shortcut Ctrl+Shift+F10\nUse the dropdown menu Session -&gt; Restart R\n\n————–\nNote: RStudio has a default option for saving your environment, but that is not best practice anymore. Make sure that you have changed to the following setting (you only need to do this once):\nGo to Tools -&gt; Global Options -&gt; General and set “Save .RData on exit” to Never. This does not mean you can’t or shouldn’t save your work in .RData/.rda files, but it is best to do it consciously and load exactly what you need to load, rather than letting R always save and load everything for you, as this could also include broken data or objects.",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 3: Getting started with R"
    ]
  },
  {
    "objectID": "content/week-1/topic-3.html#keyboard-shortcuts",
    "href": "content/week-1/topic-3.html#keyboard-shortcuts",
    "title": "Topic 3: Getting Started with R",
    "section": "Keyboard Shortcuts",
    "text": "Keyboard Shortcuts\nHere are some of the main keyboard shortcuts in R you may find useful to begin with:\n\n\n\n\n\n\n\n\nI want to…\nWindows\nMac\n\n\n  Restart R Session\nCtrl+Shift+F10\nCmd+Shift+F10\n\n\n  Run current line/selection\nCtrl+Enter\nCmd+Enter\n\n\n  Run the whole script\nCtrl+Shift+S\nCmd+Shift+S\n\n\n  Insert %&gt;%\nCtrl+Shift+M\nCmd+Shift+M\n\n\n  Reformat selection\nCtrl+Shift+A\nCmd+Shift+A\n\n\n  Comment/uncomment (multiple)       lines of code\nCtrl+Shift+C\nCmd+Shift+C\n\n\n  Open up Help tab on function\nF1\nF1 \n\n\n  Go to the end of the document\nCtrl + End\nCmd + down arrow",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 3: Getting started with R"
    ]
  },
  {
    "objectID": "content/week-1/topic-3.html#projects",
    "href": "content/week-1/topic-3.html#projects",
    "title": "Topic 3: Getting Started with R",
    "section": "Projects",
    "text": "Projects\nRStudio has a great way of helping you to keep all your analysis files together so that R can access them easily. You can do this by creating an RStudio project for each of your data analysis projects. The following video shows you how (switch to full screen to see the video better):\n\n\n\nRStudio projects",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 3: Getting started with R"
    ]
  },
  {
    "objectID": "content/week-1/topic-3.html#how-does-r-work",
    "href": "content/week-1/topic-3.html#how-does-r-work",
    "title": "Topic 3: Getting Started with R",
    "section": "How does R work?",
    "text": "How does R work?\nWhen using R, you will predominantly be working with your own data which you must first load in before you can start exploring it, and we’ll be covering how to do this in-depth in the next topic. But to begin with, let’s see how R works.\nNow that you have RStudio up and running, try copying the code from above these boxes and running them yourself in a new script.\nCan you remember what the # symbol does?\n\n# R is like a calculator\n7 + 3\n\n[1] 10\n\n\nThe # symbol allows us to add comments by letting R know not to run this line of text as R code.\nWhat if we want to save the value of this calculation to refer to at a later date?\n\nAssignment Operators\nTo save our value, we have to create an object, a, and then assign the value to the object using the = sign or alternatively, you could use these &lt;- symbol. They are both called assignment operators. In R there are often multiple ways of doing the same thing as you will continue to discover, and which you choose often depends on personal preference.\nMy preference is for the &lt;- because (a) it shows the direction of assignment (right to left) and (b) is distinct from = which can also be used within functions for specific arguments. But both work!\nTry adding and running the following code in your script. Why do you think we have to type a again on its own underneath?\n\n  # assign the results of our calculation to an object \"a\"\n  a &lt;- 7 + 3 \n  # print out the result\n  a \n\n[1] 10\n\n\nIt’s always worth remembering that in R:\n\nif you save an object, then the value does not get printed (displayed on the screen)\nif you print an object, then the value does not get saved\n\nIn our example above, the equals &lt;- sign tells R to give the object on the left of the sign the value of whatever is on the right of the sign, and the value gets saved. Underneath, where only a is typed, we are also printing it so that you can see the result. In this code chunk, we are both saving and printing.\nBut in our first example, there was no &lt;- sign so we were only printing the value, not saving it too.\nNote: When working in RStudio, when you save a value to an object it will appear in the Environment tab. If you haven’t given your values or data a name using an assignment operator, it won’t appear in the Environment tab.\n\n\nA Quick Aside on Names in R…\nYou can name objects in R anything that you like… well almost anything. There are a few rules about names in R:\n\nNo spaces - spaces inside a name are not allowed (the spaces around the &lt;- don’t matter):\nlucky_number &lt;- 5 ✔ lucky number &lt;- 5 ❌\nNames must start with a letter:\nlucky_number &lt;- 5✔1lucky_number &lt;- 5` ❌\nCase sensitive:\nlucky_number is different from Lucky_Number\nReserved words - there is a set of words you can’t use as names, including: if, else, for, in, TRUE, FALSE, NULL, NA, NaN, function (Don’t worry about remembering these, R will tell you if you make the mistake of trying to name a variable after one of these).\n\nFor more information on naming objects in R, check out this blog post.\n\n\nArithmetic Operators\nLet’s go one step further and save each of the values in our previous calculation ( 7 + 3 ) as objects too.\nIn the code chunk below you will see that we’ve added some more calculations. These include the most basic arithmetic operators we’ll be using and as you can see, R uses standard symbols.\nWhy might we want to save each of our values as objects first?\n\n# assign the values \"7\" and \"3\" to objects \"b\" and \"c\"\nb &lt;- 7\nc &lt;- 3\n\n# arithmetic operators\nadd_opr       &lt;- b + c\nsubtract_opr  &lt;- b - c\nmultiply_opr  &lt;- b * c\ndivide_opr    &lt;- b / c\n\n# print out the results\nadd_opr\n\n[1] 10\n\nsubtract_opr\n\n[1] 4\n\nmultiply_opr\n\n[1] 21\n\ndivide_opr\n\n[1] 2.333333\n\n\nSaving our values as objects allows us to carry out various calculations using one set of saved values. If we want to change the value of b or c, we can do so without having to change the values in all subsequent calculations too.\nWhy not try it just now? Change the value of b to 9 ( b &lt;- 9 ) and rerun the chunk to see your updated results.\nRemember, the name on the left of the &lt;- is the object name. Now change the name of one of the objects, but make sure you also change the name in the code for printing the results too.\nAll of R is just an extension of these types of processes: applying more complex functions (calculations) across more complex objects.\nIt’s important to appreciate that objects can be more than just single numbers. They can be entire spreadsheets, which in R tidyverse are known as tibbles. These are the types of objects you will predominantly be working with when analysing healthcare data.\nYou can find further examples of the arithmetic/mathematical operators in R via the introverse package documention.\n\n\nRelational or Comparison Operators\nAs well as performing calculations with arithmetic operators, you can also ask R things, such as is “2 greater than 10?”. In such cases, as you are asking R a questions, the output returned (or answer) is TRUE or FALSE. Another set of operators you will use frequently in R are relational operators, which are great for comparing values and for creating subgroups in your data or excluding outliers.\nNote: You can find a full list of the operators we use on the HealthyR Quick-start cheatsheet, also listed in Further Resources) for this week.\nThese operators allow us to ask questions about our data. For example, are values in a column greater than, less than or equal to a reference value?\nThe symbols used by different programs and languages for logical operators vary more widely than for arithmetic operators, let’s see what R uses, examples are shown in the following code chunk:\n\n# Assign values to \"x\" and \"y\"\nx &lt;- 3\ny &lt;- 7\n\n# Greater than\n2 &gt; 10\n\n[1] FALSE\n\n# Less than\nx &lt; y\n\n[1] TRUE\n\n# Equal to\nx == y\n\n[1] FALSE\n\n# Not equal to\nx != y\n\n[1] TRUE\n\n# Greater than or equal to\nx &gt;= 3\n\n[1] TRUE\n\n# Less than or equal to\n4 &lt;= x\n\n[1] FALSE\n\n\nThe == is very easily and commonly confused with the =. If you get an error when trying to compare values to see if they are equal, there is a good chance you have forgotten to put in the extra =.\nPrinting out a TRUE or FALSE value might seem a bit obscure, but these relational or comparison operators are really useful for filtering data and we’ll use them frequently throughout this course.\n\n\nLogical Operators\nA final set of operators that you will find useful when working with data in R is the logical set of operators. These work in a very similar way to the relational operators: R compares values and returns TRUE or FALSE as the output. You can think of logical operators are asking R to compare 2 or more things. \nYou can find further examples of the logical operators in R via the introverse package documentation.\nLet’s see how this works:\n\n  # assign multiple values to the object \"year\" \n  year &lt;- c(2000, 2001, 2002, 2003, 2004)\n \n  # check \"year\" values to see if they are equal to 2000 OR 2001 \n  year == 2000 | year == 2001\n\n[1]  TRUE  TRUE FALSE FALSE FALSE\n\n  # | for \"OR\"\n  year_subset1 &lt;- year == 2000 | year == 2001\n  year_subset1\n\n[1]  TRUE  TRUE FALSE FALSE FALSE\n\n  # & for \"AND\" \n  year_subset2 &lt;- year &gt; 2001 & year &lt; 2003\n  year_subset2\n\n[1] FALSE FALSE  TRUE FALSE FALSE\n\n  # ! for \"NOT\" \n  year_subset3 &lt;- !year == 2002\n  year_subset3\n\n[1]  TRUE  TRUE FALSE  TRUE  TRUE\n\n\n\n\nData Types\nLike many other types of analysis, database, or spreadsheet software, R needs to know what the variable type (or data type) of each column is. The types we’ll be using most frequently are characters, factors, and numbers.\n\nCharacters\nCharacters (sometimes referred to as strings or character strings) in R are letters, words, or even whole sentences (an example of this might be free text comments). We can convert objects or values to character data type using the function as.character(). Characters are displayed in-between \"\" (or '').\n\n\nFactors\nFactors can be thought of as slightly fussy characters. They’re fussy because they have something called levels. Levels are all the unique values this variable could take e.g. if we have a column with data on sex, there might be two levels, “Male” and “Female” or there might be three levels if there was an option to specify “Other” too. Using factors rather than just characters can be useful because:\n\nThe values that factor levels can take is fixed. For example, if the predefined levels of your column called sex are “Male” and “Female” and you try to add a new patient where sex is just called “F” sex was stored as a character data type rather than a factor, R would have no problem with this and you would end up with “Male”, “Female”, and “F” in your column.\nLevels have an order. By default, R sorts things alphabetically, but if you want to use a non-alphabetical order, e.g. if we had a body_weight variable where we want the levels to be ordered - “underweight” - “normal weight” - “overweight” - we need to make body_weight into a factor. Making a character column into a factor enables us to define and change the order of the levels.\n\nThese are huge benefits, especially as a lot of medical data analyses include the comparison of different risks to a reference level.\n\n\nNumbers\nIn R, we specify numbers using the as.numeric() function. Sometimes numerics are further differentiated based on whether they have decimal places or not. Integer stands for a number without decimal places, whereas double would have decimal places.\n\n\nDate\nDates can be confusing, they can appear in many different formats, and in R they can look very similar to the character data type because they too are displayed between inverted commas ( \"\" ). Once we let R know that our variable is a date however, it can do some clever things as we’ll discover later.\nYou can find out more about how R reads in dates and also more examples on all these other data types by going to Chapter 2.2 in R for health data analysis.",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 3: Getting started with R"
    ]
  },
  {
    "objectID": "content/week-1/topic-3.html#practice-time",
    "href": "content/week-1/topic-3.html#practice-time",
    "title": "Topic 3: Getting Started with R",
    "section": "Practice Time!",
    "text": "Practice Time!\n\nIn your RStudio, open a new script and do the following:\n\n\nCreate 2 new variables, x, y, with values 3, 10.\nCarry out a calculation using these variables (e.g. x+y).\nSave the value to an object called calc.\nPrint out the value of the variable to view the output.\n\n\nIn the same script, create a new variable called z and assign it the value of 5. Then create a new variable called calc2 and assign it the value of calc + z. Print out the value of calc2 to view the output.\nCreate a new variable called calc3 and assign it the value of calc2 - calc. Print out the value of calc3 to view the output.\nCreate a new variable called calc4 and assign it the value of calc3 * calc2. Print out the value of calc4 to view the output.",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 3: Getting started with R"
    ]
  },
  {
    "objectID": "content/week-1/topic-1.html",
    "href": "content/week-1/topic-1.html",
    "title": "Topic 1: Introduction to Health Data Science",
    "section": "",
    "text": "What is data science? What kinds of data are there in the healthcare sector and what can one do with them? The purpose of this topic is to introduce you to key concepts in health data science. You will also be introduced to the course team and learn how the course works.",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 1: Introduction to Health Data Science"
    ]
  },
  {
    "objectID": "content/week-1/topic-1.html#overview",
    "href": "content/week-1/topic-1.html#overview",
    "title": "Topic 1: Introduction to Health Data Science",
    "section": "",
    "text": "What is data science? What kinds of data are there in the healthcare sector and what can one do with them? The purpose of this topic is to introduce you to key concepts in health data science. You will also be introduced to the course team and learn how the course works.",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 1: Introduction to Health Data Science"
    ]
  },
  {
    "objectID": "content/week-1/topic-1.html#health-data-what-why-where-and-how",
    "href": "content/week-1/topic-1.html#health-data-what-why-where-and-how",
    "title": "Topic 1: Introduction to Health Data Science",
    "section": "Health data: what, why, where and how",
    "text": "Health data: what, why, where and how\n\nWhat is health data\nHealth data refers to any kind of data related to health conditions, reproductive outcomes, causes of death and quality of life for an individual or population. This includes biometric information, prescription data, diagnoses and screening test results, as well as data about hospital visits or sleeping and exercise patterns. There is a shift lately in thinking more broadly about health, and in addition to clinical data, this also includes environmental (e.g. pollution levels), socioeconomic (e.g. income level) and lifestyle information. In fact, it is not uncommon nowadays to consider things that are not traditionally associated with health, such as social media posts (e.g. researchers have used data science methods to detect signs of depression in tweets). In this course, we will adopt such an open-minded view of health data.\n\n\nSources of health data\nThere are lots of different sources of health data. Clinical data is produced in hospitals and it is stored in hospital information systems, picture archiving and communication systems, laboratory databases, etc. Clinical data is also produced in primary care (e.g. GP practices in the UK) and stored in local systems. \nBiomedical research data, such as clinical trial data, is produced by academia and pharmaceutical companies and it is stored in databases and libraries. \nHealth business data (e.g. around accounting, billing, management, etc.) is produced by healthcare providers, insurance companies and other organisations, and is typically stored in local information and transaction systems. \nFinally, there is a growing volume of patient-generated data. This includes data produced by wearable devices (e.g. activity trackers that capture data including heart rate, steps and calories) and web-based apps and platforms (e.g. self-management platforms for conditions like diabetes or cancer), among others. This data is typically stored on the cloud or in local databases of the corresponding companies. One of the challenges in health data science is that data currently sits in silos. This means that data from different sources is not integrated. In fact, even data within a single healthcare provider is often fragmented and not joined up.\n\n\nHealth data levels\nHealth data can be captured at different levels, from the microscopic world (e.g. genomics, epigenomics, metagenomics, proteomics, metabolomics) to individuals (e.g. data captured in electronic patient records) and populations (e.g. data about disease spreading across different communities). The data published on the WHO Coronavirus Disease (COVID-19) Dashboard is an example of population-level data. \nA large volume of data is produced at all these levels. For example, we estimate many terabytes of genomic data for each individual. Considering the size of the world population, this is really “big data”. Data generated by sensors in wearable devices is also very big, as they continuously generate data. Medical imaging data (e.g. X-rays) are also very heavy, making it challenging to store and computationally expensive to analyse.\n\n\nData forms\nHealth data comes in lots of different forms, from highly structured to unstructured data. Relational databases e.g. in hospital information systems provide a clear structure to data captured. You can think of a relational database as a collection of tables with rows and columns and links between them (e.g. you could have a Patient table containing demographic information about thousands of patients, a Hospital table with key information about different hospitals in Scotland, and an Admissions table with information about hospital admissions while pointing to the other two tables). \nData captured in spreadsheets also has some structure, but very often there is redundancy or duplication of information captured. \nEven less structure is present in clinical notes, which is natural language data, or in medical images. The lack of clear structure in the way this data is captured makes it harder to analyse and make sense of in an automated fashion. It is also very important to note that the choice of a data representation paradigm, and its underlying structure or absence of, can play a key role in data integration and linkage.\n\n\nBenefits of analysing data\nAnd why is it important to analyse health data? Learning more from data can contribute to important discoveries that allow us to better understand disease and improve the way we deliver care. This includes improved diagnostics, better decision-making and more effective predictions, among others.",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 1: Introduction to Health Data Science"
    ]
  },
  {
    "objectID": "content/week-1/topic-1.html#data-science-data-analytics",
    "href": "content/week-1/topic-1.html#data-science-data-analytics",
    "title": "Topic 1: Introduction to Health Data Science",
    "section": "Data Science & Data Analytics",
    "text": "Data Science & Data Analytics\nWatch the following 2 videos to find out what data science is, and what people mean when they use the term “data analytics”.\n\n\n\nData Science & Data Analytics videos",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 1: Introduction to Health Data Science"
    ]
  },
  {
    "objectID": "content/week-1/topic-1.html#precision-medicine-and-stratified-healthcare",
    "href": "content/week-1/topic-1.html#precision-medicine-and-stratified-healthcare",
    "title": "Topic 1: Introduction to Health Data Science",
    "section": "Precision Medicine and Stratified Healthcare",
    "text": "Precision Medicine and Stratified Healthcare\nPrecision medicine is an emerging field and it is transforming the medical approach to disease treatment and prevention. It focuses on identifying the most effective strategy for each patient, based on genetic, environmental, and lifestyle factors.\nThe ability to characterise individuals much more precisely then allows us to identify key differences across human populations and to act accordingly in healthcare provision – this is stratified healthcare. So, one can consider stratified healthcare and precision medicine to go hand-in-hand.\nHealth data science is a key enabler to the development of precision medicine and stratified healthcare. By bringing different types of data together for each patient, we can take a more personalised approach to therapies, tailoring them to suit each individual. This is a rapidly changing environment, and a very exciting one!",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 1: Introduction to Health Data Science"
    ]
  },
  {
    "objectID": "content/week-1/further-resources.html",
    "href": "content/week-1/further-resources.html",
    "title": "Further Optional Resources",
    "section": "",
    "text": "Data Saves Lives is a European initiative with the aim of raising awareness about the importance of health data, improving understanding of how it is used and establishing a trusted environment for multi-stakeholder dialogue about responsible use and good practices across Europe. Check out their website for interesting case studies and surveys about public opinions and behaviour around health data.\nThe Topol Review was an independent report published in 2019, outlining recommendations to ensure the NHS is the world leader in using digital technologies to benefit patients. This informed look into the future discusses among others, the major impact that genomics is going to have on patient care, as well as how the healthcare workforce can be to prepared deliver the digital future.\nThe NHS, AI and Our Data is an episode of the BBC Radio 4 programme “Analysis”, providing an overview of data in the NHS, describing how it is already transforming healthcare, and discussing privacy and other risks.",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Further Optional Resources"
    ]
  },
  {
    "objectID": "content/week-10/topic-4.html",
    "href": "content/week-10/topic-4.html",
    "title": "GWAS Resources",
    "section": "",
    "text": "A number of software packages have been developed for conducting genome-wide association studies. A few of the more common packages are:\n\nPLINK: General-purpose command line program available on Linux, Windows, and MacOS. Contains routines for opening, filtering, and converting genomic data in a number of formats. Runs linear and logistic association tests for unrelated participants. Also has a version 2 under development. This is the program that is taught in most GWAS courses.\nGCTA: Software package primarily for estimating heritabilty using genomic data, it can also perform GWAS while correcting for relatedness among participants. Contians a number of other features for downstream analysis, such as conditional analysis and gene-based tests. Uses a linear approximation for case-control GWAS.\nLDAK: Similar to GCTA but can fit models for different genetic architectures.\nBOLT-LMM: Similar to GCTA but uses a fast approximation that speeds up computation considerably. Appropriate when analysing very large samples. Uses a linear approximation for case-control GWAS.\nregenie: Fast software when GWASing multiple traits, possibly with related participants. Applicable to case-control GWAS.\n\nThere are several R packages that are useful for GWAS:\n\nbigsnpr: Managing and performing quality control on SNP data\nBioconductor: repository of bioinformatics packages\nieugwasr: query the OpenGWAS database\n\nWeb applications:\n\nFUMA: functional annotation and mapping of GWAS summary statistics\nLocusZoom: interactive region plots from GWAS summary statistics\n\nAdditional tools for different stages of a GWAS analysis are listed in Table 1 of the Uffelmann primer.\nThere is also a big list of software used for genetic analysis maintained on GitHub as the Rockefeller List: https://gaow.github.io/genetic-analysis-software/0/",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Topic 4:GWAS Resources"
    ]
  },
  {
    "objectID": "content/week-10/topic-4.html#software",
    "href": "content/week-10/topic-4.html#software",
    "title": "GWAS Resources",
    "section": "",
    "text": "A number of software packages have been developed for conducting genome-wide association studies. A few of the more common packages are:\n\nPLINK: General-purpose command line program available on Linux, Windows, and MacOS. Contains routines for opening, filtering, and converting genomic data in a number of formats. Runs linear and logistic association tests for unrelated participants. Also has a version 2 under development. This is the program that is taught in most GWAS courses.\nGCTA: Software package primarily for estimating heritabilty using genomic data, it can also perform GWAS while correcting for relatedness among participants. Contians a number of other features for downstream analysis, such as conditional analysis and gene-based tests. Uses a linear approximation for case-control GWAS.\nLDAK: Similar to GCTA but can fit models for different genetic architectures.\nBOLT-LMM: Similar to GCTA but uses a fast approximation that speeds up computation considerably. Appropriate when analysing very large samples. Uses a linear approximation for case-control GWAS.\nregenie: Fast software when GWASing multiple traits, possibly with related participants. Applicable to case-control GWAS.\n\nThere are several R packages that are useful for GWAS:\n\nbigsnpr: Managing and performing quality control on SNP data\nBioconductor: repository of bioinformatics packages\nieugwasr: query the OpenGWAS database\n\nWeb applications:\n\nFUMA: functional annotation and mapping of GWAS summary statistics\nLocusZoom: interactive region plots from GWAS summary statistics\n\nAdditional tools for different stages of a GWAS analysis are listed in Table 1 of the Uffelmann primer.\nThere is also a big list of software used for genetic analysis maintained on GitHub as the Rockefeller List: https://gaow.github.io/genetic-analysis-software/0/",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Topic 4:GWAS Resources"
    ]
  },
  {
    "objectID": "content/week-10/topic-4.html#summary-statistics",
    "href": "content/week-10/topic-4.html#summary-statistics",
    "title": "GWAS Resources",
    "section": "Summary statistics",
    "text": "Summary statistics\n\nSources\nSummary statistics are the output of SNP association effect sizes and p-values from a GWAS. Summary statistics are available in full or in part from a number of websites:\n\nGWAS Catalog: Comprehensive catalog of phenotype-SNP associations. Usually only contains independent significant SNPs from each study.\nOpenGWAS: Database of full summary statistics for a large number of traits, available for querying and downloading in a standardised format.\nPsychiatric Genomics Consortium (PGC): Full summary statistics on psychiatric disorders.\nSocial Sciences Genetic Association Consortium (SSGAC): Full summary statistics on social and behavioural traits like education and personality.\nNealelab: Transancestry GWAS from UK Biobank.\nGIANT Consortium: GWAS of anthropomorphic phenotypes (weight and height).\nGlobal Lipids Genetics Consortium: GWAS of lipids.\n\n\n\nFile structure\nSummary statistics are typically provided as a plain-text, tabular data file (columns are usually space or tab delimited). There is no single standard format (for example, see daner, GWAS-VCF, and GWAS-SSF), but most files will look something like the following:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCHR\nSNP\nPOS\nREF\nALT\nAF\nBETA\nSE\nP\n\n\n1\nrs189107123\n10611\nC\nG\n0.985\n-0.128\n0.0761\n0.0926\n\n\n1\nrs180734498\n13302\nT\nC\n0.118\n0.0389\n0.0278\n0.162\n\n\n1\nrs144762171\n13327\nC\nG\n0.0339\n0.0417\n0.049\n0.394\n\n\n…\n\n\n\n\n\n\n\n\n\n\n\n\nCHR: the chromosome number of symbol. Usually specified as a number (1, 2, 3, …) for GCRh37 or prepended with ‘chr’ for GCRh38 (chr1, chr2, chr3, …)\nSNP: Reference SNP number (RSID) for the genetic marker, or some other unique name. Sometimes listed just using the chromsome and base pair position (CPID) identifier with out without the alleles (e.g., SNP rs189107123 might be called 1:10611 or 1:10611_C_G in some datasets)\nPOS: basepair position in genomic coordinates of the reference genome build. This column is sometimes also labelled as BP.\nREF: reference allele. Usually, but not always, the effect allele for the association. This column is also sometimes labelled A1, allele1, or alleleB\nALT: alternative allele. Usually, but not always, the non-effect allele. This column might also be called A2 or allele0.\nAF: frequency of the reference allele. May be multiple columns if allele frequencies in cases and controls are reported seperately.\nBETA: effect size of the association (substitution effect of each additional copy of the effect allele). For case/control and binary phenotypes, usually the odds ratio (OR) is reported instead.\nSE: standard error of the effect size. When the effect size is an odds ratio, this column usually represents the standard error of log(OR)\nP: p-value of the association\n\nDepending on the software and study design, additional columns might be present or missing.\nThe three most important points to check when processing GWAS summary statistics are:\n\nwhat genome build the summary statistics used? RSIDs and basepair coordinates differ between genome builds, so this can influence merging and look up with other tools.\nwhich allele is the effect allele? There is inconsistency in whether the REF or ALT allele is the effect allele, and even how these columns are labelled. (“Let’s call it the effect allele“)\nwhich strand do the alleles refer to? While the positive strand is usually assumed, some datasets might mix in alleles coded from the reverse complementary strand of DNA (e.g., the alleles for rs180734498 might be reported as REF=A, ALT=G). This causes problems when the two alleles are complementary (C and G, A and T).",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Topic 4:GWAS Resources"
    ]
  },
  {
    "objectID": "content/week-10/topic-4.html#genomic-databases",
    "href": "content/week-10/topic-4.html#genomic-databases",
    "title": "GWAS Resources",
    "section": "Genomic databases",
    "text": "Genomic databases\nThe National Center for Biotechnology Information (NCBI), run by NIH, has a number of specialised, cross-referenced databases for querying genomic information:\n\ndbSNP: database of SNP variants with information on population frequency and genomic context.\nGene: cross species information on genes.\nClinVar: Genetic variation with clinical relevance to human health\n\nOther useful resources include\n\nOMIM: Online Catalog of Human Genes and Genetic Disorders\ngenomeAD: Aggregation database of exome and genome sequencing datasets.",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Topic 4:GWAS Resources"
    ]
  },
  {
    "objectID": "content/week-10/topic-4.html#suggested-read-gwas-paper",
    "href": "content/week-10/topic-4.html#suggested-read-gwas-paper",
    "title": "GWAS Resources",
    "section": "Suggested Read: GWAS Paper",
    "text": "Suggested Read: GWAS Paper\nLet’s call it the effect allele: a suggestion for GWAS naming conventions",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Topic 4:GWAS Resources"
    ]
  },
  {
    "objectID": "content/week-10/topic-2.html",
    "href": "content/week-10/topic-2.html",
    "title": "Genotyping, sequencing, and profiling",
    "section": "",
    "text": "Deoxyribonucleic acid is a large macromolecule composed of antiparallel (going in opposite directions) chains of nucleotides. Each nucleotide consists of a pair of nucleobases (Adenine paired with Thymine, Cytosine paired with Guanine).\n\n\n\n\n\nDNA Structure\nEach DNA molecule is packed into a chromosome. Humans are diploid, with two versions of each chromosome inherited from each parent, and have 22 pairs of autosomes and 1 pair of sex chromosomes). The human genome contains around 3 billion basepairs.\nA portion of the genome contains genes that encode proteins. A gene has a promotor region, which is a DNA sequence that where and how much the gene is expressed. A gene contains a number of exons (coding sequence, determining the amino acide composition of the protein) and introns (non-coding sequence). DNA is transcripted into RNA, which is then spliced into mRNA and then becomes the template for the animo acid chain which folds into a protein.",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Topic 2: Genotyping, sequencing, and profiling"
    ]
  },
  {
    "objectID": "content/week-10/topic-2.html#structure-of-dna",
    "href": "content/week-10/topic-2.html#structure-of-dna",
    "title": "Genotyping, sequencing, and profiling",
    "section": "",
    "text": "Deoxyribonucleic acid is a large macromolecule composed of antiparallel (going in opposite directions) chains of nucleotides. Each nucleotide consists of a pair of nucleobases (Adenine paired with Thymine, Cytosine paired with Guanine).\n\n\n\n\n\nDNA Structure\nEach DNA molecule is packed into a chromosome. Humans are diploid, with two versions of each chromosome inherited from each parent, and have 22 pairs of autosomes and 1 pair of sex chromosomes). The human genome contains around 3 billion basepairs.\nA portion of the genome contains genes that encode proteins. A gene has a promotor region, which is a DNA sequence that where and how much the gene is expressed. A gene contains a number of exons (coding sequence, determining the amino acide composition of the protein) and introns (non-coding sequence). DNA is transcripted into RNA, which is then spliced into mRNA and then becomes the template for the animo acid chain which folds into a protein.",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Topic 2: Genotyping, sequencing, and profiling"
    ]
  },
  {
    "objectID": "content/week-10/topic-2.html#microarrays",
    "href": "content/week-10/topic-2.html#microarrays",
    "title": "Genotyping, sequencing, and profiling",
    "section": "Microarrays",
    "text": "Microarrays\nModern DNA genotyping is performed with microarrays. Microarrays contain thousands of labelled probes of short DNA sequences of different genotypes. The target DNA hybridises with the matching probe. Probes are coupled to a dye that is read with a laser. The intensity of the alternative probes is used to cluster each sample to determine the genotype at each locus on the array.\n\nWatch a Microarray Method for Genetic Testing Video here\nBecause microarray genotypes only assay a small portion of the genome, imputation is often used to statistically fill in the non-genotyped portions of the genome. Imputation compares the genotyped target sample to the full genome sequence of a reference dataset. This is effective for variants that are common in the population.",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Topic 2: Genotyping, sequencing, and profiling"
    ]
  },
  {
    "objectID": "content/week-10/topic-2.html#sequencing",
    "href": "content/week-10/topic-2.html#sequencing",
    "title": "Genotyping, sequencing, and profiling",
    "section": "Sequencing",
    "text": "Sequencing\nIn contrast to genotyping, which only assays around 1 million (&lt; 0.1%) parts of the genome, sequencing captures 10x (for exome sequencing) or 1000x (for whole genome sequencing) the amount of genomic information. While genotyping can only assay genetic variants that have previously been identified, sequencing can assay previously unknown genetic variants. Sequencing is typically peformed by cutting DNA into small fragments, reading each fragment (using different technologies), and then reassembling the sequences using a reference genome.\nExome sequencing captures the exomes of genes (that code proteins) while whole genome sequencing captures most of the genome (usually excluding segements that are difficult to sequence such as repetitive elements or portions of the genome that are near the telomeres or centromeres).\n\n\n\n\n\nMapping reads",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Topic 2: Genotyping, sequencing, and profiling"
    ]
  },
  {
    "objectID": "content/week-10/topic-2.html#genomic-coordinates",
    "href": "content/week-10/topic-2.html#genomic-coordinates",
    "title": "Genotyping, sequencing, and profiling",
    "section": "Genomic coordinates",
    "text": "Genomic coordinates\n\n\n\n\n\nEvery portion of the genome is assigned a standard coordinate position. Chromosomes were numbered based on their apparent size based on traditional light microscopy. The autosomes are numbered 1-22 and the allosomes (X = 23, Y = 24, MT = 25). Within each chromosome, basepair positions are counted from one end of the chromosome to the other. As more is learned about the structure of the genome, the coordinate system gets periodically updated as a “build” of the genome. Most datasets currently available use a build called “GRCh37” (Genome Reference Consortium Human Build 37) or “hg19” (Human Genome Issue HG-19). More recent datasets have moved to a new build, called “GRCh38/hg38“.",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Topic 2: Genotyping, sequencing, and profiling"
    ]
  },
  {
    "objectID": "content/week-10/index.html",
    "href": "content/week-10/index.html",
    "title": "Overview",
    "section": "",
    "text": "For the final week of the course, we will look at an application of data science within genomics. Namely we will look towards gaining an understanding of the topic of genome wide association studies (GWAS).",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Overview"
    ]
  },
  {
    "objectID": "content/week-10/index.html#learning-outcomes",
    "href": "content/week-10/index.html#learning-outcomes",
    "title": "Overview",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nUnderstand how genomic data is collected and processed\nLearn what methods are used to analyse genomic data\nConduct a small portion of a genome-wide association study (GWAS)\nUnderstand the format of GWAS summary statistics and where to obtain them",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Overview"
    ]
  },
  {
    "objectID": "assessment/index.html",
    "href": "assessment/index.html",
    "title": "Assessment and Feedback Information",
    "section": "",
    "text": "Assessment Description\n\n\n\nAssessment Type\nPercentage of Final Mark\n\n\nCoursework - R Programming Assignment\n70%\n\n\nCoursework - Essay\n30%\n\n\n\n\n\nImportant Dates\nR Programming Assignment submission - 25 November 2024 @ 13:00\nEssay submission - 2 December 2024 @ 13:00\n\n\nFeedback and Grades\nStudents will be invited to complete a weekly formative quiz, and will be provided feedback on this.\nStudents will also be given an opportunity to submit a draft of one section of the assessment (data visualisation), and will receive tutor feedback on this work.\nFinally, tutorial classes will be structured to allow students to ask questions, and gain feedback on the data interpretation skills that they will develop throughout the course.\nPlease be aware that grades located within Learn are provisional until these have been confirmed by the exam board, at which point they will be available via EUCLID.\nDates for the return of provisional marks and feedback will be published in course documentation and will normally be within 3 weeks of the submission deadline. In some cases the feedback return date may be later, for example, if different groups of students have different submission deadlines for the same assignment. In such cases course documentation will still inform you of the date to expect provisional marks and feedback. Please be aware that the marking for all assessments is moderated to ensure consistency and marks and this affects the marking timeline. Marks released remain provisional until they are ratified in by the Board of Examiners meeting after the course finishes. Occasionally, this may result in changes to the provisional marks.\n\n\nMarking Scheme\n\nUniversity’s Common Marking Scheme (Opens in a new window).\nAssessment regulations (Opens in a new window).\n\n\n\nLate Submission Penalties\n\n(a) Word Limits and Word Counts\n\nPenalties for exceeding the word limit\n\nWhere an absolute word limit is specified in assessment documentation the following penalties will be applied if the word limit is exceeded.\n\nExceeding word count by ≤ 10%: 10% deduction of the maximum mark or a reduction to the minimum pass mark for the assessment, whichever results in the higher mark.\nExceeding word count by &gt;10% and ≤ 20%: 20% deduction of the maximum mark or a reduction to the minimum pass mark for the assessment, whichever results in the higher mark.\nExceeding word count by &gt; 20%: mark reduced to the minimum pass mark for the assessment.\n\n\n\n(b) What is and is not included in the word limit.\n\nWhen evaluating the word count it does not include the assessment title, table of contents, references, text in figures, text in tables, table and figure titles, table and figure legends, a list of abbreviations or any acknowledgements.\nThe word limit does include in-text citations (where Harvard referencing is required), headings and sub-headings.\nThe use of appendices is not an allowable mechanism for exceeding the maximum word limit. Therefore, unless otherwise stated in the assessment guidance, material included in an appendix will be regarded as non-assessable and will not be considered by the markers/examiners when arriving at a final mark.\n\n\n\n(c) Additional Guidance: Figures and Table With regard to words in figure and / or diagram legends, a figure legend is primarily descriptive of the contents of the figure. The legend must not be used to introduce new\n\ndata or information not referred to in the main text of the assessment. Tables should be used to summarise several pieces of information in a succinct and easily approachable manner. As such, figure legends that go beyond the description of figure contents and unnecessarily extensive tables will not be regarded favourably by markers.\n\nAttempts to deliberately circumvent the word limit through, for example,inappropriate hyphenation, including blocks of text as picture images, etc., will be regarded as potential academic misconduct and referred to the Deanery Academic Misconduct Officer.\n\n\n(d) How the policy will be applied.\n\nStudents will be required to indicate the total word count on the assessment cover sheet. Where a word limit is specified, word counts will be checked by the course administrator by opening submissions in MS Word and removing all sections not included in the final word limit. The word count then specified in MS Word will be taken as the final word count.\n\nNOTE: If you experience problems uploading your document, do not panic; simply send the file directly to the Course Administrator (stewart3.smith@ed.ac.uk). The sent date and time on your email will be taken as the time of submission.\nNote that any submissions made after the return of feedback dates will not be marked.\n\n\nCourse Work Extensions and Exceptional Circumstances\nFor students who need to request an extension for in-course assessments, or need to apply for exceptional circumstances, please refer to the ‘Help and Support’ section on Learn for further details. Note that the maximum period for eligible course assessments is 4 days.\n\n\nHelp and Support\nLinks to University Systems, BMTO Polices & Guidance and Central University Support can be found in the Help and Support section of Learn.",
    "crumbs": [
      "Assessment",
      "Overview",
      "Assessment and Feedback Information"
    ]
  },
  {
    "objectID": "assessment/ica1.html",
    "href": "assessment/ica1.html",
    "title": "ica1",
    "section": "",
    "text": "place holder for ica1"
  },
  {
    "objectID": "assessment/assessment-brief-and-rubric.html",
    "href": "assessment/assessment-brief-and-rubric.html",
    "title": "Assessment Brief And Rubric",
    "section": "",
    "text": "In this page, you can find the marking rubric for the assessment, an assessment brief, and instructions on how to create GitHub pages:\n\nMarking Rubric (PDF)\n\n\n\nAssessment Brief (PDF)\n\n\n\nGitHub Pages Instructions",
    "crumbs": [
      "Assessment",
      "ICA1 - R Programming Assignment",
      "Assessment Brief and Rubric"
    ]
  },
  {
    "objectID": "assessment/assessment-and-submission-details.html",
    "href": "assessment/assessment-and-submission-details.html",
    "title": "assessment-and-submission-details",
    "section": "",
    "text": "This component of assessment is worth 30% of the overall course mark.\nThe deadline to submit your final essay is 2nd December 2024 @ 13:00.\nAll in course assessments should be submitted to the drop box on Learn.\nWork submitted after the deadline will be subject to penalties. All written work will be submitted to a plagiarism check.\nThe essay topic and brief is available below.\nEssay topic: Unpack the proposition “Data Saves Lives”. Critically examine the origins of this phrase, using examples to support it, but also critical perspectives from the guest lectures in the course. Consider what forms of value are being created when data are collected and/or analysed, and to whom the value is flowing. Ensure your discussion is well informed by course content (particularly the guest lectures), published literature and current references. If appropriate, add links, figures or quotes, using the Harvard referencing style.\nThe maximum word count for essays is 1200 words. The word limit for the essay does not include figure legends, tables or the final reference list, but does include in-text citations, headings and subheadings. Please include a final word count on the essay cover sheet. Any essay longer than 1200 words will be marked subject to the BMS word limit policy.\nYour assignment should have a cover page with your title, exam number, and word count.\nThe assessment criteria are available in the Course Handbook.",
    "crumbs": [
      "Assessment",
      "ICA2 - Essay",
      "Assessment and Submission Details"
    ]
  },
  {
    "objectID": "assessment/assessment-and-submission-details.html#data-science-for-health-and-biomedical-sciences",
    "href": "assessment/assessment-and-submission-details.html#data-science-for-health-and-biomedical-sciences",
    "title": "assessment-and-submission-details",
    "section": "Data Science for Health and Biomedical Sciences",
    "text": "Data Science for Health and Biomedical Sciences\nEthics essay brief 2025/26",
    "crumbs": [
      "Assessment",
      "ICA2 - Essay",
      "Assessment and Submission Details"
    ]
  },
  {
    "objectID": "assessment/formative-assessment.html",
    "href": "assessment/formative-assessment.html",
    "title": "ICA1 - R Programming Assignment: Formative Assessment",
    "section": "",
    "text": "As the formative part of your assessment, please use the TurnItIn box below submit a document with a link to your assessment website. On the assessment website, we’d like to see about 1 page of content. Ideally, this would include these three elements:\n\nAn indication of the topic (focus) of your data exploration\nA data visualisation that represents a first stab at investigating your research question\nA commentary about what the data visualisation depicts, and what you plan to do next\n\nA key part of this formative exercise is for you to go through the steps to set up and update your assessment GitHub repository and website. This is why we insist on you submitting a link to your website. If you encounter any problems, please use the drop-in sessions to ask for help.\nYou can continue working on the assessment while you wait for feedback, but please don’t push any changes to GitHub.",
    "crumbs": [
      "Assessment",
      "ICA1 - R Programming Assignment",
      "Formative assessment"
    ]
  },
  {
    "objectID": "assessment/ica2-cover-sheet.html",
    "href": "assessment/ica2-cover-sheet.html",
    "title": "ICA Cover Sheet",
    "section": "",
    "text": "Download ICA Cover Sheet",
    "crumbs": [
      "Assessment",
      "ICA2 - Essay",
      "ICA Cover Sheet"
    ]
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Readings, videos and exercises",
    "section": "",
    "text": "Each week, you will be given a set of readings, videos and exercises to complete. These are designed to help you understand the material and to prepare you for the tutorials and assignment.\nIn-person tutorials will be held on Thursday and Friday afternoons from 2-5pm. During the tutorials you will work in pairs to complete coding exercises together. This is called pair programming and is an effective and fun way to learn to code. The tutorials are an important part of the course and you are expected to attend.\nIt is important to complete the material before the tutorials, as the exercises will build on the material covered in the videos and readings. The tutorials will be a chance to ask questions and to get help with the exercises.",
    "crumbs": [
      "Content",
      "Readings, videos and exercises"
    ]
  },
  {
    "objectID": "content/index.html#quiz",
    "href": "content/index.html#quiz",
    "title": "Readings, videos and exercises",
    "section": " Quiz",
    "text": "Quiz\nThere will be a quiz at the beginning of each tutorial to informally test your understanding of the material and discuss in your pairs.",
    "crumbs": [
      "Content",
      "Readings, videos and exercises"
    ]
  },
  {
    "objectID": "content/index.html#homework",
    "href": "content/index.html#homework",
    "title": "Readings, videos and exercises",
    "section": " Homework",
    "text": "Homework\nOn some weeks, there will be a homework exercise that you can complete on your own for practice. These won’t be marked, but they will be a good way to practice the material and to prepare for the assignment.\nNote: The content will be released weekly",
    "crumbs": [
      "Content",
      "Readings, videos and exercises"
    ]
  },
  {
    "objectID": "content/index.html#weekly-topics",
    "href": "content/index.html#weekly-topics",
    "title": "Readings, videos and exercises",
    "section": "Weekly Topics:",
    "text": "Weekly Topics:\n Week 1: Introduction\n Week 2: Data Tidying and Wrangling\n Week 3: Data Visualisation and Storytelling\n Week 4: Analysing & Presenting Data in R\n Week 5: Maps, tables, functions\n Week 6: Machine Learning & Analysing Clinical Tests\n Week 7: Improving Healthcare Processes & Integrating Health Data\n Week 8: Data and Identity\n Week 9: Data Ownership\n Week 10: Working with Genomic Data",
    "crumbs": [
      "Content",
      "Readings, videos and exercises"
    ]
  },
  {
    "objectID": "content/week-10/topic-1.html",
    "href": "content/week-10/topic-1.html",
    "title": "Genomic data: What is it and how to analyse it",
    "section": "",
    "text": "What is genomic data, how can one analyse it and why is it important? The purpose of this topic is to introduce you to genomic data and explain how its analysis enables personalised treatment and prevention of disease.",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Topic 1: Genomic data: What is it and how to analyse it"
    ]
  },
  {
    "objectID": "content/week-10/topic-1.html#overview",
    "href": "content/week-10/topic-1.html#overview",
    "title": "Genomic data: What is it and how to analyse it",
    "section": "",
    "text": "What is genomic data, how can one analyse it and why is it important? The purpose of this topic is to introduce you to genomic data and explain how its analysis enables personalised treatment and prevention of disease.",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Topic 1: Genomic data: What is it and how to analyse it"
    ]
  },
  {
    "objectID": "content/week-10/topic-1.html#dna-data-sequencing-and-modelling",
    "href": "content/week-10/topic-1.html#dna-data-sequencing-and-modelling",
    "title": "Genomic data: What is it and how to analyse it",
    "section": "DNA Data: Sequencing and Modelling",
    "text": "DNA Data: Sequencing and Modelling\nWatch the following 4 videos to find out what DNA data looks like, and how it is sequenced, modelled and analysed.\nIntroduction to Genomic Data\nDNA and Sequencing\nModelling Genomic Data\nAnalysing Genomic Data",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Topic 1: Genomic data: What is it and how to analyse it"
    ]
  },
  {
    "objectID": "content/week-10/topic-3.html",
    "href": "content/week-10/topic-3.html",
    "title": "Genome-wide association studies",
    "section": "",
    "text": "A genome-wide association study (GWAS) is a hypothesis-free method of finding genetic factors that are associated with a trait or disease. Any two individuals randomly sampled from the population will differ in only about 0.5% of their genetic makeup:\n\n0.1%: Single nucleotide polymorphisms (SNPs)\n0.4%: Structural variations including copy number variations (CNVs) and large scale insertions, deletions, or inversions\n\nA SNP occurs when a single base of DNA is substituted with another, resulting in a polymorphism (for example, a C–G basepair being subtituted with a T-A basepair). SNPs occur once in every 100–1000 basepairs (BP) on average. Currently there are about 80 million known SNPs in the human genome.\n\n\n\n\n\nGWAS typically analyse common SNPs (with frequencies &gt;= 1%). In a GWAS, every SNP available in the data set is tested, so there will be as many tests as their are SNPs. A typical dataset will contain hundreds of thousands to several million SNPs.",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Topic 3: Genome-wide association studies"
    ]
  },
  {
    "objectID": "content/week-10/topic-3.html#introduction",
    "href": "content/week-10/topic-3.html#introduction",
    "title": "Genome-wide association studies",
    "section": "",
    "text": "A genome-wide association study (GWAS) is a hypothesis-free method of finding genetic factors that are associated with a trait or disease. Any two individuals randomly sampled from the population will differ in only about 0.5% of their genetic makeup:\n\n0.1%: Single nucleotide polymorphisms (SNPs)\n0.4%: Structural variations including copy number variations (CNVs) and large scale insertions, deletions, or inversions\n\nA SNP occurs when a single base of DNA is substituted with another, resulting in a polymorphism (for example, a C–G basepair being subtituted with a T-A basepair). SNPs occur once in every 100–1000 basepairs (BP) on average. Currently there are about 80 million known SNPs in the human genome.\n\n\n\n\n\nGWAS typically analyse common SNPs (with frequencies &gt;= 1%). In a GWAS, every SNP available in the data set is tested, so there will be as many tests as their are SNPs. A typical dataset will contain hundreds of thousands to several million SNPs.",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Topic 3: Genome-wide association studies"
    ]
  },
  {
    "objectID": "content/week-10/topic-3.html#performing-an-association-test",
    "href": "content/week-10/topic-3.html#performing-an-association-test",
    "title": "Genome-wide association studies",
    "section": "Performing an association test",
    "text": "Performing an association test\n\n\n\n\n\nAn association test is performed by comparing a genotype at a particular SNP marker to a phenotype (a trait or disease). This test is usually done within a regression framework, using linear regression for continuous/quantitative phenotypes (like height) or a logistic regression for a binary or case/control phenotype (like disease status).\nAt each SNP there are typically two alleles, for example C and T. This allows for four possible genotypes: CC homozygote, CT or TC heterozyogotes, and TT homozygotes. The basic association tests assumes that substituting one allele for the other has an additive effect on the phenotype. This means that the model assumes that the average difference in phenotype between the CC genotype and the CT genotype is the same as between the CT and TT genotype.\nTo run the regression, the SNP genotype is encoded to a number as a count of one of the alleles (which allele is chosen is arbitrary, but traditionally it is the minor allele). The chosen allele is called the “effect allele”. In this example, if the T allele is chosen as the effect allele, then the numeric encoding is the number of T alleles in the genotype: CC = 0, CT and TC = 1, and TT = 2. This numeric representation can then be entered into a regression formula:\n\n\n\n\n\nwhere Yi is the phenotype of participant i, Xi is a vector of covariates for participant i, gij is the numerical genotype for participant i at SNP j, and eij is the error term. B is a vector of regression coefficients for the covariates and βj is the regression coefficient for SNP j. From the fitted regression model, the beta coefficient βj for the SNP represents the average substitution effect of having one additional copy of the effect allele.\nBinary traits like disease status are analysed as case-control studies using logistic regression. In a case-control analysis, the beta coefficient represents the change in log-odds of having the disease for every additional copy of the effect allele. Exponentiating the beta coefficient,  , converts the effect size to an odds ratio.",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Topic 3: Genome-wide association studies"
    ]
  },
  {
    "objectID": "content/week-10/topic-3.html#population-stratification",
    "href": "content/week-10/topic-3.html#population-stratification",
    "title": "Genome-wide association studies",
    "section": "Population stratification",
    "text": "Population stratification\nOne possible confounder when performing association tests is that many allele frequencies differ between populations and subpopulations, while disease prevalences also differ. This can create false associations between SNPs and diseases.\nThese systematic differences in allele frequencies between subpopulations is called “population stratification”. The most obvious cause of stratification is ancestry but it can also be caused by confounding between environmental factors and geography. A principal components analysis of genetic variation across the whole genome can be used to visualise this variation.\nA statistical summary of the first two major dimensions of variation (PC = principal component). Each participant is plotted at their genetic coordinates using text label for their country of origin. The plot resembles the geographic relationship between each country.\n\n\n\n\n\nNovembre et al “Genes mirror geography within Europe” 2008\nThe association between polygenic burden for traits and diseases and UK geography can be explored on the Genes & Geography in Great Britain web app, based on the paper Abdellaoui et al. “Genetic correlates of social stratification in Great Britain” 2019.\nThere are other sources of false positives, such as technical artifacts and batch effects when cases and controls are genotyped separately, but these are usually easier to detect and remove during standard quality control (QC) processing.",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Topic 3: Genome-wide association studies"
    ]
  },
  {
    "objectID": "content/week-10/topic-3.html#multiple-testing-and-significance",
    "href": "content/week-10/topic-3.html#multiple-testing-and-significance",
    "title": "Genome-wide association studies",
    "section": "Multiple testing and significance",
    "text": "Multiple testing and significance\nGenetic data can contain hundreds of thousands or millions of variants. What p-value should be used to determine statistical significance?\nStandard methods to correct for multiple testing, such as Bonferroni correction based on the number of tests, are too conservative. This is because SNP markers are not independent. Instead, SNPs that are physically close to each other on a chromosome tend to be correlated (termed “linkage disequilibrium”). This correlation is because recombination breaks up the linkage between a mutation and the rest of the chromosome as it is passed down through generations.\n\n\n\n\n\nHettiarachchi, G., Komar, A.A. (2022). GWAS to Identify SNPs Associated with Common Diseases and Individual Risk: Genome Wide Association Studies (GWAS) to Identify SNPs Associated with Common Diseases and Individual Risk. https://doi.org/10.1007/978-3-031-05616-1_4\nThe p-value threshold can be derived empirically from the number of independent genomic regions, which has been determined to be about 1 million. Thus a multiple testing correction of p &lt;= 0.05/1,000,000 = 5 × 10-8 is used as the standard significance threshold in GWAS.\nThe lack of independence among SNPs also means that, for any genomic region found to be associated with the trait, it is necessary to select one or more SNPs that represent or cause the signal in that region (“clumping”).",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Topic 3: Genome-wide association studies"
    ]
  },
  {
    "objectID": "content/week-10/topic-3.html#summarising-the-results",
    "href": "content/week-10/topic-3.html#summarising-the-results",
    "title": "Genome-wide association studies",
    "section": "Summarising the results",
    "text": "Summarising the results\nThe results from a genome-wide association study can be visualised using a graph that is called a Manhattan plot. The plot features results for each SNP plotted by chromosome and basepair position on the x-axis versus transformed p-values on the y-axis. The p-values are transformed as the negative log 10 [-log10(P)], which turns very small p-values into large, positive numbers.\n\n\n\n\n\nPGC SCZ 2014.\nTo summarise the results, SNPs that nearby and in linkage disequilibrium with each other are clumped together, and the SNP within the clumped region with the smallest p-value is selected. There are other more advanced techniques for determining the causal SNP or set of SNPs in a region, such as conditional-and-joint analysis and finemapping.\n\n\n\n\n\nOnce the SNPs are selected, the GWAS signals can be integrated with other bioinformatics data to map the SNPs to genes and explore the functional implications of the associations.",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Topic 3: Genome-wide association studies"
    ]
  },
  {
    "objectID": "content/week-10/topic-3.html#reading-list",
    "href": "content/week-10/topic-3.html#reading-list",
    "title": "Genome-wide association studies",
    "section": "Reading list",
    "text": "Reading list\n\nA tutorial on statistical methods for population association studies\nGene–environment correlations across geographic regions affect genome-wide association studies\nGenome-wide association studies",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Topic 3: Genome-wide association studies"
    ]
  },
  {
    "objectID": "content/week-1/index.html",
    "href": "content/week-1/index.html",
    "title": "Overview",
    "section": "",
    "text": "Welcome to the course! This week you will be introduced to key concepts in health data science. We’ll also cover basic R operators and an intro to RMarkdown. You will also get to know your classmates and your lecturer in the live sessions on Thursday and Friday.",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "content/week-1/index.html#learning-outcomes",
    "href": "content/week-1/index.html#learning-outcomes",
    "title": "Overview",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of the week you will be able to:\n\nDescribe different examples of data in healthcare\nSummarise key phases in the data science process\nUse some basic R functions\nKnow what RMarkdown is\nUnderstand how to approach errors in code\n\n\nStructure of this week’s materials\n\nIntroduction to Health Data Science: 2 videos, 1 reading\nRole of Ethics in Health and Social Care: 1 reading \nGetting started with R: 1 video, 1 reading, 5 code samples, 4 practice questions\nTypes of Health Data: 1 reading with code samples\nGuide to R Markdown: 1 reading with RStudio examples\nTroubleshooting and Error Messages: 1 reading\n\nEstimated time to complete: 2.5 hrs",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Overview"
    ]
  },
  {
    "objectID": "content/week-1/topic-2.html",
    "href": "content/week-1/topic-2.html",
    "title": "Topic 2: The Role of Ethics in Health and Social Care",
    "section": "",
    "text": "Ethics can be defined as a discipline that systematically conceptualises, defines and defends ideas of right and wrong, good and bad. It is the means by which we evaluate actions, behaviours, attitudes, characteristics or state of affairs as good or bad. There are many philosophical lenses and frameworks which are applied to ethical approaches in health and social care.\n\n\n\nEthics in Health and Social Care\n\n\nOn a practical level, the Health Research Authority protects and promotes the interests of patients and the public in health and social care research. One of their core functions is to promote ethical review for research in the NHS in Scotland. Research Ethics Committees (REC) are based in geographical Health Boards receiving applications via IRAS (Integrated Research Application Service). All universities will also have their own version of ethical review panel acting as a precursor to seeking HRA approval. The recently formed Research Data Scotland (RDS) has a mission ‘to improve the economic, social and environmental wellbeing in Scotland by enabling access to and linkage of data about people, places and businesses for research in the public good’. RDS have established ‘partnership with Scottish Government, Scotland’s leading academic institutions and Public Health Scotland to facilitate insight from public sector data and promote and advance health and social wellbeing in Scotland’. Safe Havens, including virtual options, are in place for researchers to access public datasets under supervision with ethical review already in place.",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 2: The Role of Ethics in Health and Social Care"
    ]
  },
  {
    "objectID": "content/week-1/topic-2.html#overview",
    "href": "content/week-1/topic-2.html#overview",
    "title": "Topic 2: The Role of Ethics in Health and Social Care",
    "section": "",
    "text": "Ethics can be defined as a discipline that systematically conceptualises, defines and defends ideas of right and wrong, good and bad. It is the means by which we evaluate actions, behaviours, attitudes, characteristics or state of affairs as good or bad. There are many philosophical lenses and frameworks which are applied to ethical approaches in health and social care.\n\n\n\nEthics in Health and Social Care\n\n\nOn a practical level, the Health Research Authority protects and promotes the interests of patients and the public in health and social care research. One of their core functions is to promote ethical review for research in the NHS in Scotland. Research Ethics Committees (REC) are based in geographical Health Boards receiving applications via IRAS (Integrated Research Application Service). All universities will also have their own version of ethical review panel acting as a precursor to seeking HRA approval. The recently formed Research Data Scotland (RDS) has a mission ‘to improve the economic, social and environmental wellbeing in Scotland by enabling access to and linkage of data about people, places and businesses for research in the public good’. RDS have established ‘partnership with Scottish Government, Scotland’s leading academic institutions and Public Health Scotland to facilitate insight from public sector data and promote and advance health and social wellbeing in Scotland’. Safe Havens, including virtual options, are in place for researchers to access public datasets under supervision with ethical review already in place.",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 2: The Role of Ethics in Health and Social Care"
    ]
  },
  {
    "objectID": "content/week-1/topic-4.html",
    "href": "content/week-1/topic-4.html",
    "title": "Types of Health Data",
    "section": "",
    "text": "When we are interacting with data sources for our analysis, we need to be aware of the different types of data that we may encounter.\nWe will mainly be using CSV files in this course but it’s useful to know about other formats that you may come across.\nThe following are some examples of the common types of data along with some code snippets illustrating how you would use them. You don’t need to remember all of these snippets but you will certainly come across them one day!",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 4: Types of Health Data"
    ]
  },
  {
    "objectID": "content/week-1/topic-4.html#csv-comma-separated-values",
    "href": "content/week-1/topic-4.html#csv-comma-separated-values",
    "title": "Types of Health Data",
    "section": "CSV (Comma Separated Values)",
    "text": "CSV (Comma Separated Values)\nCSV files are the most common format for storing tabular data, particularly in this course. They are simple text files that use commas to separate values. Each line in the file represents a row of data, and each value in the row is separated by a comma. CSV files can be easily imported into RStudio using the read_csv function in the readr package (also included in Tidyverse package which we will cover later).\nThe examples below use code that you may not be familiar with. Don’t worry! You will soon be using very similar code.\n\nExample\nlibrary(readr)\ndata &lt;- read_csv(\"data.csv\")",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 4: Types of Health Data"
    ]
  },
  {
    "objectID": "content/week-1/topic-4.html#tsv-tab-separated-values",
    "href": "content/week-1/topic-4.html#tsv-tab-separated-values",
    "title": "Types of Health Data",
    "section": "TSV (Tab Separated Values)",
    "text": "TSV (Tab Separated Values)\nSimilar to CSV files, TSV files use tabs to separate values instead of commas. They are also simple text files that can be easily imported into RStudio using the read_tsv function in the readr package.\n\nExample\nlibrary(readr)\ndata &lt;- read_tsv(\"data.tsv\")",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 4: Types of Health Data"
    ]
  },
  {
    "objectID": "content/week-1/topic-4.html#json-javascript-object-notation",
    "href": "content/week-1/topic-4.html#json-javascript-object-notation",
    "title": "Types of Health Data",
    "section": "JSON (JavaScript Object Notation)",
    "text": "JSON (JavaScript Object Notation)\nJSON is a lightweight data interchange format that is easy for humans to read and write, and easy for machines to parse and generate. JSON files are often used to store structured data, such as data from APIs (Application Programming Interfaces).\nDon’t worry if you don’t know what an API or JavaScript are. JSON files can be imported into RStudio using the fromJSON function in the jsonlite package.\n\nExample\nlibrary(jsonlite)\ndata &lt;- fromJSON(\"data.json\")",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 4: Types of Health Data"
    ]
  },
  {
    "objectID": "content/week-1/topic-4.html#xml-extensible-markup-language",
    "href": "content/week-1/topic-4.html#xml-extensible-markup-language",
    "title": "Types of Health Data",
    "section": "XML (eXtensible Markup Language)",
    "text": "XML (eXtensible Markup Language)\nXML is a markup language that defines a set of rules for encoding documents in a format that is both human-readable and machine-readable. XML files are often used to store structured data, such as data from web services.\nXML files can be imported into RStudio using the xml2 package.\n\nExample\nlibrary(xml2)\ndata &lt;- read_xml(\"data.xml\")",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 4: Types of Health Data"
    ]
  },
  {
    "objectID": "content/week-1/topic-4.html#databases",
    "href": "content/week-1/topic-4.html#databases",
    "title": "Types of Health Data",
    "section": "Databases",
    "text": "Databases\nDatabases are structured collections of data that can be accessed and managed using a database management system (DBMS). There are many types of databases, including relational databases (e.g., MySQL, PostgreSQL) and NoSQL databases (e.g., MongoDB, Cassandra).\n\nExample\nlibrary(DBI)\nlibrary(RSQLite)\ncon &lt;- dbConnect(RSQLite::SQLite(), \"database.db\")\ndata &lt;- dbGetQuery(con, \"SELECT * FROM table_name\")\ndbDisconnect(con)",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 4: Types of Health Data"
    ]
  },
  {
    "objectID": "content/week-1/topic-4.html#what-types-of-data-are-used-in-health-research",
    "href": "content/week-1/topic-4.html#what-types-of-data-are-used-in-health-research",
    "title": "Types of Health Data",
    "section": "What types of data are used in health research?",
    "text": "What types of data are used in health research?\nThe datasets used by health data scientists comes from lots of different sources. Read this short document from Health Data Research UK that describes some of the most common types of data used in health research such as patient data and data from samples.",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 4: Types of Health Data"
    ]
  },
  {
    "objectID": "content/week-1/topic-6.html",
    "href": "content/week-1/topic-6.html",
    "title": "Topic 6: Troubleshooting and Error Messages",
    "section": "",
    "text": "When coding in R it is normal to spend a large part of your time dealing with error messages. This is the same whether you are a beginner or advanced R user. It can be time consuming trying to decipher these error messages and also very frustrating. Troubleshooting error messages is a skill in itself but it does get easier, promise!\nHere are a few tips which will hopefully help to speed up the process. First, here’s a quick checklist, with more detail included below.\n\ncheck you’ve not done something daft (spelling, comma, bracket, etc.)\nreally look at the error message for clues (sometimes they’re surprisingly helpful)\nRestart R and try running the code again (this is often very successful)\nif using an unfamiliar function - check documentation (F1)\ntry Googling the error message directly (Google is actually quite clever)\ntry simplifying your code and run smaller chunks (build from what you know works)\nask a fellow R user or the community (Slack: NHS-R or R4DS)\n\nMost of the error messages that you see will be because there’s a typo or other minor issue like a misspelled word, or forgotten a bracket or comma, or typed one = instead of two ==. So quickly check over your code to see if any of these might be the culprit first. Watch out for capitalisation (case sensitivity); R does recognise upper and lower case.\nTake a closer look at your error message to see if it might be helpful in working out what has gone wrong. Error messages can look a bit daunting at first. Aside from the alarming red text, the error messages often include unfamiliar words, but hidden amongst this, you will often find some more useful plain English or a hint as to what might be wrong. For example, the error messages will often let you know which line your code broke down on and there is also a visual reference shown alongside your code, indicated by a red line and cross.\nUnfortunately, warnings are also printed in the same red font. In many cases, warnings can be safely ignored. Especially if the output, whether a plot or a table, otherwise looks like what you’d expect it to look like. \nRestart R and do this often! There is no harm in restarting R, you will not lose any work as all of your code is written down, ready to be run again at the click of a button (or press of a keyboard shortcut). The joy of reproducibility! It is good practice to restart R often as a messy environment can cause unexpected behaviour, particularly when you are starting your analysis and still slightly undecided on the best object names to use so might change them a number of times before settling.\nWhen trying out a new function, make sure you know how it works and have explored examples of the function “in action”.You can check the documentation (press F1 with cursor inside function name), and scroll down to the bottom to see code examples. Google will also provide you with copious examples of how a function might be used. Or another way of finding the most relevant help is to restrict your search to more recent years. For ggplot2, dplyr, assignment operators, and many other introductory R topic-related questions, there is also the new introverse package which provides beginner-oriented help to complement the official documentation.\nIf you have no clue why you are getting a particular error message, it can be worth copying and pasting the error messages directly into Google. This can help to give some indication as to why your code isn’t working and then you can start to refine your search to make it more relevant. Try to remove text which is specific to your own project,e.g., your specific file or variable name, although often Google is clever enough to ignore these anyway.\nIf you’re still struggling to work out what’s going on, you can always ask someone.A friendly colleague is best, even if they’re not an “R expert” a second pair of eyes can often spot that extra ” or missing ) you’re too tired to see.\nThere are several online communities for free R help:\n\nhttps://community.rstudio.com/ (search through existing questions, or post your own, everyone is really friendly there)\nhttps://www.rfordatasci.com/ - join the Slack group\nAlternatively, there’s a similar group for those who identify as a woman or gender minority: https://rladies.org/\nFind further events and training course from the NHS-R community: https://nhsrcommunity.com/\n\nFollow the #rstats hashtag on Twitter, and when posting about R on Twitter, use this hashtag so other people can find your Tweet.",
    "crumbs": [
      "Content",
      "Week 1: Introduction",
      "Topic 6: Troubleshooting and Error Messages"
    ]
  },
  {
    "objectID": "content/week-2/index.html",
    "href": "content/week-2/index.html",
    "title": "Overview",
    "section": "",
    "text": "Welcome to Week 2 of the course! This week we will delve into working with data in R, from importing data to tidying and wrangling. You will have the opportunity to practice the concepts you learn in the live sessions on Thursday and Friday.",
    "crumbs": [
      "Content",
      "Week 2: Data Tidying and Wrangling",
      "Overview"
    ]
  },
  {
    "objectID": "content/week-2/index.html#learning-outcomes",
    "href": "content/week-2/index.html#learning-outcomes",
    "title": "Overview",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of the week you will be able to:\n\nGet an overview of the main data science tools in R: how to import, tidy, transform, visualise, model, and communicate data\nLearn how to organise your data projects, including structuring files and folders in a reproducible and consistent way\nIdentify and reflect on ethical issues that can arise when working with health and care data\n\n\nStructure of this week’s materials\n\nThe Tidyverse:\nImporting Data: 2 videos, 1 practise, 1 test your knowledge\nExploring Data: 2 videos, 1 test your knowledge\nEthical Dilemmas in Health Care Data:\nFiles and Folders:\n\nEstimated time to complete: 2.5 hrs",
    "crumbs": [
      "Content",
      "Week 2: Data Tidying and Wrangling",
      "Overview"
    ]
  },
  {
    "objectID": "content/week-2/topic-2.html",
    "href": "content/week-2/topic-2.html",
    "title": "Importing Data",
    "section": "",
    "text": "We will be using one of the datasets available from Public Health Scotland, called Cancelled Planned Operations by Health Board.\nHere are the functions we’ll be covering in the the videos:\n\n base::library() we’re already familiar with for loading packages\n readr::read_csv() for importing CSV files\n here::here() is a function that helps R find files\n readxl::read_excel() for importing Excel files\n\n\n artwork by Allison Horst\nNote: The word before the ::  is the name of the package and the word after the ::  is the name of a function from that package, in the format: package::function()",
    "crumbs": [
      "Content",
      "Week 2: Data Tidying and Wrangling",
      "Topic 2: Importing Data"
    ]
  },
  {
    "objectID": "content/week-2/topic-2.html#how-to-import-data",
    "href": "content/week-2/topic-2.html#how-to-import-data",
    "title": "Importing Data",
    "section": "",
    "text": "We will be using one of the datasets available from Public Health Scotland, called Cancelled Planned Operations by Health Board.\nHere are the functions we’ll be covering in the the videos:\n\n base::library() we’re already familiar with for loading packages\n readr::read_csv() for importing CSV files\n here::here() is a function that helps R find files\n readxl::read_excel() for importing Excel files\n\n\n artwork by Allison Horst\nNote: The word before the ::  is the name of the package and the word after the ::  is the name of a function from that package, in the format: package::function()",
    "crumbs": [
      "Content",
      "Week 2: Data Tidying and Wrangling",
      "Topic 2: Importing Data"
    ]
  },
  {
    "objectID": "content/week-2/topic-2.html#csv-files",
    "href": "content/week-2/topic-2.html#csv-files",
    "title": "Importing Data",
    "section": "CSV Files",
    "text": "CSV Files\nWatch the following video to find out how to import CSV files using R code and also using the import wizard, with some extra tips to help you deal with dates.\nIf you want to recreate the session in the video on your own computer, you can download the cancelled_operations project folder here: cancelled_operations.zip Note: To open the project, make sure to “unzip” or “extract all” from the folder, before clicking on the cancelled_operations.Rproj file to open the project in RStudio.\nAlternatively, here are the individual files, but to recreate the file structure, you will need to create your own RStudio project and within this a “data” folder to store examples 2, 3 and 4 in, for the examples to work:\n\n\nWarning: package 'downloadthis' was built under R version 4.4.3\n\n\n\n phs_cancelled1.csv\n\n\n\n\n\n\n\n phs_cancelled2.csv\n\n\n\n\n\n\n\n phs_cancelled3.csv\n\n\n\n\n\n\n\n phs_cancelled4.csv\n\n\n\n\n\nDownload the video transcript link here.\n\nUncommon File Formats\nIn the video on how to import CSV files above, we show you how to use the import wizard to provide a preview of how R might read in your file.\nThe example CSV file we are using is in the common format where values are separated by commas, but as mentioned briefly in the video, if your file uses a different delimeter (separator), for example, dots or spaces or semi-colons, you can specify this in the import options of the wizard.\n\n\nExcel Files\nWatch the following video to find out how to import Excel files using R code and also using the import wizard.\nIf you want to recreate the session in the video on your own computer, you can download the cancelled_operations project folder here: cancelled_operations_excel.zip Note: To open the project, make sure to “unzip” or “extract all” from the folder, before clicking on the cancelled_operations.Rproj file to open the project in RStudio.\nAlternatively, here is the individual file, but to recreate the file structure, you will need to create your own RStudio project and within this a “data” folder to store the file for it to work phs_cancelled.xlsx\n\nDownload the video transcript link here.\nNote: the readxl package was developed fairly recently so if you are searching on the internet to find solutions to problems reading in Excel files, make sure to include the package name, or limit the time range.\n\n\nOpen Data\nOpen data refers to information made freely available to the general public and can be used by anyone for any purpose. The government and other public bodies such as the NHS are increasingly making datasets available to the public on the web.",
    "crumbs": [
      "Content",
      "Week 2: Data Tidying and Wrangling",
      "Topic 2: Importing Data"
    ]
  },
  {
    "objectID": "content/week-2/topic-2.html#benefits-of-sharing",
    "href": "content/week-2/topic-2.html#benefits-of-sharing",
    "title": "Importing Data",
    "section": "Benefits of sharing",
    "text": "Benefits of sharing\nThere are many benefits to opening up healthcare data to the public, including greater transparency and the empowerment of patients to be more involved in their own healthcare. But perhaps more importantly still, these datasets are a rich source of information which data scientists and analysts from around the world can tap into in order to improve our understanding of health and disease. After all, Data Saves Lives!\n\nShow me more data!\nThroughout this course, we are using open data freely available from the Public Health Scotland website, but there are many other excellent sources of open data available online too, here are a few:\n\nNHS Open Data Portal: launched in March 2020 to improve access to healthcare data\nWHO: datasets based on global health priorities\nKaggle: a whole variety of topics, not just healthcare, excellent for practicing on\nUK Government: contains nearly 25,000 data sets from all central government departments and a number of other public sector bodies and local authorities\nUK Data Archive: larger datasets\nGlobal Burden of Disease: a catalog of surveys, censuses, vital statistics, and other global health-related data\n\nLet’s look at how we can import data from the web directly from inside RStudio.\n\n\nRecap Download and Import\nThe open data we’re going to import is one you’ll be very familiar with by now, from the Public Health Scotland website. It can be found at the following link:\nwww.opendata.nhs.scot/dataset/cancelled-planned-operations\nAs you can see from the screenshot below, the CSV files can be manually downloaded from the website.\n\nIn order to start looking at them in R, you would then have to save your file to a relevant folder on your computer, then from within R, call on the file using the read_csv() command, making sure to include the correct path location.\nThis is what we did when learning how to read in CSV files from our computer (see the Import CSV video above). Let’s just remind ourselves of how we did this:\n\n# Load the tidyverse  \nlibrary(tidyverse) \n\nread_csv(\"data/phs_cancelled.csv\")\n\n\n\nImport CSV files directly from the web\nThe great news is that you can save time by extracting the data directly from the website using R. The code for this should look very familiar to you as this is the method we have been using frequently in many of the examples. There is no need to download the data first.\nTry running the following code in your RStudio environment, where instead of a folder and file name as our argument, we have the URL address from the website.\n\n library(tidyverse)\n \n # Import the open data csv file on cancelled operations\n read_csv(\"https://www.opendata.nhs.scot/dataset/479848ef-41f8-44c5-bfb5-666e0df8f574/resource/0f1cf6b1-ebf6-4928-b490-0a721cc98884/download/cancellations_by_board_february_2022.csv\")\n\nNote: You can copy the URL address by right-clicking on the download option and selecting **Copy link address. See screenshot below:\n\n\nHow does it know?\nHow are we able to use the same command to extract data from our own local computer or from a remote web page? R is clever enough to be able to work out what you want. If it detects that you have used a URL, it carries out a different underlying operation than if you had included a local file path.\n\n\n\nStay up to date\nExtracting the data directly from the website, means that if the underlying dataset on the website is updated, you will be able to quickly rerun the code to immediately update your own analysis in R too.\nIn the code above we imported the data but we didn’t save the result as an object. If we want to carry on exploring this data we need to store the result.\n\n # Load the tidyverse  \n library(tidyverse) \n \n # Store the open data csv file as \"cancelled_ops\"\n cancelled_ops &lt;- read_csv(\"https://www.opendata.nhs.scot/dataset/479848ef-41f8-44c5-bfb5-666e0df8f574/resource/0f1cf6b1-ebf6-4928-b490-0a721cc98884/download/cancellations_by_board_february_2022.csv\")\n \n # Print \"cancelled_ops\" in order to view it \n cancelled_ops \n \n dim(cancelled_ops)\n\nRemember, if you store or save an object to your environment, R doesn’t immediately display the output but it is saved for future reference. On the other hand, if you run code without saving it as an object, you will see the output immediately, but it won’t be saved for future reference.\nIn the code above, we have first saved the object, and then “printed” it too so that we can view the output immediately.\n\n\nDatabases\nThere are various R packages designed to help you connect to different types of databases. We don’t have time to cover these in this course but if you would like to find out more, here are a few pointers:\n\nfor MySQL databases have a look at the RMySQL package\nfor PostgresSQL, the RPostgresSQL package\nfor Oracle, the ROracle package\n\nYou get the idea!\nYou will also need to have a look at the DBI package which will allow you to access and interrogate your database.",
    "crumbs": [
      "Content",
      "Week 2: Data Tidying and Wrangling",
      "Topic 2: Importing Data"
    ]
  },
  {
    "objectID": "content/week-2/topic-2.html#practice",
    "href": "content/week-2/topic-2.html#practice",
    "title": "Importing Data",
    "section": "Practice",
    "text": "Practice\nHere is the link to a dataset on the delivery methods for births in Scottish hospitals from the Public Health Scotland open data store:\nBirths in Scottish hospitals\nTry reading in the data in RStudio, by using read_csv()",
    "crumbs": [
      "Content",
      "Week 2: Data Tidying and Wrangling",
      "Topic 2: Importing Data"
    ]
  },
  {
    "objectID": "content/week-2/topic-4.html",
    "href": "content/week-2/topic-4.html",
    "title": "Ethical Dilemmas in Health Care Data",
    "section": "",
    "text": "Spend some time considering ethical dilemmas in health care and health care data from some time ago, more recently and during Covid-19:\n\nGuttman N (1997). Ethical dilemmas in health campaigns. Health Communication, 9(2), 155-190.\nRaine J, Schneider JK, & Lorenz RA (2018). Ethical dilemmas in nursing: An integrative review. Journal of Clinical Nursing, 27(19-20), 3446-3461.\nDoyal L & Muinzer, T (2011) Should the skeleton of “the Irish giant” be buried at sea? BMJ 2011; 343 doi: https://doi.org/10.1136/bmj.d7597 & https://www.youtube.com/watch?v=u5sRJgBQyz4\n\n\nThe giant’s cause\n\n\n\n\n\nVayena E, Blasimme A, Cohen IG (2018) Machine learning in medicine: Addressing ethical challenges. PLoS Med 15(11): e1002689. https://doi.org/10.1371/journal.pmed.1002689\nJeffrey DI (2020) Relational ethical approaches to the COVID-19 pandemic. Journal of Medical Ethics 2020;46:495-498.",
    "crumbs": [
      "Content",
      "Week 2: Data Tidying and Wrangling",
      "Topic 4: Ethical Dilemmas in Health Care Data"
    ]
  },
  {
    "objectID": "content/week-2/tutorial.html",
    "href": "content/week-2/tutorial.html",
    "title": "Tutorial Materials",
    "section": "",
    "text": "Student led discussion on Ethical Dilemmas in Health Care Data\n1_warmup.Rmd (chick diets)\nExercise reading in files",
    "crumbs": [
      "Content",
      "Week 2: Data Tidying and Wrangling",
      "Tutorial materials"
    ]
  },
  {
    "objectID": "content/week-2/tutorial.html#tutorial-materials-for-thursdays-lab",
    "href": "content/week-2/tutorial.html#tutorial-materials-for-thursdays-lab",
    "title": "Tutorial Materials",
    "section": "",
    "text": "Student led discussion on Ethical Dilemmas in Health Care Data\n1_warmup.Rmd (chick diets)\nExercise reading in files",
    "crumbs": [
      "Content",
      "Week 2: Data Tidying and Wrangling",
      "Tutorial materials"
    ]
  },
  {
    "objectID": "content/week-2/tutorial.html#tutorial-materials-for-fridays-lab",
    "href": "content/week-2/tutorial.html#tutorial-materials-for-fridays-lab",
    "title": "Tutorial Materials",
    "section": "Tutorial materials for Friday’s lab",
    "text": "Tutorial materials for Friday’s lab\n2_dplyr.Rmd\nMore dplyr exercises\nWhole game exercises (rapid overview of the main tools of data science: import, tidy, transform, visualise, model, communicate).",
    "crumbs": [
      "Content",
      "Week 2: Data Tidying and Wrangling",
      "Tutorial materials"
    ]
  },
  {
    "objectID": "content/week-3/index.html",
    "href": "content/week-3/index.html",
    "title": "Overview",
    "section": "",
    "text": "Welcome to Week 3 of the course, focussing on ‘Data visualisation and storytelling’.\nThis week you will learn about key concepts in visualisation and storytelling and know how to create customised plots in R.",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Overview"
    ]
  },
  {
    "objectID": "content/week-3/index.html#learning-outcome",
    "href": "content/week-3/index.html#learning-outcome",
    "title": "Overview",
    "section": "Learning Outcome",
    "text": "Learning Outcome\nBy the end of this week, you should have a critical awareness of:\n1. Data visualisation and its aims\n2. How data visualisation fits into data storytelling\n3. Explain and demonstrate the use of the key elements required when creating visualisations in ggplot2",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Overview"
    ]
  },
  {
    "objectID": "content/week-3/topic-2.html",
    "href": "content/week-3/topic-2.html",
    "title": "Data visualisation aims",
    "section": "",
    "text": "Data visualisation can be used for:\nThere are so many different pieces of information you can try to display using data visualisation, such as proportions, magnitudes, associations between multiple variables, differences between groups, and more. But for all of these objectives, there is also a spectrum of broader aims from objectivity to subjectivity.\nIf you were to be truly objective, hypothetically you would be presenting raw data with no processing conducted. In practice, this makes it impossible to get any message across. Any time you conduct any data processing, like removing rows or renaming variables, you are presenting some opinion about how the information should be presented best to get your message across. On the other hand, the most subjective approach would be simply to state your conclusion, with no explanation of how you reached that conclusion.\nIn practice, data visualisation happens within the middle of this space. You present the data in some way that allows the reader to easily absorb some message, without losing so much data that the message can not be validated.\nThere are numerous (seemingly endless) decisions you must make when creating a data visualisation, particularly when that data visualisation will then be used for data storytelling.",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 2: Data visualisation aims"
    ]
  },
  {
    "objectID": "content/week-3/topic-2.html#data-visualisation-principles-cheat-sheet",
    "href": "content/week-3/topic-2.html#data-visualisation-principles-cheat-sheet",
    "title": "Data visualisation aims",
    "section": "Data visualisation principles cheat sheet",
    "text": "Data visualisation principles cheat sheet\nThere is a super useful graphics cheat sheet by Marc Vandemeulebroecke, Mark Baillie, Alison Margolskee, and Baldur Magnusson, which you can use as a reference for some of the different considerations in creating an effective data visualisation.\nIn fact, the authors created the visualisation examples in R and the ggplot2 package in particular and shared much of the example code on github.",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 2: Data visualisation aims"
    ]
  },
  {
    "objectID": "content/week-3/topic-4.html",
    "href": "content/week-3/topic-4.html",
    "title": "Plotting data with ggplot2",
    "section": "",
    "text": "Plotting in R is great because your code will always be there for you to check and reproduce what you did, and even better than that, you’ve already seen that you can start creating nice-looking plots straight away with just a few lines of code.\nIt’s extremely satisfying to be able to produce multiple plots with consistent formatting by just changing one line here or there, and not having to remember what or where you clicked in a menu!\nThe R package we use most frequently for plotting is ggplot2, and it’s part of the tidyverse which you’ll now be starting to get quite familiar with.\nLet’s start by looking at the main commands you need to know in ggplot2 and how to use these commands in order to display your data and format your plot the way you want.\n\n\n\n\n\nartwork by Allison Horst\n# Filter out/remove NA values (which were the code for a specific hospital) from HBName (SB0801 before the join)\n\n\nAfter opening a new script or RMarkdown file, the next thing we have to do is import our data.\nSince we’re using the same data we have used before, the Cancelled Planned Operations dataset from Public Health Scotland, we can just copy some of our previous code (look back at Week 2, topic 6). This is one of the great things about coding!\nYou’ll spend a lot of time copying previous code you’ve written, copying code you’ve Googled on the web, or copying a colleague’s code. Don’t worry about trying to learn commands from memory. This will happen naturally for the ones you use most frequently, and for the rest, there is always Google or the Help tab!\nCopy and paste the following code into your R Script or into a code chunk in your RMarkdown file.\n\n# Load packages \nlibrary(tidyverse)\n# library(lubridate)\n## library(lubridate) is only necessary if you are using Noteable, the newest version of tidyverse includes lubridate. If using Noteable, delete the # comment in the line above  \n \n# Read in the cancellations dataset \ncancelled_raw &lt;- read_csv(\"https://www.opendata.nhs.scot/dataset/479848ef-41f8-44c5-bfb5-666e0df8f574/resource/0f1cf6b1-ebf6-4928-b490-0a721cc98884/download/cancellations_by_board_august_2024.csv\")\n \nhb &lt;- read_csv(\"https://www.opendata.nhs.scot/dataset/9f942fdb-e59e-44f5-b534-d6e17229cc7b/resource/652ff726-e676-4a20-abda-435b98dd7bdc/download/hb14_hb19.csv\")\n \ncancelled &lt;- cancelled_raw %&gt;%\n  # Join cancelled to hb\n  left_join(hb, by = c(\"HBT\" = \"HB\")) %&gt;%\n  # Select the variables we're interested in\n  select(Month,\n         HBName,\n         TotalOperations,\n         TotalCancelled) %&gt;%\n  # Filter out/remove NA values (which were the code for a specific hospital) from HBName (SB0801 before the join) \n  filter(HBName != \"NA\") %&gt;%\n  # Reformat the month column to separate out year, month, day \n  mutate(Month = ymd(Month, truncated = TRUE))\n\nWe don’t need to load ggplot2 separately because it is loaded as part of the tidyverse set of packages.\nHaving run that code, you should now see three objects in your environment tab. The two initial datasets we read in, and then our tidied dataset, cancelled.\n\n\n\n\n\nCheck the data\nClick on cancelled to check that it has been read in ok. Remember we can do this by clicking on the object in the Environment tab and it will open in a new tab beside your script, looking just like a spreadsheet.\nWe can also check the range of our data to see what time span is covered:\nHere we are using the year() function from lubridate to extract only the year from the part of the date in the Month variable.\nYou should see the following output\n\ncancelled %&gt;%\n   distinct(year(Month))\n\n# A tibble: 11 × 1\n   `year(Month)`\n           &lt;dbl&gt;\n 1          2015\n 2          2016\n 3          2017\n 4          2018\n 5          2019\n 6          2020\n 7          2021\n 8          2022\n 9          2023\n10          2024\n11          2025\n\n\nWe can use a similar bit of code to look at the Health Boards:\nFrom which you should see the following output\n\ncancelled %&gt;%\n   distinct(HBName)\n\n# A tibble: 14 × 1\n   HBName                       \n   &lt;chr&gt;                        \n 1 NHS Ayrshire and Arran       \n 2 NHS Borders                  \n 3 NHS Dumfries and Galloway    \n 4 NHS Fife                     \n 5 NHS Forth Valley             \n 6 NHS Grampian                 \n 7 NHS Greater Glasgow and Clyde\n 8 NHS Highland                 \n 9 NHS Lanarkshire              \n10 NHS Lothian                  \n11 NHS Orkney                   \n12 NHS Shetland                 \n13 NHS Tayside                  \n14 NHS Western Isles            \n\n\nNotice that in both of these chunks of code, we have not assigned the value to an object. That is, we have not given them names using the assignment operator &lt;- and so they have not appeared in our environment.\nThat’s because we’re just doing a quick check. This code is not particularly relevant to our plotting or analysis code. So we can either write and run it in a code chunk, then delete it if we don’t want to keep it there, or we can run it in the console so that it doesn’t get saved in our RMD file. Either is fine.",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 4: Plotting data with ggplot2"
    ]
  },
  {
    "objectID": "content/week-3/topic-4.html#the-ggplot2-package",
    "href": "content/week-3/topic-4.html#the-ggplot2-package",
    "title": "Plotting data with ggplot2",
    "section": "",
    "text": "Plotting in R is great because your code will always be there for you to check and reproduce what you did, and even better than that, you’ve already seen that you can start creating nice-looking plots straight away with just a few lines of code.\nIt’s extremely satisfying to be able to produce multiple plots with consistent formatting by just changing one line here or there, and not having to remember what or where you clicked in a menu!\nThe R package we use most frequently for plotting is ggplot2, and it’s part of the tidyverse which you’ll now be starting to get quite familiar with.\nLet’s start by looking at the main commands you need to know in ggplot2 and how to use these commands in order to display your data and format your plot the way you want.\n\n\n\n\n\nartwork by Allison Horst\n# Filter out/remove NA values (which were the code for a specific hospital) from HBName (SB0801 before the join)\n\n\nAfter opening a new script or RMarkdown file, the next thing we have to do is import our data.\nSince we’re using the same data we have used before, the Cancelled Planned Operations dataset from Public Health Scotland, we can just copy some of our previous code (look back at Week 2, topic 6). This is one of the great things about coding!\nYou’ll spend a lot of time copying previous code you’ve written, copying code you’ve Googled on the web, or copying a colleague’s code. Don’t worry about trying to learn commands from memory. This will happen naturally for the ones you use most frequently, and for the rest, there is always Google or the Help tab!\nCopy and paste the following code into your R Script or into a code chunk in your RMarkdown file.\n\n# Load packages \nlibrary(tidyverse)\n# library(lubridate)\n## library(lubridate) is only necessary if you are using Noteable, the newest version of tidyverse includes lubridate. If using Noteable, delete the # comment in the line above  \n \n# Read in the cancellations dataset \ncancelled_raw &lt;- read_csv(\"https://www.opendata.nhs.scot/dataset/479848ef-41f8-44c5-bfb5-666e0df8f574/resource/0f1cf6b1-ebf6-4928-b490-0a721cc98884/download/cancellations_by_board_august_2024.csv\")\n \nhb &lt;- read_csv(\"https://www.opendata.nhs.scot/dataset/9f942fdb-e59e-44f5-b534-d6e17229cc7b/resource/652ff726-e676-4a20-abda-435b98dd7bdc/download/hb14_hb19.csv\")\n \ncancelled &lt;- cancelled_raw %&gt;%\n  # Join cancelled to hb\n  left_join(hb, by = c(\"HBT\" = \"HB\")) %&gt;%\n  # Select the variables we're interested in\n  select(Month,\n         HBName,\n         TotalOperations,\n         TotalCancelled) %&gt;%\n  # Filter out/remove NA values (which were the code for a specific hospital) from HBName (SB0801 before the join) \n  filter(HBName != \"NA\") %&gt;%\n  # Reformat the month column to separate out year, month, day \n  mutate(Month = ymd(Month, truncated = TRUE))\n\nWe don’t need to load ggplot2 separately because it is loaded as part of the tidyverse set of packages.\nHaving run that code, you should now see three objects in your environment tab. The two initial datasets we read in, and then our tidied dataset, cancelled.\n\n\n\n\n\nCheck the data\nClick on cancelled to check that it has been read in ok. Remember we can do this by clicking on the object in the Environment tab and it will open in a new tab beside your script, looking just like a spreadsheet.\nWe can also check the range of our data to see what time span is covered:\nHere we are using the year() function from lubridate to extract only the year from the part of the date in the Month variable.\nYou should see the following output\n\ncancelled %&gt;%\n   distinct(year(Month))\n\n# A tibble: 11 × 1\n   `year(Month)`\n           &lt;dbl&gt;\n 1          2015\n 2          2016\n 3          2017\n 4          2018\n 5          2019\n 6          2020\n 7          2021\n 8          2022\n 9          2023\n10          2024\n11          2025\n\n\nWe can use a similar bit of code to look at the Health Boards:\nFrom which you should see the following output\n\ncancelled %&gt;%\n   distinct(HBName)\n\n# A tibble: 14 × 1\n   HBName                       \n   &lt;chr&gt;                        \n 1 NHS Ayrshire and Arran       \n 2 NHS Borders                  \n 3 NHS Dumfries and Galloway    \n 4 NHS Fife                     \n 5 NHS Forth Valley             \n 6 NHS Grampian                 \n 7 NHS Greater Glasgow and Clyde\n 8 NHS Highland                 \n 9 NHS Lanarkshire              \n10 NHS Lothian                  \n11 NHS Orkney                   \n12 NHS Shetland                 \n13 NHS Tayside                  \n14 NHS Western Isles            \n\n\nNotice that in both of these chunks of code, we have not assigned the value to an object. That is, we have not given them names using the assignment operator &lt;- and so they have not appeared in our environment.\nThat’s because we’re just doing a quick check. This code is not particularly relevant to our plotting or analysis code. So we can either write and run it in a code chunk, then delete it if we don’t want to keep it there, or we can run it in the console so that it doesn’t get saved in our RMD file. Either is fine.",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 4: Plotting data with ggplot2"
    ]
  },
  {
    "objectID": "content/week-3/topic-4.html#the-core-elements",
    "href": "content/week-3/topic-4.html#the-core-elements",
    "title": "Plotting data with ggplot2",
    "section": "The core elements",
    "text": "The core elements\n\nThe Elements of a ggplot\nThe first command you’ll need for creating any type of plot is the command ggplot(). The “gg” in ggplot stands for “grammar of graphics” and it has to do with the layering framework which we use to build up our plots.\nWe’re familiar with the grammar of the English language, which refers to the structure of the language and the rules we use to construct meaningful sentences. Similar concepts have been applied to plotting with ggplot2.\nThere is a structure of layers, a bit like the adjectives, nouns, and verbs, in the English language, and then there are the rules for how we assemble these layers (also called aesthetic mappings).\nWe’ll look in more depth at how these elements interact later in this topic, but for just now it’s worth noting that this setup makes it a lot easier to produce informative plots in R.\nThere are 7 main elements or layers we can use in our plots, and 3 of these are essential, without which, the plot just won’t work. We’ll explore the essential elements first, then go on to look at a few additional ones too.\nThe 3 essential elements are:\n\nthe data - fairly obvious, without this we wouldn’t have a plot!\nthe aesthetics - how we map our data onto the canvas (e.g., x and y variables).\nthe geometrical objects - the type of plot we want to make, (e.g., scatter, bar, line, etc.).\n\nSome of the non-essential elements include:\n\na facet layer - allows us to subset our data so that we can view multiple plots side by side, and arranged by category.\na theme layer, which will provide the finishing touch to make our plots look lovely.\n\n\n\n\n\n\n\n\nThe Canvas (data and aesthetics)\nLet’s start with the plotting function ggplot() which lets R know we are about to create a plot. And we’ll specify which variables we want to display on the x and y axes. Copy and paste the following code below the last bit of code in your script:\n\n cancelled %&gt;%\n   ggplot(aes(x = Month, y = TotalOperations))\n\n\n\n\n\n\n\n\nA blank canvas should have appeared in your Plots tab or under your code chunk if you have your output showing inline in RMarkdown (the default). This is the first layer in our plot and lets us know… that R knows… that we want to create a plot, but there are no data points yet.\n\nthe data cancelled is being piped into our ggplot() function, the starting point for all our plots\nthe aesthetics are present within the aes() function letting R know what we want on the x and y axes\nthe geometrical objects layer is missing!\n\nWithout the geometrical layer, R doesn’t know what type of plot to make.\n\n\nThe Geoms\nWe use, what are called, geoms (short for geometrical objects or geometries) to let R know the type of plot we want.\nTo add a layer in ggplot2, we have to make sure to add a + sign at the end of each line. This + sign behaves in a very similar way to the %&gt;% operator we are already familiar with, and indeed there has been some discussion surrounding the development of the ggplot2 package, as to whether the %&gt;% should be used instead of the + but for various reasons the + remains. We just need to remember that everything after the ggplot() function needs a + sign.\nLet’s add a geom, and since we’re looking at changes over time, let’s add a geom to create a line graph, geom_line():\n\ncancelled %&gt;%\n   ggplot(aes(x = Month, y = TotalOperations)) +\n    geom_line()\n\n\n\n\n\n\n\n\nNow we have a complete plot, with only 3 short lines of code! But, our lines don’t look right. That’s because for each date in our dataset, we have observations or values for 14 different Health Boards, and ggplot2 joins the points for all of these Health Board values first before moving on to the next date and so forth. To correct this, all we need to do is add a group mapping within our aes() function. This lets R know that each category within the specified group gets its own line:\n\ncancelled %&gt;%\n  ggplot(aes(x = Month, y = TotalOperations, group = HBName)) +\n  geom_line()\n\n\n\n\n\n\n\n\nNow we have a complete plot, with only 3 short lines of code! But, our lines don’t look right. That’s because for each date in our dataset, we have observations or values for 14 different Health Boards, and ggplot2 joins the points for all of these Health Board values first before moving on to the next date and so forth. To correct this, all we need to do is add a group mapping within our aes() function. This lets R know that each category within the specified group gets its own line:\n\ncancelled%&gt;%\n  ggplot(aes(x = Month, y =TotalOperations, group =HBName)) +\n  geom_line()\n\n\n\n\n\n\n\n\nThere are various geoms that we can use, and we’ll cover these in more detail later in the topic. For example, we have geoms for bar charts, line plots, heat maps, scatter plots, box plots, and density plots. You name it, there’s probably a geom for it! There are currently at least 40 geoms from which to choose!",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 4: Plotting data with ggplot2"
    ]
  },
  {
    "objectID": "content/week-3/topic-4.html#the-aesthetics-inside-and-outside",
    "href": "content/week-3/topic-4.html#the-aesthetics-inside-and-outside",
    "title": "Plotting data with ggplot2",
    "section": "The aesthetics: Inside and outside",
    "text": "The aesthetics: Inside and outside\n\nThe Aesthetics - Outside\nThe aesthetics is where all the cool stuff happens in ggplot2. This is where the values in our columns (our variables) are converted into visual scales, be that x- or y-axis scales, or colour or size scales. The aes() function allows us to access and visualise large amounts of information quickly and efficiently.\n\ncancelled %&gt;%\n  ggplot(aes(x = Month, y = TotalOperations, group = HBName)) +\n  geom_line(color = \"darkgreen\")\n\n\n\n\n\n\n\n\nWhat we can include or not include within the aesthetic argument however, is sometimes slightly confusing, and an argument such as colour, which we see here, has a slightly different purpose depending on where it appears in our code, so let’s dive a little bit deeper into this.\nAesthetic-type arguments, e.g. line type and colour, can be specified both inside, and outside of the aesthetic brackets ( aes() ), but they have different roles to play in the different settings.\nIn the code above we can see that the colour aesthetic is set to “darkgreen” which lets R know that we want all of the lines to be dark green.\nR understands a lot of colours by their name, so try out a few and see how good it is! Change the code above to be a color of your choosing - but do not forget the quotation marks around the name (demarking a character string). For a more nuanced choice of colours, R also understands hex (hexadecimal) codes so you have a full range of colours at your fingertips. More on colour palettes later.\nComing back to our code example above, we can see that the colour argument is not within the aesthetic brackets, it’s included instead, inside geom_line(). This is because it has been set to a single value (“darkgreen”), and so that colour is the same for all points in the dataset, regardless of the values in the dataset.\nIf we wanted our data points to have a different shape or colour, depending on the values in the dataset, we would have to pass colour as an aesthetic argument, inside the brackets.\n\n\nThe Aesthetics - Inside\nAt the moment, our plot is very busy, it’s hard to distinguish between the different Health Boards and it would be useful to make them more visible by assigning each a different colour, so let’s add colour = HBName within our aesthetic mapping:\n\ncancelled %&gt;%\n  ggplot(aes(x = Month, y = TotalOperations, colour = HBName)) +\n  geom_line()\n\n\n\n\n\n\n\n\nNotice that we no longer need to specify group = HBName in our aesthetics because the colour attribute is both grouping and colouring each category within HBName.\nTo summarise, when our aesthetic arguments (colour, line type, etc.) appear:\n\nwithin aes(), the argument changes based on the values of the variable\noutside aes(), the argument is given a single value and doesn’t change based on the values of the variable\n\nThis ability to “map” within the aesthetic function allows us to visualise variations across multiple variables simultaneously.\nNotice that as well as assigning different colours to the different Health Boards, ggplot2 was also clever enough to create a legend automatically without us even telling it to do so!\n\n\nAesthetics galore!\nWe’ve seen how we can map a variable to color, to distinguish between different categories. Depending on the type of plot we are using, we can also map a variable to many other aesthetic arguments. For example, line thickness, the size of points, the shape of points, and how transparent they are (useful for overplotting in busy graphs). ggplot2 gives you a lot of control over these and you can find out more by searching for aes() in the help tab and clicking on the ggplot2-specs vignette, or by clicking here.\nOur plot now has colour but it’s still looking quite crowded so let’s explore how we can improve the presentation further.",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 4: Plotting data with ggplot2"
    ]
  },
  {
    "objectID": "content/week-3/topic-4.html#additional-elements-facets-and-themes",
    "href": "content/week-3/topic-4.html#additional-elements-facets-and-themes",
    "title": "Plotting data with ggplot2",
    "section": "Additional elements: Facets and themes",
    "text": "Additional elements: Facets and themes\n\nFaceting\nFaceting provides yet another dimension to our data and allows us to compare values within a plot and between plots, making it easier to see what’s going on.\nLet’s take a look at the code for faceting:\n\ncancelled %&gt;%\n  ggplot(aes(x = Month, y = TotalOperations)) +\n  geom_line(colour = \"darkgreen\") +\n  facet_wrap(~HBName)\n\n\n\n\n\n\n\n\nThe function facet_wrap() takes a formula argument, which just means that we need to remember to use the tilde sign (~) before the variable we’re interested in.\nWhen we use facet_wrap(), we’re creating separate graphs subset by the specified variable (in this case Health Board). It’s a way of easily splitting up our data into the various levels, in a variable with categories.\nAfter faceting, we can see that it’s very difficult to see the trends for the smaller Health Boards as all of the axes scales have been aligned. This can be useful when comparing categories but in this dataset, we might be interested in the overall trends across each individual Health Board.\nTo change this there is an argument within facet_wrap() which makes our axes “free”! Let’s add that in:\n\ncancelled %&gt;%\n  ggplot(aes(x = Month, y = TotalOperations)) +\n  geom_line(colour = \"darkgreen\") +\n  facet_wrap(~HBName, scales = \"free\")\n\n\n\n\n\n\n\n\nNow we can see very similar trends across all Health Boards concerning the recent drop in the total number of operations in 2020 and an increase around 2021.\n\n\nThemes\nThe default plot theme for ggplot2 has a grey background, but you do not have to be stuck with this, there are endless options for customising your plots, but one way you can do this quickly is by using ggplot2’s ready-to-go themes.\n\ncancelled %&gt;%\n  ggplot(aes(x = Month, y = TotalOperations)) +\n  geom_line(colour = \"darkgreen\") +\n  facet_wrap(~HBName, scales = \"free\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nIn the code above you can see that we have added another line of code, theme_bw()which calls on ggplot2’s black-and-white theme.\nThe theme layer controls all of the visual elements which are not directly related to the interpretation of the values in your data. So it doesn’t control the colours of your data points, but it can control the background colour on which your data points lie.\nThere are many built-in themes in ggplot2 and you can explore them and discover the ones that you like best.\nThe nice thing about the layering structure of ggplot2, is that you can take a default layer, like theme_bw(), and then add your own modifications on top of it, by calling the theme() command, so your plots are entirely customisable.\nYou can find out more about the generic theme()command in the help menu of RStudio, where there are endless options for customisation.",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 4: Plotting data with ggplot2"
    ]
  },
  {
    "objectID": "content/week-3/topic-4.html#geoms-galore",
    "href": "content/week-3/topic-4.html#geoms-galore",
    "title": "Plotting data with ggplot2",
    "section": "Geoms galore!",
    "text": "Geoms galore!\nSo far in this topic, we’ve explored some of the options we can use in geom_line() the geom used to create line graphs, but there are many more geoms to choose from.\nHere are some of the main ones you’re likely to find useful in your analyses and reporting:\ngeom_point() and geom_jitter() are scatter plots and are quite similar. geom_jitter() can be useful when you have over-plotting. It shifts overlapping points to the side slightly, making it easier to see the density in areas of clustering.\n\n\n\n\n\ngeom_bar() and geom_col() are also very similar but with a few slight differences to do with how data is counted. If you are interested in these counting differences, read on. If not, you can skip the following paragraph.\nIf you want the height of the bars to represent values in the data, use geom_col(). If rather, you would like the height of the bar to be proportional to the number of cases in each group, use geom_bar(). On a more technical note, this is because geom_bar()uses stat_count() by default meaning it counts the number of cases at each x position, while geom_col() uses stat_indentity() by default meaning it leaves the data as it is.\n\n\n\n\n\ngeom_histogram() is very useful for looking at the distribution of your data, and good for detecting any unusual observations or outliers. A histogram is used to visualise the distraction of a single continuous variable.\n\n\n\n\n\nNote: We wouldn’t normally use a histogram to explore time-series data in this way, but we’ve kept the same dataset for consistency.\ngeom_boxplot() is another useful plot for seeing how your values are spread out, particularly if you’re comparing a number of variables together or wish to compare between multiple groups.\nBoxplots are a standardized way of showing the distribution of data based on a five-number summary:\n\nminimum\nfirst quartile (Q1)\nmedian\nthird quartile (Q3)\nmaximum.\n\nThe “box” shows where the middle portion of the data is: the interquartile range (IQR).\n \nIf we would like to add labels or text to our plots, in addition to points and lines, or even to replace points, we can use geom_label() and geom_text().\n\nAn intentional omission…\nYou may have noticed that one of the most common, and controversial, types of charts has not yet been discussed: the pie chart.\nWhile it is possible to make pie charts in R, we will not be learning how to do so in the course. In fact, I hope to convince you that you should not even want to know how to make one! Pie charts are what we can consider bad by definition. A pie chart is a circle (or pie) divided into sections (slices of the pie) that represent a portion of the whole. Nothing offensive there. However, the issue is that humans are not good at reading and distinguishing angles. By definition, they are not human-readable and certainly not a tool to tell a data story.\nYan Holtz has a wonderful, and succinct, post called The issue with pie chart that I would recommend reading if you need more evidence.\n\n\nSelecting an appropriate geometry\nThere are many factors to consider when selecting the geometry or type of plot to make. Most importantly, you need to first consider your data and what options are available to you, reflecting on questions such as:\n\nwhat data types are my variables of interest (e.g., numeric/continuous, factor/categorical, etc.)?\nhow much, if any, data is missing from my variables of interest?\nwhat format is my data in, is it suitable for plotting?\nwhat is the goal of the visualisation (e.g., to show change over time, to compare values between groups, to show how data is distributed, to show a part-to-whole composition, etc.)\n\nThere is a wonderful web page called From Data to Viz (also by Yan Holtz) which can help you in this decision making process. Data To Viz serves as a comprehensive guide to chart types, organized by the format of the data you have. The web page effectively functions as a decision tree to help guide to showing actionable insights in your data visualisation. What I enjoy the most is that once you have decided on a chart type, there are recommendations and warnings to consider around that chart type as well as links to the R Graph Gallery with R code (largely ggplot2 and its extensions) and an accompanying explanation!",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 4: Plotting data with ggplot2"
    ]
  },
  {
    "objectID": "content/week-3/topic-4.html#optional-practice",
    "href": "content/week-3/topic-4.html#optional-practice",
    "title": "Plotting data with ggplot2",
    "section": "Optional: Practice 👩‍💻",
    "text": "Optional: Practice 👩‍💻\nIf you are feeling confident, why not see if you can reproduce the following plots?\nThink of the different elements needed to recreate the plots and use the materials in this topic to assist you! If you are struggling to know where to begin in recreating either plot, I would suggest moving on to the Plotting Practice Document in the next topic and then returning to this task.\nThe plots below are quite complex, so do share your results on the discussion board if you are able to replicate parts of them! We can then all learn from each other the different ways one could approach the tasks. Feel free to post on the discussion boards if you are having problems reproducing either plot below, or if you have any questions (such as how the plots above were made where the code is not shared)!\n\nPlot 1\n\n\n\n\n\nHint: You will need to use the dplyr::mutate() function, and some functions from the lubridate package!\n\n\nPlot 2\n\n\n\n\n\nHint: You will need to use the dplyr::mutate() function, and some functions from the lubridate package! Remember that functions typically have arguments to help you customize different things.\nHint 2: as we have discussed, ggplot2 works in layers, which means you can layer multiple elements on top of each other!",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 4: Plotting data with ggplot2"
    ]
  },
  {
    "objectID": "content/week-3/topic-4.html#optional-data-visualisation-using-ggplot2",
    "href": "content/week-3/topic-4.html#optional-data-visualisation-using-ggplot2",
    "title": "Plotting data with ggplot2",
    "section": "Optional: Data visualisation using ggplot2",
    "text": "Optional: Data visualisation using ggplot2\n\nData visualisation in R, an introduction with ggplot2\nThe following video is a recording from a guest lecture by Dr Sam Tyner introducing data visualisation in R using the ggplot2 package.\n\nHere are Dr Tyner’s slides.\nThe code used to produce the slides is available on GitHub.\nOn the penultimate slide, Dr Tyner lists many links for some additional resources, which are copied below for your convenience.\n\nggplot2 book\nR for Data science book\nTidy Tuesday\nMy advice for getting help in R\nThomas Lin Pedersen’s (one of the current maintainers of ggplot) ggplot2 webinar: part 1 and part 2\nAll RStudio Cheat Sheets\nWill Chase’s Design Talk\na 2-hour version of this tutorial\n\n\nExamples of Extensions\n\nDomain Specific\n\nNetworks\n\ngeomnet\nggraph\n\nTime Series\n\nggaluvial\nsugrrants\n\nSciences\n\nggenes\nggtree\nggseqlogo\nggspectra\n\n\n\n\nAppearance customisation\n\nArrange ggplots\n\ncowplot\npatchwork\ngganimate\n\nCustom themes and/or scales\n\nggthemes\nggsci\nggtech\nggthemr\nxkcd\nggpubr",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 4: Plotting data with ggplot2"
    ]
  },
  {
    "objectID": "content/week-3/topic-4.html#optional-creating-a-line-graph-in-ggplot2",
    "href": "content/week-3/topic-4.html#optional-creating-a-line-graph-in-ggplot2",
    "title": "Plotting data with ggplot2",
    "section": "Optional: Creating a line graph in ggplot2",
    "text": "Optional: Creating a line graph in ggplot2\nData Visualisation Demonstration in R\nIn the video below, Dr Holly Tibble, Chancellor’s Fellow in Medical Informatics at the University of Edinburgh will demonstrate how to plan, create, and describe a simple visualisation in R for asthma deaths in the UK throughout the year, between 2011 and 2012.\n\nHere is the R script and the transcript of the presentation.",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 4: Plotting data with ggplot2"
    ]
  },
  {
    "objectID": "content/week-3/topic-6.html",
    "href": "content/week-3/topic-6.html",
    "title": "More Wrangling and Plotting",
    "section": "",
    "text": "First, create a new project in RStudio, e.g. you could call it rmd_exercises.\nThen download the R Markdown and associated CSV files by right clicking on the links below and choosing, Save Link As, navigate to your newly created project folder, and save.\n\n\n\n   Download Wrangling and Summarising RMarkdown  \n   Download the wrangling dataset  \n\n\n\n   Download the plotting RMarkdown  \nNote: The data associated with each R Markdown document must be stored within the same project folder or R won’t be able to find it.\nIn RStudio, navigate to your newly created project folder and click to open your newly saved R Markdown document (you can use the … in the Files tab to browse for your folder).\nYou are ready to go!\n\n\n\nWhen you first open up your R Markdown document, it’s worth taking a quick look at the document outline to get an idea of the content covered and exercises to complete. To do this, you might remember, you can click on the Show document outline short cut as shown below:\n\n\n\nThere are two R Markdown documents, one covering more functions to help with data wrangling ( wrangling & summarising.Rmd ) and the other exploring different types of plotting in more depth ( plotting.Rmd).\nThe code and concepts covered in these two exercise documents are some of the most challenging that you will encounter in this course, so don’t worry if you don’t always fully understand what each code chunk is doing, this will come with practice. Sample solutions to the exercises are at the end of each document.\nWork through both sets of exercises and have fun experimenting. Artwork inspired by the functions used in each document can be found below, created by RStudio’s artist-in-residence, Allison Horst, to brighten up this Learn space. Enjoy!\nAnd remember, embrace error messages!",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 6: More Wrangling and Plotting"
    ]
  },
  {
    "objectID": "content/week-3/topic-6.html#download-r-markdown-document-into-an-rstudio-project",
    "href": "content/week-3/topic-6.html#download-r-markdown-document-into-an-rstudio-project",
    "title": "More Wrangling and Plotting",
    "section": "",
    "text": "First, create a new project in RStudio, e.g. you could call it rmd_exercises.\nThen download the R Markdown and associated CSV files by right clicking on the links below and choosing, Save Link As, navigate to your newly created project folder, and save.\n\n\n\n   Download Wrangling and Summarising RMarkdown  \n   Download the wrangling dataset  \n\n\n\n   Download the plotting RMarkdown  \nNote: The data associated with each R Markdown document must be stored within the same project folder or R won’t be able to find it.\nIn RStudio, navigate to your newly created project folder and click to open your newly saved R Markdown document (you can use the … in the Files tab to browse for your folder).\nYou are ready to go!\n\n\n\nWhen you first open up your R Markdown document, it’s worth taking a quick look at the document outline to get an idea of the content covered and exercises to complete. To do this, you might remember, you can click on the Show document outline short cut as shown below:\n\n\n\nThere are two R Markdown documents, one covering more functions to help with data wrangling ( wrangling & summarising.Rmd ) and the other exploring different types of plotting in more depth ( plotting.Rmd).\nThe code and concepts covered in these two exercise documents are some of the most challenging that you will encounter in this course, so don’t worry if you don’t always fully understand what each code chunk is doing, this will come with practice. Sample solutions to the exercises are at the end of each document.\nWork through both sets of exercises and have fun experimenting. Artwork inspired by the functions used in each document can be found below, created by RStudio’s artist-in-residence, Allison Horst, to brighten up this Learn space. Enjoy!\nAnd remember, embrace error messages!",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 6: More Wrangling and Plotting"
    ]
  },
  {
    "objectID": "content/week-4/data-engagements-and-controversies.html",
    "href": "content/week-4/data-engagements-and-controversies.html",
    "title": "Data Engagements & Data Controversies",
    "section": "",
    "text": "Critically review the arguments posed in this article:\nMittelstadt B (2019) Principles alone cannot guarantee ethical AI. Nat Mach Intell 1, 501–507.\nYou can download the article here",
    "crumbs": [
      "Content",
      "Week 4: Analysing and Presenting Data in R",
      "Optional: Data Engagements & Data Controversies"
    ]
  },
  {
    "objectID": "content/week-4/topic-1.html",
    "href": "content/week-4/topic-1.html",
    "title": "Wrangling factors and joins",
    "section": "",
    "text": "Factors were mentioned very briefly in Week 2 of this course.\nLet’s just recap. Here is how we described them:\nFactors can be thought of as slightly fussy characters. They’re fussy because they have something called levels. Levels are all the unique values this variable could take e.g. if we have a column with data on sex, there might be two levels, “Male” and “Female” or there might be three levels if there was an option to specify “Other” too. Using factors rather than just characters can be useful because:\n\nThe values that factor levels can take is fixed. For example, if the predefined levels of your column called  sex  are “Male” and “Female” and you try to add a new patient where sex is just called “F” you will get a warning from R. If the column  sex  was stored as a character data type rather than a factor, R would have no problem with this and you would end up with “Male”, “Female”, and “F” in your column.\nLevels have an order. By default R sorts things alphabetically, but if you want to use a non-alphabetical order, e.g. if we had a  body_weight  variable where we want the levels to be ordered - “underweight”-“normal weight”-“overweight” - we need make  body_weight  into a factor. Making a character column into a factor enables us to define and change the order of the levels. \n\nThese can be huge benefits, especially as a lot of medical data analyses include the comparison of different risks to a reference level. There is a handy cheatsheet around the forcats package in R, which provides tools for working with factor (or categorical data).",
    "crumbs": [
      "Content",
      "Week 4: Analysing and Presenting Data in R",
      "Topic 1: Wrangling factors and joins"
    ]
  },
  {
    "objectID": "content/week-4/topic-1.html#working-with-factors",
    "href": "content/week-4/topic-1.html#working-with-factors",
    "title": "Wrangling factors and joins",
    "section": "",
    "text": "Factors were mentioned very briefly in Week 2 of this course.\nLet’s just recap. Here is how we described them:\nFactors can be thought of as slightly fussy characters. They’re fussy because they have something called levels. Levels are all the unique values this variable could take e.g. if we have a column with data on sex, there might be two levels, “Male” and “Female” or there might be three levels if there was an option to specify “Other” too. Using factors rather than just characters can be useful because:\n\nThe values that factor levels can take is fixed. For example, if the predefined levels of your column called  sex  are “Male” and “Female” and you try to add a new patient where sex is just called “F” you will get a warning from R. If the column  sex  was stored as a character data type rather than a factor, R would have no problem with this and you would end up with “Male”, “Female”, and “F” in your column.\nLevels have an order. By default R sorts things alphabetically, but if you want to use a non-alphabetical order, e.g. if we had a  body_weight  variable where we want the levels to be ordered - “underweight”-“normal weight”-“overweight” - we need make  body_weight  into a factor. Making a character column into a factor enables us to define and change the order of the levels. \n\nThese can be huge benefits, especially as a lot of medical data analyses include the comparison of different risks to a reference level. There is a handy cheatsheet around the forcats package in R, which provides tools for working with factor (or categorical data).",
    "crumbs": [
      "Content",
      "Week 4: Analysing and Presenting Data in R",
      "Topic 1: Wrangling factors and joins"
    ]
  },
  {
    "objectID": "content/week-4/topic-1.html#factors-practice",
    "href": "content/week-4/topic-1.html#factors-practice",
    "title": "Wrangling factors and joins",
    "section": "Factors Practice",
    "text": "Factors Practice\nRemember to begin by creating a new project in RStudio.\nThen download the R Markdown document and associated CSV files by right clicking on the links below and choosing, Save Link As. Then navigate to your newly created project folder, and save.\nfactors.Rmd \nmelanoma_data.csv   \nNote: There is some optional, advanced Bonus Content at the end of this practice document.\nHave fun!\nOften the data you want to work with is spread between multiple tables or spreadsheets. Before you can begin analysing it, you need to know how to combine these datasets. There are a number of different ways you can join your data together, although you’ll probably find yourself using just a couple of these joins on a regular basis.\n The ones we’ll cover here are:\n\n left_join() \n inner_join() \n full_join() \n\nWhen joining two sets of data, R will look for a common column or columns on which to join. If it finds a name and data type match then it will join on this/these unless told otherwise.\nFor demonstration purposes, the code below creates two tibbles (tidyverse data frames), x and y with a column in common,  id , which we can imagine is patient id. Let’s also imagine that the two tibbles created here contain different information about a group of patients: x might be about outpatient appointments and y about treatments. You will see that we have intentionally included some patients in both tibbles but there are some only in tibble x and some only in tibble y. We now want to combine these tibbles for analysis.\n\n\n\n\n\n```{r}\n\n# create a new tibble called x\nx &lt;- tibble(id = c(1, 2, 3, 4, 5),\n            x = c(\"x1\", \"x2\", \"x3\", \"x4\", \"x5\"))\nx\n# create a new tibble called y\ny &lt;- tibble(id = c(3, 4, 5, 6, 7),\n            y = c(\"y3\", \"y4\", \"y5\", \"y6\", \"y7\"))\ny\n```\nWhich join you choose depends on which rows you want to keep and from which datasets. In the joins we’ll be discussing below, all columns from both datasets are kept. \n\nI want to keep all data from both x and y\n\n\n\n\n\n```{r}\nfull_join(x, y, by = \"id\")\n```\n\n\n\n\n\n\nI want to keep all data from x and only matching data from y\n\n\n\n\n\nChoose  left_join() which keeps all data from the (primary) dataset x and only adds data from (another) dataset y if a match is found with the dataset x. Where no matching value is found for x NA is returned in the y column(s).\n```{r}\nleft_join(x, y, by = \"id\")\n```\n\n\n\n\n\nIf there is more than one match between x and y (perhaps x is a list of patients and y is a list of medications each patients is on), all combinations of the matches are returned.\n\nI only want to keep data that is matched on both x and y\n\n\n\n\n\nChoose   inner_join() which keeps only rows of data where a match is found between x and y.\n```{r}\ninner_join(x, y, by = \"id\")\n```\n\n\n\n\n\nYou can also think of joins using Venn diagrams:\n\n\n\n\n\nTo find out more information on the full set of joins you can use, take a look in the Help tab under “join” (or type  ?join into the console). The data wrangling with dplyr and tidyr cheatsheet can also be quite helpful. \n\n\n\n\nSaving Wrangled Data, Objects, or Tables\nOnce you have wrangled your data and/or produced a table or object, you may wish to save and share it with stakeholders or collaborators. \nTo save a csv file, which can then be opened in Excel or other such software, you can use the write_csv() function, which was discussed in one of the Week 6 Topic 3 practice documents. This function works as follows: \n```{r}\nwrite_csv(dataobject, \"filename.csv\")\n```\nAlternatively, if you are saving data for your own purposes or to share with a colleague with also uses R, you can create an rda or RData file. rda files are a short form of RData files. The advantages of using RData files include \n\nmore quickly restoring data to R for further analysis or wrangling \nkeeping R specific information encoded in the data (e.g., attributes like factor level order, variable types, etc.) \nthe option to save multiple objects or tibbles in one file \n\nThe functions to save and load RData files are conveniently save() and load(), which work as follows:\n```{r}\n#to save one object\nsave(dataobject, \"filename.rda\")\n \n#to save more than obe object\nsave(dataobject1, dataobject2, \"filename.rda\")\n \n#to read in your RData file\nload(\"filename.rda\")\n```",
    "crumbs": [
      "Content",
      "Week 4: Analysing and Presenting Data in R",
      "Topic 1: Wrangling factors and joins"
    ]
  },
  {
    "objectID": "content/week-4/topic-3.html",
    "href": "content/week-4/topic-3.html",
    "title": "Roundup and recap",
    "section": "",
    "text": "At the beginning of this course we introduced R by highlighting one of its main attractions: reproducibility. And here we are at the end of the course still banging on about reproducibility, but that’s because it is so important.\nThere is a growing awareness of the importance of reproducibility in data processing and analysis, and thankfully the tools needed to generate reproducible outputs are more accessible than ever before, helping to make this easier to accomplish.\nThere are many ways in which you can ensure your work is reproducible. The first we would recommend, is to start using R, and the second is to become familiar with R Markdown, so you are well on your way!  \nHere is a reminder of some of the main points to bear in mind, to ensure your future self and others can easily and efficiently build on the analysis you have carried out in R:\n\nalways create an RStudio Project to work in\nuse raw data directly in R (no pre-editing with other software)\ninclude informative commenting\nensure code is easy to read with appropriate indentations and spacing\npipe  %&gt;%  with the Tidyverse\ngive your objects meaningful names\nwork in R Markdown for reporting\n\nLet’s look at these aspects in little more detail.\n\n\n\n\n\nBy now you should be familiar with the how and the why of creating Projects in RStudio as this has cropped up frequently throughout the course. Projects help to keep everything in one place, they help R to find all your files and folders easily, and they generally make your life a lot easier so you should definitely be using them. \n\n\n\n\n\nAlways use the raw data. If your data has come from a database or from a publicly updated data source, avoid the temptation of doing some quick and dirty data wrangling in Excel before importing into R. This would make your analysis less reproducible as you may not always remember exactly what steps you took during this initial phase and these steps would then need to be taken each time you wanted to repeat or carry out a similar operation. \n\n\n\n\n\nCode which is easy to decipher and understand can be revisited in the future and rerun more easily, but how many times have you looked back at scribbled notes you took in haste, or code you typed quickly, with no recollection of what it all means? By including informative commenting you can help your future self, or anyone else who might be looking at your code, understand what it is you were trying to do at each point in your script. It isn’t needed for all lines of code but should be used when there might be ambiguity, or to provide clarification, or remind yourself about something specific to do with what’s happening. The keyboard shortcut for commenting/uncommenting a line is Ctrl+Shift+C, or you can manually type  # .\n\n\n\n\n\nOn a similar theme, it is important to make your code easy to read by applying the appropriate indentations and spacing. The keyboard shortcut for quickly reformatting your code is Ctrl+I (first select your code, or use Ctrl+A to select all before doing Ctrl+I). If your script is long this is particularly important as it means you, or someone else, can scan the document more easily to find the relevant text or information. \n\n\n\n\n\nThe Tidyverse has featured heavily in this course and is another great way to make your code easier to read and understand due to the use of the  %&gt;%  operator. As we learned at the beginning of the course, the  %&gt;% reduces the need for nested functions, where each function is “nested” inside another making it potentially very difficult to debug. Instead the piped (  %&gt;% ) functions appear almost like a series of statements or instructions, which can be run line by line if needed. At the start of every R script or document, remember to include  library(tidyverse) .\n\n\n\n\n\nTo finish, just a final note about assigning your objects meaningful names. This is easily overlooked and can be surprisingly difficult but when given careful consideration, good object naming can make your life a lot easier, and again, make the code easier to understand and read. Avoid generic names such as data or plot as these can cause problems. This is particularly true if you are carrying out multiple analyses and have referred to “data” or “plot” somewhere else and not cleared your environment (Restart R often!). Also, when choosing names, no spaces or funky characters, stick to lowercase with underscores. Then be consistent, and you should be safe! \n\n\n\n\n\nR Markdown is one of the keys to producing reproducible analysis in R as it enables the code and the narrative to be easily combined, whilst providing great flexibility as to how the output might be displayed. It is certainly worth the effort becoming familiar with R Markdown which was covered in detail in Week 6: Topics 1 & 2. Creating a new R Markdown document is as simple as going to  File - New File - R Markdown - OK  then delete everything up to the first code chunk (it is useful to leave the first setup chunk) and you are ready to go!\n\n\nR Markdown is not only a powerful tool for ensuring reproducibility, but as you have already discovered, it includes many features which make it easy to create and share clear and professional looking documents for a wide range of audiences.\nHere are some things to remember when planning and designing a report with R Markdown:\n\nprovide a clear structure using Markdown section headers (#, ##, ###)\nbe conscious of the YAML, unlike R code, the YAML is very sensitive of extra spaces/formatting\nconsider the code output options depending on audience (`echo = TRUE` vs `echo = FALSE`)\nthink about the story you are trying to tell\nmake good use of figures\nuse tables appropriately\n\n\n\n\n\n\nIt is important to provide a clear structure in any report or document you are writing and R Markdown makes it easy to manage and display the structure through the use of headers. The hash symbol ( # ) is used in the Markdown sections (all the bits which aren’t code chunks or YAML) to indicate different levels of header, so how big or small the text should be. These headers also show up in the document outline which is extremely useful as a means of navigating your document when editing, helping to save you time.\n\n# global chunk options`\n\nknitr::opts_chunk$set(echo = TRUE)\n# load packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(here)\n\nhere() starts at C:/Users/ldababn/OneDrive - University of Edinburgh/job/GitHub/website-ug-data-science\n\n\nThe structure of your R code chunks is important too. For example, it is good practice to have a code chunk near the top of your document (often in the setup chunk) where you keep all the packages that you are using together. It might look something like the image on the right.  The next code chunk is often where you might choose to load your data and is likely to include the code similar to:  mydata &lt;- read_csv(here(“data”, “appointments.csv”))\n\n\n\n\n\nWe’ve already mentioned that Markdown headers ( # ) make it easy to define a clear structure in your R Markdown document. Explore some of the other Markdown features, such as **bold** and *italic* or `inline R code`, which can help to give your final document a high quality finish. Remember, you can access the Markdown Quick Reference guide from the Help menu.\nIn its most basic form, the YAML which appears at the top of your R Markdown document, could simply tell R Markdown what output format you would like your document to be in, in which case it would look like this, indicating HTML output:\n---\noutput: html_document\n---\n\n\n\n\n\nHowever, you have the potential to add many attractive features via the YAML, such as table of contents, section numbering, themes, and much more. You have to be particularly careful about indentation in the YAML (unlike in R code chunks where indentation is mainly for readability) but you can start exploring YAML options easily through changing settings in the document cog and this will automatically update your YAML.\nDepending on who your intended audience is, you have a number of output options available to you. You can choose whether or not to show or run your code in the final document. Showing you code in the Markdown output document (HTML, Word, PDF) is denoted `echo = TRUE`, not showing it is denoted `echo = FALSE`. You also have control over whether to display the output of each code chunk or not too. These options can be accessed from the individual code chunk cogs or can be set for the whole document in the setup chunk.\n\n\n\n\n\nWhen creating a report or document, think about what story you are trying to tell with the data; you might be answering a specific question or perhaps highlighting a change in activity over a period of time. R Markdown makes it easy to intersperse clear and informative visual elements throughout the narrative. These could be in the form of figures and plots, nicely formatted tables or even images (e.g. JPG or PNG).\nWhen writing the code to produce a plot, remember you don’t need to assign it to an object. For example, running  myplot &lt;- mydata %&gt;% ggplot()  will result in no plot appearing in your final document, it is saved in the Environment. Instead you can simply write  mydata %&gt;% ggplot() which will ensure your plot does appear. You can also control the size of your plots and figures in the code chunk settings cog.\nWhen writing the code for a table which you want to include in your final document, remember that the function  kable()  in the knitr package helps provide consistent table formatting across various different outputs. Also note that the same principles apply concerning whether or not your table output will appear as those referred to in plotting; no need to save the table first, you can simply write  mydata %&gt;% kable() .\nAnd finally, as addictive as it is creating beautiful plots with ggplot2, try not to bombard the reader with too many tables and figures. Choose wisely the information you would like to display in order to tell your story concisely and make good use of ggplot2’s ability to provide a breadth of information in one plot, through careful use of aesthetic mappings. \n\n\n\nIn this course we have covered a whole variety of functions which you will find useful when carrying out data wrangling, plotting and analysis in R. There are however some functions which you will find yourself using more frequently than others and in the following sections we provide a quick recap on some of these more common functions.\nThe examples below are intended as a quick reference guide to remind yourself which functions to use in which settings. They can be copied and pasted straight into an R Markdown document as code chunk to test, but if you are copying multiple code chunks, you only need to include the packages once.\nNote: The examples below all use the gapminder dataset which is loaded by running the command  library(gapminder) . In your own projects you would load your data with the command read_csv(“mydata.csv”)  or similar, depending on the format and location of your data.\n\n\ndistinct() returns only distinct (unique) rows in a data frame\nIn the example below we have included the variable  continent  so that we can explore how many unique categories there are in this column.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\nWarning: package 'gapminder' was built under R version 4.4.3\n\ngapminder %&gt;%\n  distinct(continent)\n\n# A tibble: 5 × 1\n  continent\n  &lt;fct&gt;    \n1 Asia     \n2 Europe   \n3 Africa   \n4 Americas \n5 Oceania  \n\n\nUseful for:\n\nexploring categories\nspotting spelling mistakes\nlooking at date ranges\nremoving duplicate rows\nTry this out: \n\ngapminder %&gt;%\ncount(contintent)\n\n\n\nfilter()  returns a subset of rows based on a condition\nIn the example below we have specified that we only want to see results where  country  is equal to  Ghana .\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  filter(country == \"Ghana\")\n\n# A tibble: 12 × 6\n   country continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Ghana   Africa     1952    43.1  5581001      911.\n 2 Ghana   Africa     1957    44.8  6391288     1044.\n 3 Ghana   Africa     1962    46.5  7355248     1190.\n 4 Ghana   Africa     1967    48.1  8490213     1126.\n 5 Ghana   Africa     1972    49.9  9354120     1178.\n 6 Ghana   Africa     1977    51.8 10538093      993.\n 7 Ghana   Africa     1982    53.7 11400338      876.\n 8 Ghana   Africa     1987    55.7 14168101      847.\n 9 Ghana   Africa     1992    57.5 16278738      925.\n10 Ghana   Africa     1997    58.6 18418288     1005.\n11 Ghana   Africa     2002    58.5 20550751     1112.\n12 Ghana   Africa     2007    60.0 22873338     1328.\n\n\nUseful for:\n\nfiltering your data to remove unwanted rows\nfiltering your data as part of the exploratory phase\nfiltering your data prior to piping into  ggplot() \n\nRemember:\n\nthe “equal to” comparison operator needs double  == \nto use inverted commas (““) for non-numbers \n\n\n\n\nselect() lets you choose which columns to select\nIn the example below we have chosen to select only 4 variables from our dataset and we have also changed the order in which they appear.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  select(country, year, pop, lifeExp)\n\n# A tibble: 1,704 × 4\n   country      year      pop lifeExp\n   &lt;fct&gt;       &lt;int&gt;    &lt;int&gt;   &lt;dbl&gt;\n 1 Afghanistan  1952  8425333    28.8\n 2 Afghanistan  1957  9240934    30.3\n 3 Afghanistan  1962 10267083    32.0\n 4 Afghanistan  1967 11537966    34.0\n 5 Afghanistan  1972 13079460    36.1\n 6 Afghanistan  1977 14880372    38.4\n 7 Afghanistan  1982 12881816    39.9\n 8 Afghanistan  1987 13867957    40.8\n 9 Afghanistan  1992 16317921    41.7\n10 Afghanistan  1997 22227415    41.8\n# ℹ 1,694 more rows\n\n\nUseful for:\n\nremoving unwanted columns (e.g. columns with NAs, or things you’re just not going to use)\nfocusing on selected columns for further exploration\nre-ordering columns for ease of viewing\nselect() can also be used to rename columns, e.g.\n\ngapminder %&gt;% \n  select(country, year, population = pop, life_expectancy = lifeExp)\n\nAnd select() has a sister function called rename(), that can be used to rename columns without removing the ones you don’t list, e.g.:\n\ngapminder %&gt;%\n  rename(population = pop, life_expectancy = lifeExp)\n\n\n\nmutate() allows you to add a column or update an existing column\nIn the example below we have created a new column called  pop_millions  so that we can display the population as a decimal number of millions.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  mutate(pop_millions = pop/1000000)\n\n# A tibble: 1,704 × 7\n   country     continent  year lifeExp      pop gdpPercap pop_millions\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.         8.43\n 2 Afghanistan Asia       1957    30.3  9240934      821.         9.24\n 3 Afghanistan Asia       1962    32.0 10267083      853.        10.3 \n 4 Afghanistan Asia       1967    34.0 11537966      836.        11.5 \n 5 Afghanistan Asia       1972    36.1 13079460      740.        13.1 \n 6 Afghanistan Asia       1977    38.4 14880372      786.        14.9 \n 7 Afghanistan Asia       1982    39.9 12881816      978.        12.9 \n 8 Afghanistan Asia       1987    40.8 13867957      852.        13.9 \n 9 Afghanistan Asia       1992    41.7 16317921      649.        16.3 \n10 Afghanistan Asia       1997    41.8 22227415      635.        22.2 \n# ℹ 1,694 more rows\n\n\nUseful for:\n\nadding an extra column containing a calculation\ncombining with group_by()  for grouped calculations\nupdating a column which has mistakes\nconverting a column to a different data type\n\nRemember:\n\nyour new column is always added at the end of your tibble (data frame)\nthe value to the left of the =  is the new name for you column and could be called anything\nif you want to replace a column, the value to the left of the =  must be exactly the same name as the column to replace\n\n\n\n\nsummarise() returns a new tibble (data frame) containing summary statistics based on what has been specified in the function.\nIn the example below the summarise function has calculated the average life expectancy for the whole gapminder dataset by averaging all observations (or rows). On its own,  summarise()  is not so useful, but when combined with group_by()  its use becomes apparent, see later examples.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  summarise(avg_life_exp = mean(lifeExp))\n\n# A tibble: 1 × 1\n  avg_life_exp\n         &lt;dbl&gt;\n1         59.5\n\n\nUseful for:\n\ncombining with group_by()  to perform grouped calculations\n\n\n\n\ngroup_by() lets you choose how you want the variables in your dataset to be grouped so that you can perform operations (such as sum() or mean()) on these groups.\nIn the example below we have grouped our data by the variable year. Notice that the output to this chunk of code looks no different to how it would look if we just ran  gapminderwithout any grouping. This is because the effects of group_by()  are only noticeable when we start performing operations.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  group_by(year)\n\n# A tibble: 1,704 × 6\n# Groups:   year [12]\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nIn this next example we have combined group_by() with the summarise() command and now, instead of seeing an average life expectancy for the whole tibble (as in the example above), we see that life expectancy has been averaged within each year group.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\ngroup_by(year) %&gt;%\n  summarise(avg_life_exp = mean(lifeExp))\n\n# A tibble: 12 × 2\n    year avg_life_exp\n   &lt;int&gt;        &lt;dbl&gt;\n 1  1952         49.1\n 2  1957         51.5\n 3  1962         53.6\n 4  1967         55.7\n 5  1972         57.6\n 6  1977         59.6\n 7  1982         61.5\n 8  1987         63.2\n 9  1992         64.2\n10  1997         65.0\n11  2002         65.7\n12  2007         67.0\n\n\nUseful for:\n\ncombining with summarise()or mutate() for grouped calculations\npreparing summary statistics for plotting\nexploring values within groups in your data\n\nRemember:\n\nto use  ungroup()  if carrying out further data manipulations to avoid unexpected results, as by default, mutate() and summarise() silently retain the groupings.\nthe value to the left of the  =  is the new name for your column and could be called anything\n\n\n\n\nleft_join()  allows you to combine two datasets\nIn the example below we are joining the gapminder tibble to another tibble we can access directly from the gapminder package, the  country_codes data frame, in order to add the ISO country codes.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  left_join(country_codes)\n\nJoining with `by = join_by(country)`\n\n\n# A tibble: 1,704 × 8\n   country     continent  year lifeExp      pop gdpPercap iso_alpha iso_num\n   &lt;chr&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;int&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779. AFG             4\n 2 Afghanistan Asia       1957    30.3  9240934      821. AFG             4\n 3 Afghanistan Asia       1962    32.0 10267083      853. AFG             4\n 4 Afghanistan Asia       1967    34.0 11537966      836. AFG             4\n 5 Afghanistan Asia       1972    36.1 13079460      740. AFG             4\n 6 Afghanistan Asia       1977    38.4 14880372      786. AFG             4\n 7 Afghanistan Asia       1982    39.9 12881816      978. AFG             4\n 8 Afghanistan Asia       1987    40.8 13867957      852. AFG             4\n 9 Afghanistan Asia       1992    41.7 16317921      649. AFG             4\n10 Afghanistan Asia       1997    41.8 22227415      635. AFG             4\n# ℹ 1,694 more rows\n\n\nUseful for:\n\ncombining two sets of data which share an id (e.g. patient id, appointment id)\nadding in information from a lookup table (e.g. country codes, Health Board names)\n\nRemember:\n\nto watch out what happens to missing values\nto keep an eye on the number of observations in each tibble before and after joining\nto check which columns are being joined by\n\nOther functions for wrangling data and what they might be useful for:\n\n arrange()  for sorting the values within a column\n desc() for indicating descending sort order when used within arrange()\n str_replace()  for updating spelling mistakes in a column\n paste()  for joining things together and adding new text \n ymd()  for reading in dates in strange formats\n factor()  for converting a variable to factor data type\n fct_relevel()  for changing the order of factors\n pivot_longer()  for reshaping your data if you have values in column headings\n pivot_wider()  for reshaping your data if you have multiple variables in one column",
    "crumbs": [
      "Content",
      "Week 4: Analysing and Presenting Data in R",
      "Topic 3: Roundup and recap"
    ]
  },
  {
    "objectID": "content/week-4/topic-3.html#reproducibility-in-r",
    "href": "content/week-4/topic-3.html#reproducibility-in-r",
    "title": "Roundup and recap",
    "section": "",
    "text": "At the beginning of this course we introduced R by highlighting one of its main attractions: reproducibility. And here we are at the end of the course still banging on about reproducibility, but that’s because it is so important.\nThere is a growing awareness of the importance of reproducibility in data processing and analysis, and thankfully the tools needed to generate reproducible outputs are more accessible than ever before, helping to make this easier to accomplish.\nThere are many ways in which you can ensure your work is reproducible. The first we would recommend, is to start using R, and the second is to become familiar with R Markdown, so you are well on your way!  \nHere is a reminder of some of the main points to bear in mind, to ensure your future self and others can easily and efficiently build on the analysis you have carried out in R:\n\nalways create an RStudio Project to work in\nuse raw data directly in R (no pre-editing with other software)\ninclude informative commenting\nensure code is easy to read with appropriate indentations and spacing\npipe  %&gt;%  with the Tidyverse\ngive your objects meaningful names\nwork in R Markdown for reporting\n\nLet’s look at these aspects in little more detail.\n\n\n\n\n\nBy now you should be familiar with the how and the why of creating Projects in RStudio as this has cropped up frequently throughout the course. Projects help to keep everything in one place, they help R to find all your files and folders easily, and they generally make your life a lot easier so you should definitely be using them. \n\n\n\n\n\nAlways use the raw data. If your data has come from a database or from a publicly updated data source, avoid the temptation of doing some quick and dirty data wrangling in Excel before importing into R. This would make your analysis less reproducible as you may not always remember exactly what steps you took during this initial phase and these steps would then need to be taken each time you wanted to repeat or carry out a similar operation. \n\n\n\n\n\nCode which is easy to decipher and understand can be revisited in the future and rerun more easily, but how many times have you looked back at scribbled notes you took in haste, or code you typed quickly, with no recollection of what it all means? By including informative commenting you can help your future self, or anyone else who might be looking at your code, understand what it is you were trying to do at each point in your script. It isn’t needed for all lines of code but should be used when there might be ambiguity, or to provide clarification, or remind yourself about something specific to do with what’s happening. The keyboard shortcut for commenting/uncommenting a line is Ctrl+Shift+C, or you can manually type  # .\n\n\n\n\n\nOn a similar theme, it is important to make your code easy to read by applying the appropriate indentations and spacing. The keyboard shortcut for quickly reformatting your code is Ctrl+I (first select your code, or use Ctrl+A to select all before doing Ctrl+I). If your script is long this is particularly important as it means you, or someone else, can scan the document more easily to find the relevant text or information. \n\n\n\n\n\nThe Tidyverse has featured heavily in this course and is another great way to make your code easier to read and understand due to the use of the  %&gt;%  operator. As we learned at the beginning of the course, the  %&gt;% reduces the need for nested functions, where each function is “nested” inside another making it potentially very difficult to debug. Instead the piped (  %&gt;% ) functions appear almost like a series of statements or instructions, which can be run line by line if needed. At the start of every R script or document, remember to include  library(tidyverse) .\n\n\n\n\n\nTo finish, just a final note about assigning your objects meaningful names. This is easily overlooked and can be surprisingly difficult but when given careful consideration, good object naming can make your life a lot easier, and again, make the code easier to understand and read. Avoid generic names such as data or plot as these can cause problems. This is particularly true if you are carrying out multiple analyses and have referred to “data” or “plot” somewhere else and not cleared your environment (Restart R often!). Also, when choosing names, no spaces or funky characters, stick to lowercase with underscores. Then be consistent, and you should be safe! \n\n\n\n\n\nR Markdown is one of the keys to producing reproducible analysis in R as it enables the code and the narrative to be easily combined, whilst providing great flexibility as to how the output might be displayed. It is certainly worth the effort becoming familiar with R Markdown which was covered in detail in Week 6: Topics 1 & 2. Creating a new R Markdown document is as simple as going to  File - New File - R Markdown - OK  then delete everything up to the first code chunk (it is useful to leave the first setup chunk) and you are ready to go!\n\n\nR Markdown is not only a powerful tool for ensuring reproducibility, but as you have already discovered, it includes many features which make it easy to create and share clear and professional looking documents for a wide range of audiences.\nHere are some things to remember when planning and designing a report with R Markdown:\n\nprovide a clear structure using Markdown section headers (#, ##, ###)\nbe conscious of the YAML, unlike R code, the YAML is very sensitive of extra spaces/formatting\nconsider the code output options depending on audience (`echo = TRUE` vs `echo = FALSE`)\nthink about the story you are trying to tell\nmake good use of figures\nuse tables appropriately\n\n\n\n\n\n\nIt is important to provide a clear structure in any report or document you are writing and R Markdown makes it easy to manage and display the structure through the use of headers. The hash symbol ( # ) is used in the Markdown sections (all the bits which aren’t code chunks or YAML) to indicate different levels of header, so how big or small the text should be. These headers also show up in the document outline which is extremely useful as a means of navigating your document when editing, helping to save you time.\n\n# global chunk options`\n\nknitr::opts_chunk$set(echo = TRUE)\n# load packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(here)\n\nhere() starts at C:/Users/ldababn/OneDrive - University of Edinburgh/job/GitHub/website-ug-data-science\n\n\nThe structure of your R code chunks is important too. For example, it is good practice to have a code chunk near the top of your document (often in the setup chunk) where you keep all the packages that you are using together. It might look something like the image on the right.  The next code chunk is often where you might choose to load your data and is likely to include the code similar to:  mydata &lt;- read_csv(here(“data”, “appointments.csv”))\n\n\n\n\n\nWe’ve already mentioned that Markdown headers ( # ) make it easy to define a clear structure in your R Markdown document. Explore some of the other Markdown features, such as **bold** and *italic* or `inline R code`, which can help to give your final document a high quality finish. Remember, you can access the Markdown Quick Reference guide from the Help menu.\nIn its most basic form, the YAML which appears at the top of your R Markdown document, could simply tell R Markdown what output format you would like your document to be in, in which case it would look like this, indicating HTML output:\n---\noutput: html_document\n---\n\n\n\n\n\nHowever, you have the potential to add many attractive features via the YAML, such as table of contents, section numbering, themes, and much more. You have to be particularly careful about indentation in the YAML (unlike in R code chunks where indentation is mainly for readability) but you can start exploring YAML options easily through changing settings in the document cog and this will automatically update your YAML.\nDepending on who your intended audience is, you have a number of output options available to you. You can choose whether or not to show or run your code in the final document. Showing you code in the Markdown output document (HTML, Word, PDF) is denoted `echo = TRUE`, not showing it is denoted `echo = FALSE`. You also have control over whether to display the output of each code chunk or not too. These options can be accessed from the individual code chunk cogs or can be set for the whole document in the setup chunk.\n\n\n\n\n\nWhen creating a report or document, think about what story you are trying to tell with the data; you might be answering a specific question or perhaps highlighting a change in activity over a period of time. R Markdown makes it easy to intersperse clear and informative visual elements throughout the narrative. These could be in the form of figures and plots, nicely formatted tables or even images (e.g. JPG or PNG).\nWhen writing the code to produce a plot, remember you don’t need to assign it to an object. For example, running  myplot &lt;- mydata %&gt;% ggplot()  will result in no plot appearing in your final document, it is saved in the Environment. Instead you can simply write  mydata %&gt;% ggplot() which will ensure your plot does appear. You can also control the size of your plots and figures in the code chunk settings cog.\nWhen writing the code for a table which you want to include in your final document, remember that the function  kable()  in the knitr package helps provide consistent table formatting across various different outputs. Also note that the same principles apply concerning whether or not your table output will appear as those referred to in plotting; no need to save the table first, you can simply write  mydata %&gt;% kable() .\nAnd finally, as addictive as it is creating beautiful plots with ggplot2, try not to bombard the reader with too many tables and figures. Choose wisely the information you would like to display in order to tell your story concisely and make good use of ggplot2’s ability to provide a breadth of information in one plot, through careful use of aesthetic mappings. \n\n\n\nIn this course we have covered a whole variety of functions which you will find useful when carrying out data wrangling, plotting and analysis in R. There are however some functions which you will find yourself using more frequently than others and in the following sections we provide a quick recap on some of these more common functions.\nThe examples below are intended as a quick reference guide to remind yourself which functions to use in which settings. They can be copied and pasted straight into an R Markdown document as code chunk to test, but if you are copying multiple code chunks, you only need to include the packages once.\nNote: The examples below all use the gapminder dataset which is loaded by running the command  library(gapminder) . In your own projects you would load your data with the command read_csv(“mydata.csv”)  or similar, depending on the format and location of your data.\n\n\ndistinct() returns only distinct (unique) rows in a data frame\nIn the example below we have included the variable  continent  so that we can explore how many unique categories there are in this column.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\nWarning: package 'gapminder' was built under R version 4.4.3\n\ngapminder %&gt;%\n  distinct(continent)\n\n# A tibble: 5 × 1\n  continent\n  &lt;fct&gt;    \n1 Asia     \n2 Europe   \n3 Africa   \n4 Americas \n5 Oceania  \n\n\nUseful for:\n\nexploring categories\nspotting spelling mistakes\nlooking at date ranges\nremoving duplicate rows\nTry this out: \n\ngapminder %&gt;%\ncount(contintent)\n\n\n\nfilter()  returns a subset of rows based on a condition\nIn the example below we have specified that we only want to see results where  country  is equal to  Ghana .\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  filter(country == \"Ghana\")\n\n# A tibble: 12 × 6\n   country continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Ghana   Africa     1952    43.1  5581001      911.\n 2 Ghana   Africa     1957    44.8  6391288     1044.\n 3 Ghana   Africa     1962    46.5  7355248     1190.\n 4 Ghana   Africa     1967    48.1  8490213     1126.\n 5 Ghana   Africa     1972    49.9  9354120     1178.\n 6 Ghana   Africa     1977    51.8 10538093      993.\n 7 Ghana   Africa     1982    53.7 11400338      876.\n 8 Ghana   Africa     1987    55.7 14168101      847.\n 9 Ghana   Africa     1992    57.5 16278738      925.\n10 Ghana   Africa     1997    58.6 18418288     1005.\n11 Ghana   Africa     2002    58.5 20550751     1112.\n12 Ghana   Africa     2007    60.0 22873338     1328.\n\n\nUseful for:\n\nfiltering your data to remove unwanted rows\nfiltering your data as part of the exploratory phase\nfiltering your data prior to piping into  ggplot() \n\nRemember:\n\nthe “equal to” comparison operator needs double  == \nto use inverted commas (““) for non-numbers \n\n\n\n\nselect() lets you choose which columns to select\nIn the example below we have chosen to select only 4 variables from our dataset and we have also changed the order in which they appear.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  select(country, year, pop, lifeExp)\n\n# A tibble: 1,704 × 4\n   country      year      pop lifeExp\n   &lt;fct&gt;       &lt;int&gt;    &lt;int&gt;   &lt;dbl&gt;\n 1 Afghanistan  1952  8425333    28.8\n 2 Afghanistan  1957  9240934    30.3\n 3 Afghanistan  1962 10267083    32.0\n 4 Afghanistan  1967 11537966    34.0\n 5 Afghanistan  1972 13079460    36.1\n 6 Afghanistan  1977 14880372    38.4\n 7 Afghanistan  1982 12881816    39.9\n 8 Afghanistan  1987 13867957    40.8\n 9 Afghanistan  1992 16317921    41.7\n10 Afghanistan  1997 22227415    41.8\n# ℹ 1,694 more rows\n\n\nUseful for:\n\nremoving unwanted columns (e.g. columns with NAs, or things you’re just not going to use)\nfocusing on selected columns for further exploration\nre-ordering columns for ease of viewing\nselect() can also be used to rename columns, e.g.\n\ngapminder %&gt;% \n  select(country, year, population = pop, life_expectancy = lifeExp)\n\nAnd select() has a sister function called rename(), that can be used to rename columns without removing the ones you don’t list, e.g.:\n\ngapminder %&gt;%\n  rename(population = pop, life_expectancy = lifeExp)\n\n\n\nmutate() allows you to add a column or update an existing column\nIn the example below we have created a new column called  pop_millions  so that we can display the population as a decimal number of millions.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  mutate(pop_millions = pop/1000000)\n\n# A tibble: 1,704 × 7\n   country     continent  year lifeExp      pop gdpPercap pop_millions\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.         8.43\n 2 Afghanistan Asia       1957    30.3  9240934      821.         9.24\n 3 Afghanistan Asia       1962    32.0 10267083      853.        10.3 \n 4 Afghanistan Asia       1967    34.0 11537966      836.        11.5 \n 5 Afghanistan Asia       1972    36.1 13079460      740.        13.1 \n 6 Afghanistan Asia       1977    38.4 14880372      786.        14.9 \n 7 Afghanistan Asia       1982    39.9 12881816      978.        12.9 \n 8 Afghanistan Asia       1987    40.8 13867957      852.        13.9 \n 9 Afghanistan Asia       1992    41.7 16317921      649.        16.3 \n10 Afghanistan Asia       1997    41.8 22227415      635.        22.2 \n# ℹ 1,694 more rows\n\n\nUseful for:\n\nadding an extra column containing a calculation\ncombining with group_by()  for grouped calculations\nupdating a column which has mistakes\nconverting a column to a different data type\n\nRemember:\n\nyour new column is always added at the end of your tibble (data frame)\nthe value to the left of the =  is the new name for you column and could be called anything\nif you want to replace a column, the value to the left of the =  must be exactly the same name as the column to replace\n\n\n\n\nsummarise() returns a new tibble (data frame) containing summary statistics based on what has been specified in the function.\nIn the example below the summarise function has calculated the average life expectancy for the whole gapminder dataset by averaging all observations (or rows). On its own,  summarise()  is not so useful, but when combined with group_by()  its use becomes apparent, see later examples.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  summarise(avg_life_exp = mean(lifeExp))\n\n# A tibble: 1 × 1\n  avg_life_exp\n         &lt;dbl&gt;\n1         59.5\n\n\nUseful for:\n\ncombining with group_by()  to perform grouped calculations\n\n\n\n\ngroup_by() lets you choose how you want the variables in your dataset to be grouped so that you can perform operations (such as sum() or mean()) on these groups.\nIn the example below we have grouped our data by the variable year. Notice that the output to this chunk of code looks no different to how it would look if we just ran  gapminderwithout any grouping. This is because the effects of group_by()  are only noticeable when we start performing operations.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  group_by(year)\n\n# A tibble: 1,704 × 6\n# Groups:   year [12]\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nIn this next example we have combined group_by() with the summarise() command and now, instead of seeing an average life expectancy for the whole tibble (as in the example above), we see that life expectancy has been averaged within each year group.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\ngroup_by(year) %&gt;%\n  summarise(avg_life_exp = mean(lifeExp))\n\n# A tibble: 12 × 2\n    year avg_life_exp\n   &lt;int&gt;        &lt;dbl&gt;\n 1  1952         49.1\n 2  1957         51.5\n 3  1962         53.6\n 4  1967         55.7\n 5  1972         57.6\n 6  1977         59.6\n 7  1982         61.5\n 8  1987         63.2\n 9  1992         64.2\n10  1997         65.0\n11  2002         65.7\n12  2007         67.0\n\n\nUseful for:\n\ncombining with summarise()or mutate() for grouped calculations\npreparing summary statistics for plotting\nexploring values within groups in your data\n\nRemember:\n\nto use  ungroup()  if carrying out further data manipulations to avoid unexpected results, as by default, mutate() and summarise() silently retain the groupings.\nthe value to the left of the  =  is the new name for your column and could be called anything\n\n\n\n\nleft_join()  allows you to combine two datasets\nIn the example below we are joining the gapminder tibble to another tibble we can access directly from the gapminder package, the  country_codes data frame, in order to add the ISO country codes.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  left_join(country_codes)\n\nJoining with `by = join_by(country)`\n\n\n# A tibble: 1,704 × 8\n   country     continent  year lifeExp      pop gdpPercap iso_alpha iso_num\n   &lt;chr&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;int&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779. AFG             4\n 2 Afghanistan Asia       1957    30.3  9240934      821. AFG             4\n 3 Afghanistan Asia       1962    32.0 10267083      853. AFG             4\n 4 Afghanistan Asia       1967    34.0 11537966      836. AFG             4\n 5 Afghanistan Asia       1972    36.1 13079460      740. AFG             4\n 6 Afghanistan Asia       1977    38.4 14880372      786. AFG             4\n 7 Afghanistan Asia       1982    39.9 12881816      978. AFG             4\n 8 Afghanistan Asia       1987    40.8 13867957      852. AFG             4\n 9 Afghanistan Asia       1992    41.7 16317921      649. AFG             4\n10 Afghanistan Asia       1997    41.8 22227415      635. AFG             4\n# ℹ 1,694 more rows\n\n\nUseful for:\n\ncombining two sets of data which share an id (e.g. patient id, appointment id)\nadding in information from a lookup table (e.g. country codes, Health Board names)\n\nRemember:\n\nto watch out what happens to missing values\nto keep an eye on the number of observations in each tibble before and after joining\nto check which columns are being joined by\n\nOther functions for wrangling data and what they might be useful for:\n\n arrange()  for sorting the values within a column\n desc() for indicating descending sort order when used within arrange()\n str_replace()  for updating spelling mistakes in a column\n paste()  for joining things together and adding new text \n ymd()  for reading in dates in strange formats\n factor()  for converting a variable to factor data type\n fct_relevel()  for changing the order of factors\n pivot_longer()  for reshaping your data if you have values in column headings\n pivot_wider()  for reshaping your data if you have multiple variables in one column",
    "crumbs": [
      "Content",
      "Week 4: Analysing and Presenting Data in R",
      "Topic 3: Roundup and recap"
    ]
  },
  {
    "objectID": "content/week-4/topic-3.html#plotting-and-tables-recap",
    "href": "content/week-4/topic-3.html#plotting-and-tables-recap",
    "title": "Roundup and recap",
    "section": "Plotting and tables recap",
    "text": "Plotting and tables recap\nIn the examples below we provide a recap on how to use ggplot2 to build up layers on a plot. Included are examples of argument options which you might want to experiment with in your own plots. I rarely remember any of the argument specifics and always fine-tune my plots by pressing `F1` on the various functions I’m using and exploring the documentation (F1, or fn+F1 on some computers opens up the Help tab). Then it’s a process of trial and error.  \nComments are included before each new line or layer to describe what is happening.\n\nThe blank canvas\n\nlibrary(tidyverse)\nlibrary(gapminder)\n     \n    # Data we are sending to ggplot()\ngapminder %&gt;%\n    # lets create a plot\n ggplot(aes(    # specify the axes\n      x = year,\n      y = lifeExp))\n\n\n\n\n\n\n\n\nRemember:\n\nthat everything from now on needs  + at end of line instead of  %&gt;% \n\n\n\nPlot type\n\nlibrary(tidyverse)\nlibrary(gapminder)\n     \ngapminder %&gt;%\n  ggplot(aes(x = year, y = lifeExp)) +\n  # specify type of plot: scatter with jitter\n  geom_jitter(\n    # colour life expectancy based on values\n    aes(colour = lifeExp),\n    # set \"alpha\" making points same transparency\n    alpha = 0.6,\n    # specify width of \"jitter\"\n    width = 0.5)\n\n\n\n\n\n\n\n\nRemember:\n\ncolour is specified inside the  aes() function because we want it to be dependent on the values in the data\n alpha and  width are specified outside the  aes() function because we want to specify a fixed value for these arguments\n\n\n\nFaceting\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  ggplot(aes(x = year, y = lifeExp)) +\n  geom_jitter(aes(colour = lifeExp),\n              alpha = 0.6,\n              width = 0.5) +\n  # create muti-panle plot\n  facet_wrap(\n    # based on continent grouping\n    ~ continent)\n\n\n\n\n\n\n\n\nUseful for:\n\ncomparing values between different categories in your dataset\n\n\n\nIncluding a second geom, e.g., geom_boxplot()\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  ggplot(aes(x = year, y = lifeExp)) +\n  geom_jitter(aes(colour = lifeExp),\n              alpha = 0.6,\n              width = 0.5) +\n  facet_wrap(~ continent) +\n  # specify type of plot: boxplot\n  geom_boxplot(\n    # group by year (discrete) as currently \"int\" data type (continuous)\n    aes(group = year),\n    # set \"alpha\" so we can see jitter points underneath\n    alpha = 0.3,\n    # remove boxplot outliers as already present in \"jitter\" layer\n    outlier.shape = NA)\n\n\n\n\n\n\n\n\nRemember:\n\nDepending on whether your variable is stored as a “number” or a “character” or a “factor” will determine how ggplot2 deals with it\n\n\n\nStandard themes\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  ggplot(aes(x = year, y = lifeExp)) +\n  geom_jitter(aes(colour = lifeExp),\n              alpha = 0.6,\n              width = 0.5) +\n  facet_wrap(~ continent) +\n  geom_boxplot(aes(group = year),\n               alpha = 0.3,\n               outlier.shape = NA) +\n  # choose a standard theme: dark\n  theme_dark()\n\n\n\n\n\n\n\n\nUseful for:\n\nquickly changing how your plot looks by applying one of the built in themes:  theme_bw() ,  theme_classic() ,  theme_dark() etc.",
    "crumbs": [
      "Content",
      "Week 4: Analysing and Presenting Data in R",
      "Topic 3: Roundup and recap"
    ]
  },
  {
    "objectID": "content/week-4/topic-3.html#making-tables",
    "href": "content/week-4/topic-3.html#making-tables",
    "title": "Roundup and recap",
    "section": "Making Tables",
    "text": "Making Tables\nTables are another important tool to have in your data analysis toolbox for communcating. As a general rule, when deciding between tables for graphs/plots:\n\nuse tables when the display will be used to look up individual values or you want to compare individual values \nuse graphs when the display will be used to reveal relatonships among whole sets of values or between variables \n\nTo make tables, we worked with the kable function and kableExtra package in R. \nThe general gt workflow includes 3 key steps: \n\nwrangle your data for the table by either piping %&gt;% the data or creating a new data object (tabledata &lt;- data %&gt;% mutate(…)) \npass the wrangled data to the kable function %&gt;% kable()\nformat the table object for presentation \n\n\nStep 1: Wrangle data for the table\nLets say we are interested in average life expectancy in Africa, Asia, and Oceania in the years 1952, 1972, 1992, and 2002 \n\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(gapminder)\n\ntabledata &lt;- gapminder %&gt;%\ngroup_by(year, continent) %&gt;%\nsummarise(avg_life_exp = mean(lifeExp)) %&gt;% \nfilter(continent %in% c(\"Africa\", \"Asia\", \"Oceania\"),\nyear %in% c(1952, 1972, 1992, 2002))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n\n\nStep 2: Create the kable table object\n\ntabledata %&gt;% kable()\n\n\n\n\nyear\ncontinent\navg_life_exp\n\n\n\n\n1952\nAfrica\n39.13550\n\n\n1952\nAsia\n46.31439\n\n\n1952\nOceania\n69.25500\n\n\n1972\nAfrica\n47.45094\n\n\n1972\nAsia\n57.31927\n\n\n1972\nOceania\n71.91000\n\n\n1992\nAfrica\n53.62958\n\n\n1992\nAsia\n66.53721\n\n\n1992\nOceania\n76.94500\n\n\n2002\nAfrica\n53.32523\n\n\n2002\nAsia\n69.23388\n\n\n2002\nOceania\n79.74000\n\n\n\n\n\n\n\nStep 3: Format the table for presentation\nHow you decide to format and customise your table visually is a stylistic and personal choice. However, there are a few key principles to keep in mind: \n\nyou should always change the name of your variables to be meaingful to stakeholders who are not familiar with your data - that is, do not keep column names the same as they are called in your dataset \ngenerally, it is a good idea to have a title on each table \nfor reproducibility, it is a good idea to include the data source if you are using an openly available dataset \nbe consistent: use capital or lower case letters in a consistent way\nformat numbers in an interpretable way (generally speaking, you do not need to include more than 2 or 3 decimal places)\n\nBelow is one way you could format the table for presentation: \n\ntabledata %&gt;%\n  kable(col.names = c(\"Year\", \"Continent\", \"Average Life Expectancy\"),\n        align = \"clc\",\n        digits = 2,\n        caption = \"Life Expectancy in Africa, Asia, and Oceania: Data from 1952, 1972, 1992, and 2002\") %&gt;%\n  kable_styling(\"striped\", full_width = FALSE) %&gt;%\n  footnote(general = \"Data from the gapminder dataset\")\n\n\nLife Expectancy in Africa, Asia, and Oceania: Data from 1952, 1972, 1992, and 2002\n\n\nYear\nContinent\nAverage Life Expectancy\n\n\n\n\n1952\nAfrica\n39.14\n\n\n1952\nAsia\n46.31\n\n\n1952\nOceania\n69.25\n\n\n1972\nAfrica\n47.45\n\n\n1972\nAsia\n57.32\n\n\n1972\nOceania\n71.91\n\n\n1992\nAfrica\n53.63\n\n\n1992\nAsia\n66.54\n\n\n1992\nOceania\n76.94\n\n\n2002\nAfrica\n53.33\n\n\n2002\nAsia\n69.23\n\n\n2002\nOceania\n79.74\n\n\n\nNote: \n\n\n\n\n Data from the gapminder dataset",
    "crumbs": [
      "Content",
      "Week 4: Analysing and Presenting Data in R",
      "Topic 3: Roundup and recap"
    ]
  },
  {
    "objectID": "content/week-4/topic-3.html#combined-example",
    "href": "content/week-4/topic-3.html#combined-example",
    "title": "Roundup and recap",
    "section": "Combined Example",
    "text": "Combined Example\nIn the example in this Link you can see a simple R Markdown document without any narrative text added yet, just the YAML then a series of code chunks outlining the various stages in the data analysis process.\nWhich when knitted to a pdf document looks like this:\nPDF!\nThe next step would be to add in narrative text, creating a comprehensive analysis.",
    "crumbs": [
      "Content",
      "Week 4: Analysing and Presenting Data in R",
      "Topic 3: Roundup and recap"
    ]
  },
  {
    "objectID": "content/week-4/topic-3.html#optional-data-engagement-data-controversies",
    "href": "content/week-4/topic-3.html#optional-data-engagement-data-controversies",
    "title": "Roundup and recap",
    "section": "Optional: Data Engagement & Data Controversies",
    "text": "Optional: Data Engagement & Data Controversies\nCritically review the arguments posed in this article:\nMittelstadt B (2019) Principles alone cannot guarantee ethical AI. Nat Mach Intell 1, 501–507.",
    "crumbs": [
      "Content",
      "Week 4: Analysing and Presenting Data in R",
      "Topic 3: Roundup and recap"
    ]
  },
  {
    "objectID": "content/week-5/index.html",
    "href": "content/week-5/index.html",
    "title": "Overview",
    "section": "",
    "text": "This week you will learn about geospatial analysis and creating more exciting tables.\nThere is also optional material on writing functions",
    "crumbs": [
      "Content",
      "Week 5: Making Maps and Tables in R",
      "Overview"
    ]
  },
  {
    "objectID": "content/week-5/index.html#learning-outcomes",
    "href": "content/week-5/index.html#learning-outcomes",
    "title": "Overview",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of the week you will be able to:\n\nKnow how to analyse geospatial data in R.\nUse the sf package and ggplot to plot data in a map.\nUse the gt package to creating more exciting tables.\n\n\nStructure of this week’s materials\n\nTopic 1: Introduction to Geospatial Data in R with sf and ggplot: 2 readings including practicals and code samples.\nTopic 2: Creating more exciting tables: 7 readings with code samples. 1 optional video.\nOPTIONAL: Writing and applying functions and a functions quiz.",
    "crumbs": [
      "Content",
      "Week 5: Making Maps and Tables in R",
      "Overview"
    ]
  },
  {
    "objectID": "content/week-5/topic-1.html",
    "href": "content/week-5/topic-1.html",
    "title": "Introduction to Geospatial Data in R with sf and Tidyverse",
    "section": "",
    "text": "Geospatial data is data about locations and features on Earth but we often refer to geospatial data simply as “spatial data”. These data can often have latitude and longitude attached to them. An example of spatial data would be using a bus app, and you can see the location of the bus.\nAnalysing this data is referred to as spatial analysis.\nFor example, you could have a map with data layered on top:\n\n\n\ncovid_vaccine_country_Africa\n\n\nMap of share of people who received at least one dose of COVID-19 vaccine by country in Africa, 2021.svg” by Our World In Data is licensed under CC BY 4.0\n\n\n\n\n\n“Georgia Population Density by Census Tract” by Wikimedia, used under CC BY-SA 4.0\n\n\n\n\n\nOr in this example - Mountain trails in Dovrefjell National Park, Norway created by our colleague John Wilson.\n\n\n\nSpatial analysis can be used in many disciplines to aid decision makers in their decision making process. Urban planners might use it as might logistics practitioners.\nIn the health context, vaccination strategies can be informed or the tracking of infectious diseases can be greatly improved.\nOne famous example is John Snow’s investigation into a cholera outbreak in London in 1854. Not convinced that the disease was transmitted by air but rather through the water system, he plotted points on a map to show that cholera deaths were clustered around water outlets.\nRead more on BBC Bitesize\n\n\n\nJohn Snow’s Cholera Map 1854 (public domain)",
    "crumbs": [
      "Content",
      "Week 5: Making Maps and Tables in R",
      "Topic 1: Introduction to Geospatial Data in R with sf and Tidyverse"
    ]
  },
  {
    "objectID": "content/week-5/topic-1.html#spatial-analysis",
    "href": "content/week-5/topic-1.html#spatial-analysis",
    "title": "Introduction to Geospatial Data in R with sf and Tidyverse",
    "section": "",
    "text": "Geospatial data is data about locations and features on Earth but we often refer to geospatial data simply as “spatial data”. These data can often have latitude and longitude attached to them. An example of spatial data would be using a bus app, and you can see the location of the bus.\nAnalysing this data is referred to as spatial analysis.\nFor example, you could have a map with data layered on top:\n\n\n\ncovid_vaccine_country_Africa\n\n\nMap of share of people who received at least one dose of COVID-19 vaccine by country in Africa, 2021.svg” by Our World In Data is licensed under CC BY 4.0\n\n\n\n\n\n“Georgia Population Density by Census Tract” by Wikimedia, used under CC BY-SA 4.0\n\n\n\n\n\nOr in this example - Mountain trails in Dovrefjell National Park, Norway created by our colleague John Wilson.",
    "crumbs": [
      "Content",
      "Week 5: Making Maps and Tables in R",
      "Topic 1: Introduction to Geospatial Data in R with sf and Tidyverse"
    ]
  },
  {
    "objectID": "content/week-5/topic-1.html#why-is-it-important",
    "href": "content/week-5/topic-1.html#why-is-it-important",
    "title": "Introduction to Geospatial Data in R with sf and Tidyverse",
    "section": "",
    "text": "Spatial analysis can be used in many disciplines to aid decision makers in their decision making process. Urban planners might use it as might logistics practitioners.\nIn the health context, vaccination strategies can be informed or the tracking of infectious diseases can be greatly improved.\nOne famous example is John Snow’s investigation into a cholera outbreak in London in 1854. Not convinced that the disease was transmitted by air but rather through the water system, he plotted points on a map to show that cholera deaths were clustered around water outlets.\nRead more on BBC Bitesize\n\n\n\nJohn Snow’s Cholera Map 1854 (public domain)",
    "crumbs": [
      "Content",
      "Week 5: Making Maps and Tables in R",
      "Topic 1: Introduction to Geospatial Data in R with sf and Tidyverse"
    ]
  },
  {
    "objectID": "content/week-5/topic-1.html#wait-whats-vector-data",
    "href": "content/week-5/topic-1.html#wait-whats-vector-data",
    "title": "Introduction to Geospatial Data in R with sf and Tidyverse",
    "section": "Wait, what’s vector data?",
    "text": "Wait, what’s vector data?\nVector data is data that represents features in the world as either points, lines or polygons.\n\nPoints: A single pair of coordinates. For example the x,y position of a tower location.\nLines: Two or more connected points. For example, the start and end of a pathway.\nPolygons: Three or more points that are connected and closed. For example, the outline of a loch.\n\n\n\n\n\n\nNational Ecological Observatory Network.",
    "crumbs": [
      "Content",
      "Week 5: Making Maps and Tables in R",
      "Topic 1: Introduction to Geospatial Data in R with sf and Tidyverse"
    ]
  },
  {
    "objectID": "content/week-5/topic-1.html#shapefiles",
    "href": "content/week-5/topic-1.html#shapefiles",
    "title": "Introduction to Geospatial Data in R with sf and Tidyverse",
    "section": "Shapefiles",
    "text": "Shapefiles\nThe most common file format for vector data is a Shapefile which has the extension .shp. It stores the points, lines or polygons of the dataset. .shp files can store only one type: points, lines or polygons and comes with metadata which indicates which type it stores. Working with Shapefiles in R can be a bit more challenging than working with normal data frames and tibbles, but they allow you to have rich datasets with geographic content.\n\nFurther resources:\nFor more about vector data see Introduction to Vector Data (Data Carpentry)",
    "crumbs": [
      "Content",
      "Week 5: Making Maps and Tables in R",
      "Topic 1: Introduction to Geospatial Data in R with sf and Tidyverse"
    ]
  },
  {
    "objectID": "content/week-5/topic-1.html#simple-spatial-features-sf",
    "href": "content/week-5/topic-1.html#simple-spatial-features-sf",
    "title": "Introduction to Geospatial Data in R with sf and Tidyverse",
    "section": "Simple spatial features (sf)",
    "text": "Simple spatial features (sf)\nsf stands for Simple Features, a standardized format for representing spatial vector data. It follows a formal standard that defines how spatial geometries—such as points, lines, and polygons—are stored and accessed.\nBut what exactly is a feature? You can think of a feature as an object. In the real world, objects can be anything: a building, a tree, or a satellite. While we refer to them as single ‘objects’, they actually consist of various components. For instance, a tree is made up of a trunk, branches, and leaves. A feature follows the same concept: it may represent a single entity but can consist of multiple parts, along with attributes that describe it.\n“Simple Features (officially Simple Feature Access) is a set of standards that specify a common storage and access model of geographic features made of mostly two-dimensional geometries (point, line, polygon, multi-point, multi-line, etc.) used by geographic databases and geographic information systems. It is formalized by both the Open Geospatial Consortium (OGC) and the International Organization for Standardization (ISO).” - Wikipedia\nThe following seven simple feature types are the most common:\n\n\n\n\n\nIn the sf package, spatial objects are stored as a simple data frame with a special column that holds the geometry coordinates. This special column is a list, with each element corresponding to a row in the data frame. The length of each list element varies, depending on how many coordinates are needed to represent each individual feature. To work with sf objects in R, we use the sf package.\n\nNew Zealand Census Example\nLet’s look at an example. Download and unzip the file below into a “data” folder and create an RProject and a new RMarkdown file (new_zealand_census.Rmd). This data is aggregated census data from New Zealand from 2013.\nOr you can download the completed example complete-new-zealand-census2013.zip\nYou’ll notice there are 4 files. This is the format of Esri Shapefiles. (Esri is a geographic science and geospatial analytics company).\n\n.shp: The main file that contains the feature geometry\n.shx: The index file that stores the index of the feature geometry\n.dbf: The dBASE table that stores the attribute information of features\n.prj: A text file that contains information about a coordinate system and map projection\n\nThe .shp file is our spatial data and is already an sf object (the other files are part of the specification and are used to make reading the data more efficient).This means we can read the file in using the st_read() function to read it into our code. All of the functions in the sf package that operate on spatial data start with “st_”. This stands for spatial and temporal.\nLet’s load the data and have a look.\n\n# You may need to install.packages(\"sf\")\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(here) \nlibrary(janitor) # This is for cleaning the variable names\n\n\n# Read in data about New Zealand using the st_read function\nnz_census2013 &lt;- st_read(here(\"data\" ,\"nz_census2013.shp\"))\n \n# We'll use clean_names from janitor package to quickly clean up messy column names.\n# It changes them all to lowercase and adds \"_\" for spaces\nnz_census2013 &lt;- nz_census2013 %&gt;% \n  clean_names()\n\nClick on the nz_census2013 object. Here we can see:\n\nsimple features object (sf): a single observation.\nsimple feature geometry list-column (sfc) : this is the geometry column.\nsimple feature geometry (sfg) : this is the values within the geometry column. We can see the type is MULTIPOLYGON.\n\n\n\n\n\n\nWhen you inspect the data, you’ll see that the geometries are placed in a list-column. Each element of this list holds the simple feature geometry for a particular feature, as geometries are not single-valued.\n\n# check class and look at the data\nclass(nz_census2013)\n\nWhen we check the class of the nz_census2013 object we can see that it is both a data.frame and an sf object.\nWe can access the geometry aspect of your spatial data by calling the st_geometry function:\n\nnz_geometry &lt;- st_geometry(nz_census2013)\nnz_geometry\n\nWe can see that the geometry column is a list of polygons made up of points.",
    "crumbs": [
      "Content",
      "Week 5: Making Maps and Tables in R",
      "Topic 1: Introduction to Geospatial Data in R with sf and Tidyverse"
    ]
  },
  {
    "objectID": "content/week-5/topic-1.html#plotting-the-data",
    "href": "content/week-5/topic-1.html#plotting-the-data",
    "title": "Introduction to Geospatial Data in R with sf and Tidyverse",
    "section": "Plotting the data",
    "text": "Plotting the data\nFor visualisation, the sf package extends the base plot command, so it can be used on sf objects. If used without any arguments it will plot all the attributes.\n\n# If you want to plot all variables, you can do the following:\nplot(nz_census2013)\n\n\n\n\n\n\n\n# Or we could plot only the area variable of the sf object.\n# Because it's an sf object we can't use the $ operator to get the results we want (e.g. nz_census2013$area). \n# Instead, we use the square bracket notation.\nplot(nz_census2013[\"area\"])\n\n\n\n\n\n\nThis is great but often we’ll want to use ggplot because it gives us more control over our plots.\n\nPlotting spatial data with ggplot\nLuckily, there is a function built into ggplot called geom_sf. Let’s try it.\n\n# plot the data using ggplot \nggplot(data = nz_census2013, aes(fill = pop)) + \n  geom_sf() +\n  scale_fill_continuous(labels = scales::label_comma())\n\n\n\n\n\n\nMuch better! Here are a few more examples:\n\nggplot(data = nz_census2013, aes(fill = income)) + \n  # colour and linewidth refer to the outline of the map\n  geom_sf(colour = \"white\", linewidth = 0.1) +\n  scale_fill_viridis_c(name = \"Income in $\") +\n  theme_void() +\n  labs(title = \"Average Income by Region 2013\")\n\n\n\n\n\n\nLet’s say we wanted to display a map illustrating the average region percentage of each island who are Maori:\n\n# Group by island and summarise to get mean of maori per region\nnz_census_summary_by_island &lt;- nz_census2013 %&gt;% \n  group_by(island) %&gt;% \n  summarise(average_percent_maori = mean(maori))\n \n \n# Extract the North Island's average percentage Maori for the subtitle\n# We use pull() to get just the values from a specific column, rather than returning a tibble\nnorth_island_percent &lt;- nz_census_summary_by_island %&gt;%\n  filter(island == \"North\") %&gt;%\n  pull(average_percent_maori)\n \n \n# Extract the South Island's average percentage Maori for the subtitle\nsouth_island_percent &lt;- nz_census_summary_by_island %&gt;%\n  filter(island == \"South\") %&gt;%\n  pull(average_percent_maori)\n \n \nnz_census_summary_by_island  %&gt;% \n  ggplot(aes(fill = average_percent_maori)) +\n  geom_sf(colour = \"black\", linewidth = 0.5) +\n  scale_fill_viridis_c(\n    name = \"Percent\",\n    labels = function(x) x * 100\n ) +\n  theme_void() +\n  labs(\n    title =\"Average Region Percentage of Population Maori 2013\",\n    subtitle = paste0(\"North Island: \", round(north_island_percent * 100, 1), \", South Island: \", round(south_island_percent * 100, 1))\n  ) \n\n\n\n\n\n\nThinking point: Why there are only two colours on the map?\n\n\nKey Takeaways\n\nSpatial data is represented by “vector” data, including points, lines, and polygons.\nPoints: Single coordinates (e.g., a tower location).\nLines: Connected points (e.g., pathways).\nPolygons: Closed shapes formed by points (e.g., boundaries of a loch).\nShapefiles are a common file format for vector data, storing geographical features (points, lines, polygons).\nShapefiles include multiple files (.shp, .shx, .dbf, .prj) that contain feature geometry and attribute information.\nThe sf package in R is used to work with spatial vector data.\nSpatial data is stored in a data frame with a special geometry column, and functions for spatial operations start with “st_”.\nThe st_read() function is used to read shapefiles into R as sf objects.\nVisualization: Plot sf data using ggplot2 with geom_sf() for better customization.",
    "crumbs": [
      "Content",
      "Week 5: Making Maps and Tables in R",
      "Topic 1: Introduction to Geospatial Data in R with sf and Tidyverse"
    ]
  },
  {
    "objectID": "content/week-6/index.html",
    "href": "content/week-6/index.html",
    "title": "Overview",
    "section": "",
    "text": "This week you will learn about machine learning, and you will also be introduced to key methods for analysing clinical text.",
    "crumbs": [
      "Content",
      "Week 6: Machine Learning & Analysing Clinical Text",
      "Overview"
    ]
  },
  {
    "objectID": "content/week-6/index.html#learning-outcomes",
    "href": "content/week-6/index.html#learning-outcomes",
    "title": "Overview",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of the week you will be able to:\n\nExplain what machine learning is and how it relates to artificial intelligence, and argue about threats and opportunities brought to the healthcare sector\nDescribe and compare supervised to unsupervised learning methods\nDiscuss key tasks in natural language processing\nDescribe computational methods used for analysing clinical text",
    "crumbs": [
      "Content",
      "Week 6: Machine Learning & Analysing Clinical Text",
      "Overview"
    ]
  },
  {
    "objectID": "content/week-6/topic-2.html",
    "href": "content/week-6/topic-2.html",
    "title": "Analysing Clinical Text",
    "section": "",
    "text": "There is a large volume of health data captured in natural language (e.g. in clinical notes and referral letters), but its computer-based analysis is not that straightforward. In this topic you will be introduced to natural language processing, which is a subfield of linguistics, computer science, information engineering, and artificial intelligence, concerned with how to program computers to process and analyse large amounts of natural language data. You will learn about key tasks in natural language processing and computational methods used. \nNatural Language Processing: Key Tasks and Computational Methods\nWatch the following 3 videos to learn about examples of natural language data in the health sector, and to find out more about the area of natural language processing, and the computational methods used for key tasks.\nhttps://media.ed.ac.uk/media/1_ontbog3b\nLink to transcript\nhttps://media.ed.ac.uk/media/1_g6xre2mi\nLink to transcript\nhttps://media.ed.ac.uk/media/1_fpyoqqki\nLink to transcript",
    "crumbs": [
      "Content",
      "Week 6: Machine Learning & Analysing Clinical Text",
      "Topic 2: Analysing Clinical Text"
    ]
  },
  {
    "objectID": "content/week-6/topic-2.html#overview",
    "href": "content/week-6/topic-2.html#overview",
    "title": "Analysing Clinical Text",
    "section": "",
    "text": "There is a large volume of health data captured in natural language (e.g. in clinical notes and referral letters), but its computer-based analysis is not that straightforward. In this topic you will be introduced to natural language processing, which is a subfield of linguistics, computer science, information engineering, and artificial intelligence, concerned with how to program computers to process and analyse large amounts of natural language data. You will learn about key tasks in natural language processing and computational methods used. \nNatural Language Processing: Key Tasks and Computational Methods\nWatch the following 3 videos to learn about examples of natural language data in the health sector, and to find out more about the area of natural language processing, and the computational methods used for key tasks.\nhttps://media.ed.ac.uk/media/1_ontbog3b\nLink to transcript\nhttps://media.ed.ac.uk/media/1_g6xre2mi\nLink to transcript\nhttps://media.ed.ac.uk/media/1_fpyoqqki\nLink to transcript",
    "crumbs": [
      "Content",
      "Week 6: Machine Learning & Analysing Clinical Text",
      "Topic 2: Analysing Clinical Text"
    ]
  },
  {
    "objectID": "content/week-7/topic-2.html",
    "href": "content/week-7/topic-2.html",
    "title": "Integrating Health Data",
    "section": "",
    "text": "In order to realise the full potential of data science in health, it is imperative that we integrate data across different areas, organisations and countries. But integrating health data has been a key challenge for many years. In this topic you will be introduced to the graph data model and ontologies, which effectively address this challenge.",
    "crumbs": [
      "Content",
      "Week 7: Improving Healthcare Processes & Integrating Health Data",
      "Topic 2: Integrating Health Data"
    ]
  },
  {
    "objectID": "content/week-7/topic-2.html#overview",
    "href": "content/week-7/topic-2.html#overview",
    "title": "Integrating Health Data",
    "section": "",
    "text": "In order to realise the full potential of data science in health, it is imperative that we integrate data across different areas, organisations and countries. But integrating health data has been a key challenge for many years. In this topic you will be introduced to the graph data model and ontologies, which effectively address this challenge.",
    "crumbs": [
      "Content",
      "Week 7: Improving Healthcare Processes & Integrating Health Data",
      "Topic 2: Integrating Health Data"
    ]
  },
  {
    "objectID": "content/week-7/topic-2.html#addressing-challenges-to-data-integration-through-ontologies-and-the-graph-data-model",
    "href": "content/week-7/topic-2.html#addressing-challenges-to-data-integration-through-ontologies-and-the-graph-data-model",
    "title": "Integrating Health Data",
    "section": "Addressing Challenges to Data Integration through Ontologies and the Graph Data Model",
    "text": "Addressing Challenges to Data Integration through Ontologies and the Graph Data Model\nWatch the following 3 videos to find out about the main challenges to integrating health data, and to learn how ontologies and the graph data model can effectively address these challenges.\nhttps://media.ed.ac.uk/media/1_tf4ftpp5\nLink to the transcript\nhttps://media.ed.ac.uk/media/1_ut4sgw74\nLink to the transcript\nhttps://media.ed.ac.uk/media/1_ckezvjlp\nLink to the transcript",
    "crumbs": [
      "Content",
      "Week 7: Improving Healthcare Processes & Integrating Health Data",
      "Topic 2: Integrating Health Data"
    ]
  },
  {
    "objectID": "content/week-7/topic-2.html#optional-reading-health-related-linked-open-data",
    "href": "content/week-7/topic-2.html#optional-reading-health-related-linked-open-data",
    "title": "Integrating Health Data",
    "section": "Optional Reading: Health-Related Linked Open Data",
    "text": "Optional Reading: Health-Related Linked Open Data\nOver the past 15 years, there has been a worldwide effort to capture data following the RDF model. Back in the early days, people used to refer to such data as “semantic web data”, while lately the term “linked data” is more common.\nTim Berners-Lee, the inventor of the World Wide Web, has been the driving force behind this worldwide effort towards linked open data. There has been some progress across several countries, albeit somewhat slow. The following image, commonly referred to as the Linked Open Data Cloud, shows connections between existing linked open datasets. By clicking on it, you can explore data across a range of topics, including the life sciences.\n\nIn the UK, and in Scotland in particular, there has been an effort to make data available in a range of representation formalisms, including RDF. The Scottish Government Statistics website, https://statistics.gov.scot/ for example, includes a large number of datasets in health and social care that are available in N-Triples, an RDF serialisation format that is very similar to Turtle.",
    "crumbs": [
      "Content",
      "Week 7: Improving Healthcare Processes & Integrating Health Data",
      "Topic 2: Integrating Health Data"
    ]
  },
  {
    "objectID": "content/week-7/topic-2.html#sparql-querying",
    "href": "content/week-7/topic-2.html#sparql-querying",
    "title": "Integrating Health Data",
    "section": "SPARQL Querying",
    "text": "SPARQL Querying\nWe can query RDF data to get information of interest. The language for querying RDF data is called SPARQL, which stands for SPARQL Protocol And RDF Query Language.\n\n1. SPARQL Query Form\nThe basic form of SPARQL queries is as follows:\nSELECT ...\nWHERE {...}\nIn the SELECT-part we specify what results we want to get, in other words the variables that we are interested in. In the WHERE-part we specify the conditions for the data of interest. This is in the form of triple or graph patterns.\n\n\n2. Triple patterns\nTriple patterns help us select triples from a given RDF graph. Just like RDF is built on the concept of a triple, consisting of subject, predicate, and object, SPARQL is built upon the concept of triple pattern, which is also written as subject, predicate, and object (remember – in this order!), and has to be terminated with a full stop. The difference between RDF triples and SPARQL triple patterns is that a SPARQL triple pattern can include variables: any or all of the subject, predicate, and object values in a triple pattern can be a variable.\nLet’s look at an example. Suppose that we have the following triple pattern:\n\n\n\n\n\nIn this triple pattern, &lt;http://dbpedia.org/resource/Royal_Infirmary_of_Edinburgh&gt; is the subject, &lt;http://xmlns.com/foaf/0.1/name&gt; is the predicate and ?n is the object. The ? symbol in front of “n” signifies that this is a variable.\nSo how do triple patterns help us select triples from a given RDF graph? Suppose that we have an RDF dataset that consists of the following triples:\n\n\nWhen using the above triple pattern against this RDF dataset, it would be matched against the second triple (as the subject in the triple pattern matches the subject in this triple, and the predicate in the triple pattern also matches the predicate in this triple), and hence the n-variable would be matched with “Royal Infirmary of Edinburgh”.\n\nA simple SPARQL query example\nAnd how can we get the results for the n-variable? By specifying the SELECT-part of our query to include the n-variable. So a SPARQL query that gives us the name of &lt;http://dbpedia.org/resource/Royal_Infirmary_of_Edinburgh&gt; should look like this:\n\nThe result of this query will be: “Royal Infirmary of Edinburgh”\nNote that we re using the same variable name (i.e. “?n”) in the SELECT- and the WHERE- part. Note also that we could have given the variable any name we wanted, such as “?var”, “?name” or “?RIE_name”.\n\n\n\n3. Graph patterns\nA graph pattern is a collection of triple patterns, enclosed in { }.\nLet’s look at an example. Suppose that we have the following graph pattern:\n\nThis consists of two triple patterns:\n?h &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; &lt;http://schema.org/Hospital&gt; .\n?h &lt;http://xmlns.com/foaf/0.1/name&gt; ?n .\nNote that by using the same variable ?h in both triple patterns in the above graph pattern, we are asking for the same h-thing to be a hospital and to have name ?n.\nIf we used this graph pattern against the previous RDF dataset, it would be matched against the following two sets of triples:\n\nTo be more precise, in the first set of triples, ?h would be matched with &lt;http://dbpedia.org/resource/Royal_Infirmary_of_Edinburgh&gt; and ?n would be matched with “Royal Infirmary of Edinburgh” .\nWe can use the above graph pattern in a SPARQL query to get information of interest. Let’s look at two possible variations:\n\nSPARQL query - version 1 \nSuppose that we are interested in the names of all hospitals in our dataset. The SPARQL query for this is as follows:\n\nThe results of this query will be:\n\"Royal Infirmary of Edinburgh\"\n\"Aberdeen Royal Infirmary\"\n\n\nSPARQL query - version 2 \nNow suppose that we are interested in the names and URIs of all hospitals in our dataset. The SPARQL query for this is as follows:\n\nThe results of this query will be:\n“Royal Infirmary of Edinburgh” &lt;http://dbpedia.org/resource/Royal_Infirmary_of_Edinburgh&gt;\n“Aberdeen Royal Infirmary” &lt;http://dbpedia.org/resource/Aberdeen_Royal_Infirmary&gt;\n\n\nGraph patterns need to be fully matched \nNote that for a graph pattern to be matched, all its triple patterns need to be patched with the data. Let’s illustrate this with an example. Suppose that we want to run the same SPARQL query as above against the following dataset:\n\nThe results of this query will still be:\n\"Royal Infirmary of Edinburgh\" &lt;http://dbpedia.org/resource/Royal_Infirmary_of_Edinburgh&gt;\n\"Aberdeen Royal Infirmary\" &lt;http://dbpedia.org/resource/Aberdeen_Royal_Infirmary&gt;\nWhy is that? In other words, why will information for University Hospital Heidelberg not be returned? Because there is no triple in this dataset where the first triple pattern ( i.e. ?h &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#type&gt; &lt;http://schema.org/Hospital&gt; .) can be matched against &lt;http://dbpedia.org/resource/University_Hospital_Heidelberg&gt; .",
    "crumbs": [
      "Content",
      "Week 7: Improving Healthcare Processes & Integrating Health Data",
      "Topic 2: Integrating Health Data"
    ]
  },
  {
    "objectID": "content/week-7/topic-2.html#bonus-content-professor-aileen-keel",
    "href": "content/week-7/topic-2.html#bonus-content-professor-aileen-keel",
    "title": "Integrating Health Data",
    "section": "Bonus Content: Professor Aileen Keel",
    "text": "Bonus Content: Professor Aileen Keel\nProfessor Aileen Keel is the Director of the Innovative Healthcare Delivery Programme and former Deputy Chief Medical Officer for Scotland. In this interview, she is talking about the Innovative Healthcare Delivery Programme, which seeks to realise the vision of linking NHS Scotland’s rich data assets and deliver value rapidly to patients, healthcare professionals, and the wider NHS.\nhttps://media.ed.ac.uk/media/1_e06xrjmv",
    "crumbs": [
      "Content",
      "Week 7: Improving Healthcare Processes & Integrating Health Data",
      "Topic 2: Integrating Health Data"
    ]
  },
  {
    "objectID": "content/week-8/index.html",
    "href": "content/week-8/index.html",
    "title": "Overview",
    "section": "",
    "text": "This week’s topic has been curated by Dr Kevin Guyan, a Chancellor’s Fellow at the University of Edinburgh Business School.",
    "crumbs": [
      "Content",
      "Week 8: Data and Identity",
      "Overview"
    ]
  },
  {
    "objectID": "content/week-9/readings.html",
    "href": "content/week-9/readings.html",
    "title": "Reading: COVID and cause of death",
    "section": "",
    "text": "The required pre-reading for this session will be:\nArmstrong D. (2021). The COVID-19 pandemic and cause of death. Sociology of health & illness, 43(7), 1614–1626. https://doi.org/10.1111/1467-9566.13347\n Armstrong_2021.pdf \nAnd here is a prompt from Dr Perry, to guide your reading:\nIn this article, sociologist David Armstrong discusses, the complexity of death attribution – specifically during the covid 19 pandemic. A time, lest we forget, when ‘science’, ‘data’, and the very existence of a pandemic at all was subject to intense public scrutiny. He does this through an inquiry that foregrounds the sociology and the history of death certificates in the production of data.\nI think the article is nicely written, but those new to social sciences might find it a little dense. Keep in mind that Armstrong is trying to show the importance of the various social realities that shape the production of data. That is, try to think about the complex and often fraught ways in which, seemingly ‘solid’ data are constructed.\nA foundation of ‘evidence-based medicine’ is that medical data will guide us to more precise, and safer medicine. This article shows that even seemingly straight forward knowledge claims based upon healthcare data “how many people have died  of covid-19” can be tricky.\nNote how Armstrong shows that a decision was made to reorganise the ways that deaths were recorded in order that death certificates would produce more data about deaths with covid. Think about how such decisions are made, and according to what logics? Unchanged, the data would show that covid-19 was having a negligible effect on mortality. Does this change the way we think about the data we are using to know about the world? Should it? As we get and use more and more data do the processes that shape its production grow more or less important?",
    "crumbs": [
      "Content",
      "Week 9: Data Ownership",
      "Reading: COVID and cause of death"
    ]
  },
  {
    "objectID": "course-information/course-contacts.html",
    "href": "course-information/course-contacts.html",
    "title": "Course Contacts",
    "section": "",
    "text": "Course Organiser\nDr Kasia Banas\nEmail: Kasia.Banas@ed.ac.uk\n\n\nCourse Secretary\nMr Stewart Smith\nEmail: stewart3.smith@ed.ac.uk\n\n\nTeaching staff\nKarim Rivera-Lares (karim.rivera-lares@ed.ac.uk)"
  },
  {
    "objectID": "course-information/help-and-support.html",
    "href": "course-information/help-and-support.html",
    "title": "Help and Support",
    "section": "",
    "text": "Details on:\n\nHelp/support using Learn and other University systems\nBMTO Policies and Guidance\nCentral University Support\nStudent Rep Information\nCareers information\n\nhttps://www.ed.ac.uk/biomedical-sciences/bmto/bmto-undergraduate-students/academic-guidance-and-support/academic-guidance/student-guide"
  },
  {
    "objectID": "course-information/library-resources.html",
    "href": "course-information/library-resources.html",
    "title": "Library Resources",
    "section": "",
    "text": "Resource List\nThis is the Library’s online reading list tool. It links to any Resource List associated with this course and provides access to key readings. If you would like some guidance on using your Resource List, have a look at this video \n\n\nSubject Guide\nSubject Guides help you get the best out of the Library with information on finding academic literature, referencing and more.\n\nMedicine Subject Guide\nLiterature review & study skills resources for intercalating medical students Subject Guide\n\n\n\nDiscoverEd\nUse DiscoverEd to search the Library’s collections and find books, ebooks, ejournal articles and more. Use your University login to sign into your account and manage loans, requests and fines.\n\n\nInterLibrary Loans (ILL)\nIf the University of Edinburgh does not hold the material you need, you can use the InterLibrary Loans (ILL) service to request access to resources held in other libraries. Use the link, ‘InterLibrary Loan Request’ at the top of DiscoverEd. More information about ILL \n\n\nRequest a Book (RaB)\nYou can use the Library’s Request a Book (RaB) service to request purchase of books and other resources to be added to the Library collections to support your University study and research. More information about RaB \n\n\nScan & Deliver\nThis is the Library’s scan on demand service. Scan & Deliver provides scans of chapters or extracts of print items held at all University of Edinburgh Libraries (copyright restrictions apply). Use DiscoverEd to make a request. More information about Scan & Deliver"
  },
  {
    "objectID": "course-information/welcome.html",
    "href": "course-information/welcome.html",
    "title": "Welcome and Learning Outcomes",
    "section": "",
    "text": "Welcome!\nIn this course you will be introduced to data science techniques used in health and biomedicine, you’ll have a chance to practice these techniques in R, and you’ll learn about the social and societal issues surrounding the increased use of data science in human health. The course will be taught in a flipped classroom format: each week, you will be given a set of videos and readings to go through at home, and in the live sessions on Thursdays and Fridays we’ll discuss and practice what you learned from those materials. It is therefore crucial that you do complete the required preparation work before class; otherwise, you will not be able to fully participate.\nThe course materials have been prepared for you by various staff from the Usher Institute and the wider College of Medicine and Veterinary Medicine. The two weeks focused on data ethics will be delivered in collaboration with two experts in this topic: Dr Kevin Guyan from the Business School and Dr Max Perry from the School of Social and Political Science.\nWe look forward to working with you!"
  },
  {
    "objectID": "course-information/welcome.html#learning-outcomes",
    "href": "course-information/welcome.html#learning-outcomes",
    "title": "Welcome and Learning Outcomes",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nOn successful completion of this course, you should be able to:\n Apply a range of specialised data science techniques to different medical and healthcare scenarios.\n Analyse health and biomedical data with the use of the R programming language, including summarisation, visualisation and interpretation.\n Critically examine the ethical, societal and regulatory principles and implications of data science in health.\n Explain and critically discuss key concepts, principles and methods of data science in health."
  },
  {
    "objectID": "content/week-3/topic-3.html#data-storytelling-with-r",
    "href": "content/week-3/topic-3.html#data-storytelling-with-r",
    "title": "Data Storytelling",
    "section": "Data storytelling with R",
    "text": "Data storytelling with R\nIn the video below, Olga Pierce, a reporter specialising in data-driven stories, will show you how to turn a data finding into a data story with R. Olga outlines 3 case studies in her talk\nStorytelling with R, by Olga Pierce\nAttached files:\n Slides \n Transcript",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 3: Data Storytelling"
    ]
  },
  {
    "objectID": "content/week-3/version-controlled-projects.html",
    "href": "content/week-3/version-controlled-projects.html",
    "title": "Setting up repos for use with RStudio",
    "section": "",
    "text": "The following guide will walk you through the process of setting up a repo on gitHub.\nThe prerequisites for this are that you have a Github account. If you have not already created one, do so now https://github.com/signup.\nGitHub is an on-line platform for storing code or other text based projects, in repositories (repos). We will go through the process of setting up a repo step by step.\nThis is the process you should follow every week in your pair programming tutorials. Choose one person who will do this, on their own computer, and work through the steps together. Only do this on that one person’s computer. Don’t worry, all members of the team will get all work at the end, and all individuals will get a chance to do these steps on their own computer in future weeks."
  },
  {
    "objectID": "content/week-3/version-controlled-projects.html#github",
    "href": "content/week-3/version-controlled-projects.html#github",
    "title": "Setting up repos for use with RStudio",
    "section": "1 Github",
    "text": "1 Github\n\n1.1 Creating a new repo in GitHub\n\nSign into your GitHub account here https://github.com.\nNavigate to the repositories tab in your GitHub account.\n\n\n\n\n\n\n\nFigure 1: Github Repositories tab\n\n\n\n\nClick the big green button to create a new repository.\n\n\n\n\n\n\n\nFigure 2: Click “New” to begin creating a repo\n\n\n\nThis will take you to the repo initialization page.\n\n\n1.2 Initializing your repo\nYou can ignore the “template” section. If you are a keen bean, you can explore this later. It will allow you to create a template repo for all future weeks based on this week’s repo. But we will not go over it here.\n\nGive your repo a meaningful name.\n\ntutorial-repo sound good, right? 🤨…?\n\n\n\n\n\n\n\n\nFigure 3: Naming your repo. Don’t forget to give a description.\n\n\n\n\nAlso fill in the Description.\n\nIt says “optional”, but fill it in now for benefits later.\nMore detail = more benefit.\n\n\nIf we follow the defaults, we will create a completely blank repo. That’s fine, and sometimes may be what you want. But lets make gitHub do some of the tedious bits for us (Figure 4).\n\nTick the add README file box\nclick the add .gitignore drop-down and find your language of choice (if you need a clue, its R).\nWe can leave the license as none for now.\n\nBut you should totally consider adding a license for personal projects, and definitely for any future research projects.\n\n\n\n\n\n\n\n\nFigure 4: Add a readme - whatever you wrote in your description abouve will be placed here automatically. Also, select the .gitignore file appropriate for R.\n\n\n\nAnd click the big green button at the bottom.\n\n\n\n\n\n\nFigure 5: Click “Create repository”\n\n\n\n\n\n1.3 Preparing to “clone” the repository\nAfter clicking the green button, you will be taken to a new page. This is your new repo. However, it so far is only accessible form GitHub. To make use of it we need to obtain a copy of it on our local machine. the process for doing this is called cloning.\n\nLocate the next big green button - it says Code - and click.\n\nYou will see a web URL with a copy icon next to it. Click the copy icon.\n\n\n\n\n\n\n\nFigure 6: Copy the repo URL after clicking the “Code” button."
  },
  {
    "objectID": "content/week-3/version-controlled-projects.html#the-terminal",
    "href": "content/week-3/version-controlled-projects.html#the-terminal",
    "title": "Setting up repos for use with RStudio",
    "section": "2 The terminal 😨",
    "text": "2 The terminal 😨\n\n2.1 Don’t panic - only one command\nDecide on a good place to save all your course resources. It is up to you where you want these to live, but make sure it makes sense to you, e.g.:\n\nDesktop = bad\nSomewhere in Documents = better\n\nSee Figure 18 at the end for a suggestion for a suggested set-up. You can create a new directory (folder) using whichever file manager you like, e.g. Windows Explorer, Mac Finder, terminal, etc.\n\nHere I created folder in Documents called data-science-tutorials.\nI then right-clicked inside the folder in Windows explorer and clicked Open in terminal…\n…and pasted the web URL copied from GitHub\n\n\n\n\n\n\n\nFigure 7: Paste the repo URL nto your terminal. The repo will be cloned in whixhever directory you are in when you do this, so make sure you are in the directory you want to be in!\n\n\n\nPress enter, to perform the clone. Once completed you will be shown a short summary. What it says is not important, but go ahead and read it as an optional exercise; see what you can glean.\n\n\n\n\n\n\nFigure 8: Pressin genter will insruct Git to download the repo.\n\n\n\nSuccess! And that’s the only interaction with the terminal we need."
  },
  {
    "objectID": "content/week-3/version-controlled-projects.html#rstudio",
    "href": "content/week-3/version-controlled-projects.html#rstudio",
    "title": "Setting up repos for use with RStudio",
    "section": "3 RStudio",
    "text": "3 RStudio\nAt this point we now have a repo on the GitHub cloud storage platform.\nand we have cloned (copied) it to our local computer.\nNow we want to write our R code for the repo inside an RStudio project.\nThis will be mostly familiar, but follow along anyway, because there may be a few points that are slightly different.\n\n3.1 Setting Up the project in RStudio\n\nCreate a new project\n\nEither File &gt; New project\nOr click the New project drop-down\n\n\n\n\n\n\n\n\nFigure 9: The Project drop-down menu can be used for creaing new projects.\n\n\n\n\nIn the menu that opens, select Existing Directory\n\n Remember, we cloned our repo somewhere, possibly in Documents…\n\nUse the Browse button to find your repo.\nWhen you have found it click Create Project.\n\n\n\n\n\n\n\nFigure 10: Use the browse button to find the locaton you cloned your repo to.\n\n\n\n\n\n3.2 Check out out new Git tracked R projects\nOpen the Files pane, and you will see that a few things that are different from the usual state of a new project.\n\nA .git folder\n\nDon’t mess with this!\nBut, by all means, have sniff around it if you’re one of the afore mentioned keen beans.\n\nA README.md\n\nThis is the readme file from GitHub\nYou can edit this if you like - it’s just a text file and it uses markdown syntax\n\nA .gitignore file\n\nWe will take a closer look at this shortly.\n\n\n\n\n\n\n\n\nFigure 11: Your project already contains the files you used to initialise itin GitHub\n\n\n\n\n\n3.3 RStudio-Git integration\nNow that we are using Git with RStudio, we should be able to find a Git pane (Figure 12). By default, I think it appear in the top left panel of Rstudio. It may be somewhere else, so if you don’t see it, check the other panels\nThe desire outcome here, is:\n\nIdentify which files we want Git to track\n\nWe don not have to track all files.\n\nMark them as files of interest\n\nCalled “adding them to the index”.\n\nCommit them\n\nIt is only after this point that Git starts tracking the files.\n\n\nOne you have found the Git pane, you should see two files listed with some yellow squares and a check-box next to them.\nThis is how we tell Git to track and changes to files. We can choose which files we want git to track by ticking the check-box.\n\nIn this case we do want to track both files, so tick both check-boxes.\n\n - Then click the Commit button. - Add a commit message - No need to be too laconic here. - Make it meaningful. Detail is good. - Click Commit.\n\n\n\n\n\n\nFigure 12: Add a descriptive commit message. More detail is good here, especially as your project ages. It will allow you to make sense of historical commits, like if you… need to find that time… you did that thing…\n\n\n\n\nYou will then be given a short summary or the commit\nClick close\nCheckout the Git pane again - it should now be empty?\n\n\n\n\n\n\n\nFigure 13: If your commit succeeds you will be given a short summary. If it fails, you may need to call in some support.\n\n\n\n\n\n3.4 Let’s change something\n\nFind the .gitignore file in your Files pane, and click on it.\nAdd two lines at the end of the file.\n\n.DS_Store\nthumbs.db\n\n\nNo matter what computer you are using, add both lines. Your .gitignore file should now look like Figure 14\n\n\n\n\n\n\nFigure 14: .DS_Store files often crop up in reos. They are Mac only files and they are a nuisance for your repo. Add them to your .gitignore everytime. Windows has a similar file, but it seems to be less pervasive.\n\n\n\n\n\n3.5 Commit cyle\nOnce again, take a look at the Git pane. We should see that the .gitignore file has appeared again. This time instead of the yellow square with a ? inside, it has a blue square with M inside. This is Git saying\n\nYou know that file you asked me to track? Well it’s changed\n\nGo through the process again of adding the file, writing a commit message, and committing the changes.\nWe have just completed the “commit cycle”. This is the most basic and important workflow to learn in Git. Committing is like saving. Do it often!\n\n\n\n\n\n\nFigure 15: Get used to the Git commit cycle. Work &gt; Save &gt; Commit &gt; back to work. And do it often.\n\n\n\n\n\n3.6 Push: Lets share our work with the world\nMake sure you have committed all changes that you need to make sure your team mates will get the most up to date version. This is the same as (Figure 12, but with an extra step: click Push.\n\n\n\nAnother commit message. Do not use the same commit message as shoen here. A commit message needs to describe the changes that happened. What might be more meaningful here?\n\n\n\n\n\n\n\n\nFigure 16\n\n\n\nBut what does that actually do? It sends (pushes) our work to the GitHub cloud platform, and because our repos are public, the whole world can access it.\nIt allows break form the commit cyclein that once you push, your work is now backed up remotely, as well as saved locally.\n\n\n\n\n\n\nFigure 17: Commit cycle with the added Push step\n\n\n\n\n\n3.7 Pull: How do I get the work?\nMaybe the world doesn’t want our work… But our team mates probably do. The process for getting it is the same clone process we did already. Everyone else in the team that wants a copy of the repo should repeat the steps from 1.3 to 2.1\n ## Final set-up tips\nHere is a suggested directory structure to think about. You do not have to do this, but it is clear, simple, and easy to compare with your team mates\n\n\n\n\n\n\nFigure 18: A possible setup for your work, and the workflow for a pair-programming session."
  }
]