[
  {
    "objectID": "content/week-3/topic-4.html",
    "href": "content/week-3/topic-4.html",
    "title": "Plotting data with ggplot2",
    "section": "",
    "text": "Plotting in R is great because your code will always be there for you to check and reproduce what you did, and even better than that, you’ve already seen that you can start creating nice-looking plots straight away with just a few lines of code.\nIt’s extremely satisfying to be able to produce multiple plots with consistent formatting by just changing one line here or there, and not having to remember what or where you clicked in a menu!\nThe R package we use most frequently for plotting is ggplot2, and it’s part of the tidyverse which you’ll now be starting to get quite familiar with.\nLet’s start by looking at the main commands you need to know in ggplot2 and how to use these commands in order to display your data and format your plot the way you want.\n\n\n\n\n\nartwork by Allison Horst\n# Filter out/remove NA values (which were the code for a specific hospital) from HBName (SB0801 before the join)\n\n\nAfter opening a new script or RMarkdown file, the next thing we have to do is import our data.\nSince we’re using the same data we have used before, the Cancelled Planned Operations dataset from Public Health Scotland, we can just copy some of our previous code (look back at Week 2, topic 6). This is one of the great things about coding!\nYou’ll spend a lot of time copying previous code you’ve written, copying code you’ve Googled on the web, or copying a colleague’s code. Don’t worry about trying to learn commands from memory. This will happen naturally for the ones you use most frequently, and for the rest, there is always Google or the Help tab!\nCopy and paste the following code into your R Script or into a code chunk in your RMarkdown file.\n\n# Load packages \nlibrary(tidyverse)\n# library(lubridate)\n## library(lubridate) is only necessary if you are using Noteable, the newest version of tidyverse includes lubridate. If using Noteable, delete the # comment in the line above  \n \n# Read in the cancellations dataset \ncancelled_raw &lt;- read_csv(\"https://www.opendata.nhs.scot/dataset/479848ef-41f8-44c5-bfb5-666e0df8f574/resource/0f1cf6b1-ebf6-4928-b490-0a721cc98884/download/cancellations_by_board_august_2024.csv\")\n \nhb &lt;- read_csv(\"https://www.opendata.nhs.scot/dataset/9f942fdb-e59e-44f5-b534-d6e17229cc7b/resource/652ff726-e676-4a20-abda-435b98dd7bdc/download/hb14_hb19.csv\")\n \ncancelled &lt;- cancelled_raw %&gt;%\n  # Join cancelled to hb\n  left_join(hb, by = c(\"HBT\" = \"HB\")) %&gt;%\n  # Select the variables we're interested in\n  select(Month,\n         HBName,\n         TotalOperations,\n         TotalCancelled) %&gt;%\n  # Filter out/remove NA values (which were the code for a specific hospital) from HBName (SB0801 before the join) \n  filter(HBName != \"NA\") %&gt;%\n  # Reformat the month column to separate out year, month, day \n  mutate(Month = ymd(Month, truncated = TRUE))\n\nWe don’t need to load ggplot2 separately because it is loaded as part of the tidyverse set of packages.\nHaving run that code, you should now see three objects in your environment tab. The two initial datasets we read in, and then our tidied dataset, cancelled.\n\n\n\n\n\nCheck the data\nClick on cancelled to check that it has been read in ok. Remember we can do this by clicking on the object in the Environment tab and it will open in a new tab beside your script, looking just like a spreadsheet.\nWe can also check the range of our data to see what time span is covered:\nHere we are using the year() function from lubridate to extract only the year from the part of the date in the Month variable.\nYou should see the following output\n\ncancelled %&gt;%\n   distinct(year(Month))\n\n# A tibble: 11 × 1\n   `year(Month)`\n           &lt;dbl&gt;\n 1          2015\n 2          2016\n 3          2017\n 4          2018\n 5          2019\n 6          2020\n 7          2021\n 8          2022\n 9          2023\n10          2024\n11          2025\n\n\nWe can use a similar bit of code to look at the Health Boards:\nFrom which you should see the following output\n\ncancelled %&gt;%\n   distinct(HBName)\n\n# A tibble: 14 × 1\n   HBName                       \n   &lt;chr&gt;                        \n 1 NHS Ayrshire and Arran       \n 2 NHS Borders                  \n 3 NHS Dumfries and Galloway    \n 4 NHS Fife                     \n 5 NHS Forth Valley             \n 6 NHS Grampian                 \n 7 NHS Greater Glasgow and Clyde\n 8 NHS Highland                 \n 9 NHS Lanarkshire              \n10 NHS Lothian                  \n11 NHS Orkney                   \n12 NHS Shetland                 \n13 NHS Tayside                  \n14 NHS Western Isles            \n\n\nNotice that in both of these chunks of code, we have not assigned the value to an object. That is, we have not given them names using the assignment operator &lt;- and so they have not appeared in our environment.\nThat’s because we’re just doing a quick check. This code is not particularly relevant to our plotting or analysis code. So we can either write and run it in a code chunk, then delete it if we don’t want to keep it there, or we can run it in the console so that it doesn’t get saved in our RMD file. Either is fine.",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 4: Plotting data with ggplot2"
    ]
  },
  {
    "objectID": "content/week-3/topic-4.html#the-ggplot2-package",
    "href": "content/week-3/topic-4.html#the-ggplot2-package",
    "title": "Plotting data with ggplot2",
    "section": "",
    "text": "Plotting in R is great because your code will always be there for you to check and reproduce what you did, and even better than that, you’ve already seen that you can start creating nice-looking plots straight away with just a few lines of code.\nIt’s extremely satisfying to be able to produce multiple plots with consistent formatting by just changing one line here or there, and not having to remember what or where you clicked in a menu!\nThe R package we use most frequently for plotting is ggplot2, and it’s part of the tidyverse which you’ll now be starting to get quite familiar with.\nLet’s start by looking at the main commands you need to know in ggplot2 and how to use these commands in order to display your data and format your plot the way you want.\n\n\n\n\n\nartwork by Allison Horst\n# Filter out/remove NA values (which were the code for a specific hospital) from HBName (SB0801 before the join)\n\n\nAfter opening a new script or RMarkdown file, the next thing we have to do is import our data.\nSince we’re using the same data we have used before, the Cancelled Planned Operations dataset from Public Health Scotland, we can just copy some of our previous code (look back at Week 2, topic 6). This is one of the great things about coding!\nYou’ll spend a lot of time copying previous code you’ve written, copying code you’ve Googled on the web, or copying a colleague’s code. Don’t worry about trying to learn commands from memory. This will happen naturally for the ones you use most frequently, and for the rest, there is always Google or the Help tab!\nCopy and paste the following code into your R Script or into a code chunk in your RMarkdown file.\n\n# Load packages \nlibrary(tidyverse)\n# library(lubridate)\n## library(lubridate) is only necessary if you are using Noteable, the newest version of tidyverse includes lubridate. If using Noteable, delete the # comment in the line above  \n \n# Read in the cancellations dataset \ncancelled_raw &lt;- read_csv(\"https://www.opendata.nhs.scot/dataset/479848ef-41f8-44c5-bfb5-666e0df8f574/resource/0f1cf6b1-ebf6-4928-b490-0a721cc98884/download/cancellations_by_board_august_2024.csv\")\n \nhb &lt;- read_csv(\"https://www.opendata.nhs.scot/dataset/9f942fdb-e59e-44f5-b534-d6e17229cc7b/resource/652ff726-e676-4a20-abda-435b98dd7bdc/download/hb14_hb19.csv\")\n \ncancelled &lt;- cancelled_raw %&gt;%\n  # Join cancelled to hb\n  left_join(hb, by = c(\"HBT\" = \"HB\")) %&gt;%\n  # Select the variables we're interested in\n  select(Month,\n         HBName,\n         TotalOperations,\n         TotalCancelled) %&gt;%\n  # Filter out/remove NA values (which were the code for a specific hospital) from HBName (SB0801 before the join) \n  filter(HBName != \"NA\") %&gt;%\n  # Reformat the month column to separate out year, month, day \n  mutate(Month = ymd(Month, truncated = TRUE))\n\nWe don’t need to load ggplot2 separately because it is loaded as part of the tidyverse set of packages.\nHaving run that code, you should now see three objects in your environment tab. The two initial datasets we read in, and then our tidied dataset, cancelled.\n\n\n\n\n\nCheck the data\nClick on cancelled to check that it has been read in ok. Remember we can do this by clicking on the object in the Environment tab and it will open in a new tab beside your script, looking just like a spreadsheet.\nWe can also check the range of our data to see what time span is covered:\nHere we are using the year() function from lubridate to extract only the year from the part of the date in the Month variable.\nYou should see the following output\n\ncancelled %&gt;%\n   distinct(year(Month))\n\n# A tibble: 11 × 1\n   `year(Month)`\n           &lt;dbl&gt;\n 1          2015\n 2          2016\n 3          2017\n 4          2018\n 5          2019\n 6          2020\n 7          2021\n 8          2022\n 9          2023\n10          2024\n11          2025\n\n\nWe can use a similar bit of code to look at the Health Boards:\nFrom which you should see the following output\n\ncancelled %&gt;%\n   distinct(HBName)\n\n# A tibble: 14 × 1\n   HBName                       \n   &lt;chr&gt;                        \n 1 NHS Ayrshire and Arran       \n 2 NHS Borders                  \n 3 NHS Dumfries and Galloway    \n 4 NHS Fife                     \n 5 NHS Forth Valley             \n 6 NHS Grampian                 \n 7 NHS Greater Glasgow and Clyde\n 8 NHS Highland                 \n 9 NHS Lanarkshire              \n10 NHS Lothian                  \n11 NHS Orkney                   \n12 NHS Shetland                 \n13 NHS Tayside                  \n14 NHS Western Isles            \n\n\nNotice that in both of these chunks of code, we have not assigned the value to an object. That is, we have not given them names using the assignment operator &lt;- and so they have not appeared in our environment.\nThat’s because we’re just doing a quick check. This code is not particularly relevant to our plotting or analysis code. So we can either write and run it in a code chunk, then delete it if we don’t want to keep it there, or we can run it in the console so that it doesn’t get saved in our RMD file. Either is fine.",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 4: Plotting data with ggplot2"
    ]
  },
  {
    "objectID": "content/week-3/topic-4.html#the-core-elements",
    "href": "content/week-3/topic-4.html#the-core-elements",
    "title": "Plotting data with ggplot2",
    "section": "The core elements",
    "text": "The core elements\n\nThe Elements of a ggplot\nThe first command you’ll need for creating any type of plot is the command ggplot(). The “gg” in ggplot stands for “grammar of graphics” and it has to do with the layering framework which we use to build up our plots.\nWe’re familiar with the grammar of the English language, which refers to the structure of the language and the rules we use to construct meaningful sentences. Similar concepts have been applied to plotting with ggplot2.\nThere is a structure of layers, a bit like the adjectives, nouns, and verbs, in the English language, and then there are the rules for how we assemble these layers (also called aesthetic mappings).\nWe’ll look in more depth at how these elements interact later in this topic, but for just now it’s worth noting that this setup makes it a lot easier to produce informative plots in R.\nThere are 7 main elements or layers we can use in our plots, and 3 of these are essential, without which, the plot just won’t work. We’ll explore the essential elements first, then go on to look at a few additional ones too.\nThe 3 essential elements are:\n\nthe data - fairly obvious, without this we wouldn’t have a plot!\nthe aesthetics - how we map our data onto the canvas (e.g., x and y variables).\nthe geometrical objects - the type of plot we want to make, (e.g., scatter, bar, line, etc.).\n\nSome of the non-essential elements include:\n\na facet layer - allows us to subset our data so that we can view multiple plots side by side, and arranged by category.\na theme layer, which will provide the finishing touch to make our plots look lovely.\n\n\n\n\n\n\n\n\nThe Canvas (data and aesthetics)\nLet’s start with the plotting function ggplot() which lets R know we are about to create a plot. And we’ll specify which variables we want to display on the x and y axes. Copy and paste the following code below the last bit of code in your script:\n\n cancelled %&gt;%\n   ggplot(aes(x = Month, y = TotalOperations))\n\n\n\n\n\n\n\n\nA blank canvas should have appeared in your Plots tab or under your code chunk if you have your output showing inline in RMarkdown (the default). This is the first layer in our plot and lets us know… that R knows… that we want to create a plot, but there are no data points yet.\n\nthe data cancelled is being piped into our ggplot() function, the starting point for all our plots\nthe aesthetics are present within the aes() function letting R know what we want on the x and y axes\nthe geometrical objects layer is missing!\n\nWithout the geometrical layer, R doesn’t know what type of plot to make.\n\n\nThe Geoms\nWe use, what are called, geoms (short for geometrical objects or geometries) to let R know the type of plot we want.\nTo add a layer in ggplot2, we have to make sure to add a + sign at the end of each line. This + sign behaves in a very similar way to the %&gt;% operator we are already familiar with, and indeed there has been some discussion surrounding the development of the ggplot2 package, as to whether the %&gt;% should be used instead of the + but for various reasons the + remains. We just need to remember that everything after the ggplot() function needs a + sign.\nLet’s add a geom, and since we’re looking at changes over time, let’s add a geom to create a line graph, geom_line():\n\ncancelled %&gt;%\n   ggplot(aes(x = Month, y = TotalOperations)) +\n    geom_line()\n\n\n\n\n\n\n\n\nNow we have a complete plot, with only 3 short lines of code! But, our lines don’t look right. That’s because for each date in our dataset, we have observations or values for 14 different Health Boards, and ggplot2 joins the points for all of these Health Board values first before moving on to the next date and so forth. To correct this, all we need to do is add a group mapping within our aes() function. This lets R know that each category within the specified group gets its own line:\n\ncancelled %&gt;%\n  ggplot(aes(x = Month, y = TotalOperations, group = HBName)) +\n  geom_line()\n\n\n\n\n\n\n\n\nNow we have a complete plot, with only 3 short lines of code! But, our lines don’t look right. That’s because for each date in our dataset, we have observations or values for 14 different Health Boards, and ggplot2 joins the points for all of these Health Board values first before moving on to the next date and so forth. To correct this, all we need to do is add a group mapping within our aes() function. This lets R know that each category within the specified group gets its own line:\n\ncancelled%&gt;%\n  ggplot(aes(x = Month, y =TotalOperations, group =HBName)) +\n  geom_line()\n\n\n\n\n\n\n\n\nThere are various geoms that we can use, and we’ll cover these in more detail later in the topic. For example, we have geoms for bar charts, line plots, heat maps, scatter plots, box plots, and density plots. You name it, there’s probably a geom for it! There are currently at least 40 geoms from which to choose!",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 4: Plotting data with ggplot2"
    ]
  },
  {
    "objectID": "content/week-3/topic-4.html#the-aesthetics-inside-and-outside",
    "href": "content/week-3/topic-4.html#the-aesthetics-inside-and-outside",
    "title": "Plotting data with ggplot2",
    "section": "The aesthetics: Inside and outside",
    "text": "The aesthetics: Inside and outside\n\nThe Aesthetics - Outside\nThe aesthetics is where all the cool stuff happens in ggplot2. This is where the values in our columns (our variables) are converted into visual scales, be that x- or y-axis scales, or colour or size scales. The aes() function allows us to access and visualise large amounts of information quickly and efficiently.\n\ncancelled %&gt;%\n  ggplot(aes(x = Month, y = TotalOperations, group = HBName)) +\n  geom_line(color = \"darkgreen\")\n\n\n\n\n\n\n\n\nWhat we can include or not include within the aesthetic argument however, is sometimes slightly confusing, and an argument such as colour, which we see here, has a slightly different purpose depending on where it appears in our code, so let’s dive a little bit deeper into this.\nAesthetic-type arguments, e.g. line type and colour, can be specified both inside, and outside of the aesthetic brackets ( aes() ), but they have different roles to play in the different settings.\nIn the code above we can see that the colour aesthetic is set to “darkgreen” which lets R know that we want all of the lines to be dark green.\nR understands a lot of colours by their name, so try out a few and see how good it is! Change the code above to be a color of your choosing - but do not forget the quotation marks around the name (demarking a character string). For a more nuanced choice of colours, R also understands hex (hexadecimal) codes so you have a full range of colours at your fingertips. More on colour palettes later.\nComing back to our code example above, we can see that the colour argument is not within the aesthetic brackets, it’s included instead, inside geom_line(). This is because it has been set to a single value (“darkgreen”), and so that colour is the same for all points in the dataset, regardless of the values in the dataset.\nIf we wanted our data points to have a different shape or colour, depending on the values in the dataset, we would have to pass colour as an aesthetic argument, inside the brackets.\n\n\nThe Aesthetics - Inside\nAt the moment, our plot is very busy, it’s hard to distinguish between the different Health Boards and it would be useful to make them more visible by assigning each a different colour, so let’s add colour = HBName within our aesthetic mapping:\n\ncancelled %&gt;%\n  ggplot(aes(x = Month, y = TotalOperations, colour = HBName)) +\n  geom_line()\n\n\n\n\n\n\n\n\nNotice that we no longer need to specify group = HBName in our aesthetics because the colour attribute is both grouping and colouring each category within HBName.\nTo summarise, when our aesthetic arguments (colour, line type, etc.) appear:\n\nwithin aes(), the argument changes based on the values of the variable\noutside aes(), the argument is given a single value and doesn’t change based on the values of the variable\n\nThis ability to “map” within the aesthetic function allows us to visualise variations across multiple variables simultaneously.\nNotice that as well as assigning different colours to the different Health Boards, ggplot2 was also clever enough to create a legend automatically without us even telling it to do so!\n\n\nAesthetics galore!\nWe’ve seen how we can map a variable to color, to distinguish between different categories. Depending on the type of plot we are using, we can also map a variable to many other aesthetic arguments. For example, line thickness, the size of points, the shape of points, and how transparent they are (useful for overplotting in busy graphs). ggplot2 gives you a lot of control over these and you can find out more by searching for aes() in the help tab and clicking on the ggplot2-specs vignette, or by clicking here.\nOur plot now has colour but it’s still looking quite crowded so let’s explore how we can improve the presentation further.",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 4: Plotting data with ggplot2"
    ]
  },
  {
    "objectID": "content/week-3/topic-4.html#additional-elements-facets-and-themes",
    "href": "content/week-3/topic-4.html#additional-elements-facets-and-themes",
    "title": "Plotting data with ggplot2",
    "section": "Additional elements: Facets and themes",
    "text": "Additional elements: Facets and themes\n\nFaceting\nFaceting provides yet another dimension to our data and allows us to compare values within a plot and between plots, making it easier to see what’s going on.\nLet’s take a look at the code for faceting:\n\ncancelled %&gt;%\n  ggplot(aes(x = Month, y = TotalOperations)) +\n  geom_line(colour = \"darkgreen\") +\n  facet_wrap(~HBName)\n\n\n\n\n\n\n\n\nThe function facet_wrap() takes a formula argument, which just means that we need to remember to use the tilde sign (~) before the variable we’re interested in.\nWhen we use facet_wrap(), we’re creating separate graphs subset by the specified variable (in this case Health Board). It’s a way of easily splitting up our data into the various levels, in a variable with categories.\nAfter faceting, we can see that it’s very difficult to see the trends for the smaller Health Boards as all of the axes scales have been aligned. This can be useful when comparing categories but in this dataset, we might be interested in the overall trends across each individual Health Board.\nTo change this there is an argument within facet_wrap() which makes our axes “free”! Let’s add that in:\n\ncancelled %&gt;%\n  ggplot(aes(x = Month, y = TotalOperations)) +\n  geom_line(colour = \"darkgreen\") +\n  facet_wrap(~HBName, scales = \"free\")\n\n\n\n\n\n\n\n\nNow we can see very similar trends across all Health Boards concerning the recent drop in the total number of operations in 2020 and an increase around 2021.\n\n\nThemes\nThe default plot theme for ggplot2 has a grey background, but you do not have to be stuck with this, there are endless options for customising your plots, but one way you can do this quickly is by using ggplot2’s ready-to-go themes.\n\ncancelled %&gt;%\n  ggplot(aes(x = Month, y = TotalOperations)) +\n  geom_line(colour = \"darkgreen\") +\n  facet_wrap(~HBName, scales = \"free\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nIn the code above you can see that we have added another line of code, theme_bw()which calls on ggplot2’s black-and-white theme.\nThe theme layer controls all of the visual elements which are not directly related to the interpretation of the values in your data. So it doesn’t control the colours of your data points, but it can control the background colour on which your data points lie.\nThere are many built-in themes in ggplot2 and you can explore them and discover the ones that you like best.\nThe nice thing about the layering structure of ggplot2, is that you can take a default layer, like theme_bw(), and then add your own modifications on top of it, by calling the theme() command, so your plots are entirely customisable.\nYou can find out more about the generic theme()command in the help menu of RStudio, where there are endless options for customisation.",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 4: Plotting data with ggplot2"
    ]
  },
  {
    "objectID": "content/week-3/topic-4.html#geoms-galore",
    "href": "content/week-3/topic-4.html#geoms-galore",
    "title": "Plotting data with ggplot2",
    "section": "Geoms galore!",
    "text": "Geoms galore!\nSo far in this topic, we’ve explored some of the options we can use in geom_line() the geom used to create line graphs, but there are many more geoms to choose from.\nHere are some of the main ones you’re likely to find useful in your analyses and reporting:\ngeom_point() and geom_jitter() are scatter plots and are quite similar. geom_jitter() can be useful when you have over-plotting. It shifts overlapping points to the side slightly, making it easier to see the density in areas of clustering.\n\n\n\n\n\ngeom_bar() and geom_col() are also very similar but with a few slight differences to do with how data is counted. If you are interested in these counting differences, read on. If not, you can skip the following paragraph.\nIf you want the height of the bars to represent values in the data, use geom_col(). If rather, you would like the height of the bar to be proportional to the number of cases in each group, use geom_bar(). On a more technical note, this is because geom_bar()uses stat_count() by default meaning it counts the number of cases at each x position, while geom_col() uses stat_indentity() by default meaning it leaves the data as it is.\n\n\n\n\n\ngeom_histogram() is very useful for looking at the distribution of your data, and good for detecting any unusual observations or outliers. A histogram is used to visualise the distraction of a single continuous variable.\n\n\n\n\n\nNote: We wouldn’t normally use a histogram to explore time-series data in this way, but we’ve kept the same dataset for consistency.\ngeom_boxplot() is another useful plot for seeing how your values are spread out, particularly if you’re comparing a number of variables together or wish to compare between multiple groups.\nBoxplots are a standardized way of showing the distribution of data based on a five-number summary:\n\nminimum\nfirst quartile (Q1)\nmedian\nthird quartile (Q3)\nmaximum.\n\nThe “box” shows where the middle portion of the data is: the interquartile range (IQR).\n \nIf we would like to add labels or text to our plots, in addition to points and lines, or even to replace points, we can use geom_label() and geom_text().\n\nAn intentional omission…\nYou may have noticed that one of the most common, and controversial, types of charts has not yet been discussed: the pie chart.\nWhile it is possible to make pie charts in R, we will not be learning how to do so in the course. In fact, I hope to convince you that you should not even want to know how to make one! Pie charts are what we can consider bad by definition. A pie chart is a circle (or pie) divided into sections (slices of the pie) that represent a portion of the whole. Nothing offensive there. However, the issue is that humans are not good at reading and distinguishing angles. By definition, they are not human-readable and certainly not a tool to tell a data story.\nYan Holtz has a wonderful, and succinct, post called The issue with pie chart that I would recommend reading if you need more evidence.\n\n\nSelecting an appropriate geometry\nThere are many factors to consider when selecting the geometry or type of plot to make. Most importantly, you need to first consider your data and what options are available to you, reflecting on questions such as:\n\nwhat data types are my variables of interest (e.g., numeric/continuous, factor/categorical, etc.)?\nhow much, if any, data is missing from my variables of interest?\nwhat format is my data in, is it suitable for plotting?\nwhat is the goal of the visualisation (e.g., to show change over time, to compare values between groups, to show how data is distributed, to show a part-to-whole composition, etc.)\n\nThere is a wonderful web page called From Data to Viz (also by Yan Holtz) which can help you in this decision making process. Data To Viz serves as a comprehensive guide to chart types, organized by the format of the data you have. The web page effectively functions as a decision tree to help guide to showing actionable insights in your data visualisation. What I enjoy the most is that once you have decided on a chart type, there are recommendations and warnings to consider around that chart type as well as links to the R Graph Gallery with R code (largely ggplot2 and its extensions) and an accompanying explanation!",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 4: Plotting data with ggplot2"
    ]
  },
  {
    "objectID": "content/week-3/topic-4.html#optional-practice",
    "href": "content/week-3/topic-4.html#optional-practice",
    "title": "Plotting data with ggplot2",
    "section": "Optional: Practice 👩‍💻",
    "text": "Optional: Practice 👩‍💻\nIf you are feeling confident, why not see if you can reproduce the following plots?\nThink of the different elements needed to recreate the plots and use the materials in this topic to assist you! If you are struggling to know where to begin in recreating either plot, I would suggest moving on to the Plotting Practice Document in the next topic and then returning to this task.\nThe plots below are quite complex, so do share your results on the discussion board if you are able to replicate parts of them! We can then all learn from each other the different ways one could approach the tasks. Feel free to post on the discussion boards if you are having problems reproducing either plot below, or if you have any questions (such as how the plots above were made where the code is not shared)!\n\nPlot 1\n\n\n\n\n\nHint: You will need to use the dplyr::mutate() function, and some functions from the lubridate package!\n\n\nPlot 2\n\n\n\n\n\nHint: You will need to use the dplyr::mutate() function, and some functions from the lubridate package! Remember that functions typically have arguments to help you customize different things.\nHint 2: as we have discussed, ggplot2 works in layers, which means you can layer multiple elements on top of each other!",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 4: Plotting data with ggplot2"
    ]
  },
  {
    "objectID": "content/week-3/topic-4.html#optional-data-visualisation-using-ggplot2",
    "href": "content/week-3/topic-4.html#optional-data-visualisation-using-ggplot2",
    "title": "Plotting data with ggplot2",
    "section": "Optional: Data visualisation using ggplot2",
    "text": "Optional: Data visualisation using ggplot2\n\nData visualisation in R, an introduction with ggplot2\nThe following video is a recording from a guest lecture by Dr Sam Tyner introducing data visualisation in R using the ggplot2 package.\n\nHere is a Link to Dr Tyner’s slides.\nThe code used to produce the slides is available on GitHub.\nOn the penultimate slide, Dr Tyner lists many links for some additional resources, which are copied below for your convenience.\n\nggplot2 book\nR for Data science book\nTidy Tuesday\nMy advice for getting help in R\nThomas Lin Pedersen’s (one of the current maintainers of ggplot) ggplot2 webinar: part 1 and part 2\nAll RStudio Cheat Sheets\nWill Chase’s Design Talk\na 2-hour version of this tutorial\n\n\nExamples of Extensions\n\nDomain Specific\n\nNetworks\n\ngeomnet\nggraph\n\nTime Series\n\nggaluvial\nsugrrants\n\nSciences\n\nggenes\nggtree\nggseqlogo\nggspectra\n\n\n\n\nAppearance customisation\n\nArrange ggplots\n\ncowplot\npatchwork\ngganimate\n\nCustom themes and/or scales\n\nggthemes\nggsci\nggtech\nggthemr\nxkcd\nggpubr",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 4: Plotting data with ggplot2"
    ]
  },
  {
    "objectID": "content/week-3/topic-4.html#optional-creating-a-line-graph-in-ggplot2",
    "href": "content/week-3/topic-4.html#optional-creating-a-line-graph-in-ggplot2",
    "title": "Plotting data with ggplot2",
    "section": "Optional: Creating a line graph in ggplot2",
    "text": "Optional: Creating a line graph in ggplot2\nData Visualisation Demonstration in R\nIn the video below, Dr Holly Tibble, Chancellor’s Fellow in Medical Informatics at the University of Edinburgh will demonstrate how to plan, create, and describe a simple visualisation in R for asthma deaths in the UK throughout the year, between 2011 and 2012.\n\nHere is a Link to the R script and Link another to the transcript.",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 4: Plotting data with ggplot2"
    ]
  },
  {
    "objectID": "content/week-5/solutions-to-practice-exercises.html",
    "href": "content/week-5/solutions-to-practice-exercises.html",
    "title": "Solutions to practice exercises",
    "section": "",
    "text": "Define a function called add_two_numbers() that takes two arguments, adds them together, and returns the result.\n\nadd_two_numbers &lt;- function(num_1, num_2) {\n  num_1 + num_2\n}\n\nsum_3_4 &lt;- add_two_numbers(3, 4)\nsum_3_4\n\n[1] 7\n\nsum_5_5 &lt;- add_two_numbers(5, 5)\nsum_5_5\n\n[1] 10\n\n\nDefine a function called square_number() that takes a single argument and returns its square.\n\nsquare_number &lt;- function(num) {\n  num**2\n}\n\nsquare_3 &lt;- square_number(3)\nsquare_3\n\n[1] 9\n\nsquare_5 &lt;- square_number(5)\nsquare_5\n\n[1] 25\n\n\nDefine a function called is_even() that takes one argument, checks if the number is even or odd, and returns TRUE for even numbers and FALSE for odd numbers. HINT use the modulo operator %%.\n\nis_even &lt;- function(x) {\n  x %% 2 == 0\n}\n\neven_num_3 &lt;- is_even(3)\neven_num_3\n\n[1] FALSE\n\neven_num_4 &lt;- is_even(4)\neven_num_4\n\n[1] TRUE"
  },
  {
    "objectID": "content/week-10/topic-4.html",
    "href": "content/week-10/topic-4.html",
    "title": "GWAS Resources",
    "section": "",
    "text": "A number of software packages have been developed for conducting genome-wide association studies. A few of the more common packages are:\n\nPLINK: General-purpose command line program available on Linux, Windows, and MacOS. Contains routines for opening, filtering, and converting genomic data in a number of formats. Runs linear and logistic association tests for unrelated participants. Also has a version 2 under development. This is the program that is taught in most GWAS courses.\nGCTA: Software package primarily for estimating heritabilty using genomic data, it can also perform GWAS while correcting for relatedness among participants. Contians a number of other features for downstream analysis, such as conditional analysis and gene-based tests. Uses a linear approximation for case-control GWAS.\nLDAK: Similar to GCTA but can fit models for different genetic architectures. \nBOLT-LMM: Similar to GCTA but uses a fast approximation that speeds up computation considerably. Appropriate when analysing very large samples. Uses a linear approximation for case-control GWAS.\nregenie: Fast software when GWASing multiple traits, possibly with related participants. Applicable to case-control GWAS.\n\nThere are several R packages that are useful for GWAS:\n\nbigsnpr: Managing and performing quality control on SNP data\nBioconductor: repository of bioinformatics packages\nieugwasr: query the OpenGWAS database\n\nWeb applications:\n\nFUMA: functional annotation and mapping of GWAS summary statistics\nLocusZoom: interactive region plots from GWAS summary statistics\n\nAdditional tools for different stages of a GWAS analysis are listed in Table 1 of the Uffelmann primer.\nThere is also a big list of software used for genetic analysis maintained on GitHub as the Rockefeller List: https://gaow.github.io/genetic-analysis-software/0/",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Topic 4:GWAS Resources"
    ]
  },
  {
    "objectID": "content/week-10/topic-4.html#software",
    "href": "content/week-10/topic-4.html#software",
    "title": "GWAS Resources",
    "section": "",
    "text": "A number of software packages have been developed for conducting genome-wide association studies. A few of the more common packages are:\n\nPLINK: General-purpose command line program available on Linux, Windows, and MacOS. Contains routines for opening, filtering, and converting genomic data in a number of formats. Runs linear and logistic association tests for unrelated participants. Also has a version 2 under development. This is the program that is taught in most GWAS courses.\nGCTA: Software package primarily for estimating heritabilty using genomic data, it can also perform GWAS while correcting for relatedness among participants. Contians a number of other features for downstream analysis, such as conditional analysis and gene-based tests. Uses a linear approximation for case-control GWAS.\nLDAK: Similar to GCTA but can fit models for different genetic architectures. \nBOLT-LMM: Similar to GCTA but uses a fast approximation that speeds up computation considerably. Appropriate when analysing very large samples. Uses a linear approximation for case-control GWAS.\nregenie: Fast software when GWASing multiple traits, possibly with related participants. Applicable to case-control GWAS.\n\nThere are several R packages that are useful for GWAS:\n\nbigsnpr: Managing and performing quality control on SNP data\nBioconductor: repository of bioinformatics packages\nieugwasr: query the OpenGWAS database\n\nWeb applications:\n\nFUMA: functional annotation and mapping of GWAS summary statistics\nLocusZoom: interactive region plots from GWAS summary statistics\n\nAdditional tools for different stages of a GWAS analysis are listed in Table 1 of the Uffelmann primer.\nThere is also a big list of software used for genetic analysis maintained on GitHub as the Rockefeller List: https://gaow.github.io/genetic-analysis-software/0/",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Topic 4:GWAS Resources"
    ]
  },
  {
    "objectID": "content/week-10/topic-4.html#summary-statistics",
    "href": "content/week-10/topic-4.html#summary-statistics",
    "title": "GWAS Resources",
    "section": "Summary statistics",
    "text": "Summary statistics\n\nSources\nSummary statistics are the output of SNP association effect sizes and p-values from a GWAS. Summary statistics are available in full or in part from a number of websites:\n\nGWAS Catalog: Comprehensive catalog of phenotype-SNP associations. Usually only contains independent significant SNPs from each study.\nOpenGWAS: Database of full summary statistics for a large number of traits, available for querying and downloading in a standardised format.\nPsychiatric Genomics Consortium (PGC): Full summary statistics on psychiatric disorders. \nSocial Sciences Genetic Association Consortium (SSGAC): Full summary statistics on social and behavioural traits like education and personality.\nNealelab: Transancestry GWAS from UK Biobank.\nGIANT Consortium: GWAS of anthropomorphic phenotypes (weight and height).\nGlobal Lipids Genetics Consortium: GWAS of lipids.\n\n\n\nFile structure\nSummary statistics are typically provided as a plain-text, tabular data file (columns are usually space or tab delimited). There is no single standard format (for example, see daner, GWAS-VCF, and GWAS-SSF), but most files will look something like the following:\n\n\nFile structure\nSummary statistics are typically provided as a plain-text, tabular data file (columns are usually space or tab delimited). There is no single standard format (for example, see daner, GWAS-VCF, and GWAS-SSF), but most files will look something like the following:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCHR\nSNP\nPOS\nREF\nALT\nAF\nBETA\nSE\nP\n\n\n1\nrs189107123\n10611\nC\nG\n0.985 \n-0.128\n0.0761\n0.0926\n\n\n1\nrs180734498\n13302\nT  \nC  \n0.118 \n0.0389 \n0.0278\n0.162\n\n\n1\nrs144762171\n13327\nC  \nG  \n0.0339 \n0.0417\n 0.049 \n0.394\n\n\n…\n \n \n \n \n \n \n \n \n\n\n\n\nCHR: the chromosome number of symbol. Usually specified as a number (1, 2, 3, …) for GCRh37 or prepended with ‘chr’ for GCRh38 (chr1, chr2, chr3, …)\nSNP: Reference SNP number (RSID) for the genetic marker, or some other unique name. Sometimes listed just using the chromsome and base pair position (CPID) identifier with out without the alleles (e.g., SNP rs189107123 might be called 1:10611 or 1:10611_C_G in some datasets)\nPOS: basepair position in genomic coordinates of the reference genome build. This column is sometimes also labelled as BP.\nREF: reference allele. Usually, but not always, the effect allele for the association. This column is also sometimes labelled A1, allele1, or alleleB\nALT: alternative allele. Usually, but not always, the non-effect allele. This column might also be called A2 or allele0.\nAF: frequency of the reference allele. May be multiple columns if allele frequencies in cases and controls are reported seperately.\nBETA: effect size of the association (substitution effect of each additional copy of the effect allele). For case/control and binary phenotypes, usually the odds ratio (OR) is reported instead.\nSE: standard error of the effect size. When the effect size is an odds ratio, this column usually represents the standard error of log(OR)\nP: p-value of the association\n\nDepending on the software and study design, additional columns might be present or missing.\nThe three most important points to check when processing GWAS summary statistics are:\n\nwhat genome build the summary statistics used? RSIDs and basepair coordinates differ between genome builds, so this can influence merging and look up with other tools.\nwhich allele is the effect allele? There is inconsistency in whether the REF or ALT allele is the effect allele, and even how these columns are labelled. (“Let’s call it the effect allele“)\nwhich strand do the alleles refer to? While the positive strand is usually assumed, some datasets might mix in alleles coded from the reverse complementary strand of DNA (e.g., the alleles for rs180734498 might be reported as REF=A, ALT=G). This causes problems when the two alleles are complementary (C and G, A and T).",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Topic 4:GWAS Resources"
    ]
  },
  {
    "objectID": "content/week-10/topic-4.html#genomic-databases",
    "href": "content/week-10/topic-4.html#genomic-databases",
    "title": "GWAS Resources",
    "section": "Genomic databases",
    "text": "Genomic databases\nThe National Center for Biotechnology Information (NCBI), run by NIH, has a number of specialised, cross-referenced databases for querying genomic information:\n\ndbSNP: database of SNP variants with information on population frequency and genomic context.\nGene: cross species information on genes.\nClinVar: Genetic variation with clinical relevance to human health\n\nOther useful resources include\n\nOMIM: Online Catalog of Human Genes and Genetic Disorders\ngenomeAD: Aggregation database of exome and genome sequencing datasets.",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Topic 4:GWAS Resources"
    ]
  },
  {
    "objectID": "content/week-10/topic-4.html#suggested-read-gwas-paper",
    "href": "content/week-10/topic-4.html#suggested-read-gwas-paper",
    "title": "GWAS Resources",
    "section": "Suggested Read: GWAS Paper",
    "text": "Suggested Read: GWAS Paper\nLink",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Topic 4:GWAS Resources"
    ]
  },
  {
    "objectID": "content/week-10/index.html",
    "href": "content/week-10/index.html",
    "title": "Overview",
    "section": "",
    "text": "For the final week of the course, we will look at an application of data science within genomics. Namely we will look towards gaining an understanding of the topic of genome wide association studies (GWAS).",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Overview"
    ]
  },
  {
    "objectID": "content/week-10/index.html#learning-outcomes",
    "href": "content/week-10/index.html#learning-outcomes",
    "title": "Overview",
    "section": "Learning outcomes",
    "text": "Learning outcomes\n\nUnderstand how genomic data is collected and processed\nLearn what methods are used to analyse genomic data\nConduct a small portion of a genome-wide association study (GWAS)\nUnderstand the format of GWAS summary statistics and where to obtain them",
    "crumbs": [
      "Content",
      "Week 10: Working with Genomic Data",
      "Overview"
    ]
  },
  {
    "objectID": "content/week-3/topic-3.html",
    "href": "content/week-3/topic-3.html",
    "title": "Data Storytelling",
    "section": "",
    "text": "Stories are an effective way of presenting your ideas to other people. Stories have the power to inspire, influence, and change views. Thus, storytelling is one of the most efficient ways of communication. Data storytelling is the practice of building a narrative around data and associated visualisations to convey actionable insights in a commanding and engaging fashion. Data storytelling differs from data visualisation because it requires communicators to consider and provide a broad and holistic view of the idea you are trying to pitch. Storytelling with data first takes the audience, and narrative structure into consideration before any visuals are created. Therefore, before you start the storytelling process, you need to identify:\n\nWho is the audience?\nWhat does the audience care about?\nWhat level of technical detail will the audience expect or appreciate?\nWhat does the audience need to know about the data being presented?\nHow will the narrative be structured to generate the desired action?\nHow does the data being presented drive the decision-making process?\n\nWith these questions answered, you have got the road map in place to identify the technical level of your pitch and what information must be included in your narrative or left out entirely.\n\n\n\n\n\n\n\n\nKeep visuals simple. This draws actionable insights into focus.\nCreate a headline for each slide. This articulates your actionable insights and connects your slides in an easily recognisable pattern.\nUse photographs and icons. This will break up your text, making it more visually pleasing and memorable.\nDo not present superfluous data. Only present the data that directly supports your pitch.\n\nAs with any good story, a data story needs a beginning, a middle, and an end, and some actionable insights. A good data story leverages three key components: data, visuals, and narrative. As data scientists, the data component enables you to obtain and communicate actionable insights. The visual component enables you to identify and translate patterns and trends in data. Finally, the narrative component allows you to tell the data story. All combined, narrative, data and visuals generate data stories that inspire, influence, change views and drive data-driven innovation.\n\n\n\nWhen creating a plot or data visualisation which you will use in data storytelling, there are a few specific type of questions you need to ask which can guide your reader to understand the story you are trying to tell with the data and your visualisation. \n\nWhat is your story? What is the point? This determines your headline or title.\nNote: when creating a data visualisation not intended for data storytelling, the title will be more objective and lacking any interpretation, perhaps explaining what variables within the dataset are shown. \nHow can you emphasise your point in your data visualisation? Answering this question determines your decisions around plot type, specific variables used, colors, highlights, annotations, etc. \nWhat does the final plot show exactly? This is where you add things like a description, any legend or keys, and data sources. \n\nYou can read more about this approach on Lisa Charlotte Muth’s post “What Questions to Ask When Creating Charts: The Attempt of a Data Vis Worflow” \n…..",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 3: Data Storytelling"
    ]
  },
  {
    "objectID": "content/week-3/topic-3.html#what-is-storytelling-with-data-and-why-is-it-important",
    "href": "content/week-3/topic-3.html#what-is-storytelling-with-data-and-why-is-it-important",
    "title": "Data Storytelling",
    "section": "",
    "text": "Stories are an effective way of presenting your ideas to other people. Stories have the power to inspire, influence, and change views. Thus, storytelling is one of the most efficient ways of communication. Data storytelling is the practice of building a narrative around data and associated visualisations to convey actionable insights in a commanding and engaging fashion. Data storytelling differs from data visualisation because it requires communicators to consider and provide a broad and holistic view of the idea you are trying to pitch. Storytelling with data first takes the audience, and narrative structure into consideration before any visuals are created. Therefore, before you start the storytelling process, you need to identify:\n\nWho is the audience?\nWhat does the audience care about?\nWhat level of technical detail will the audience expect or appreciate?\nWhat does the audience need to know about the data being presented?\nHow will the narrative be structured to generate the desired action?\nHow does the data being presented drive the decision-making process?\n\nWith these questions answered, you have got the road map in place to identify the technical level of your pitch and what information must be included in your narrative or left out entirely.\n\n\n\n\n\n\n\n\nKeep visuals simple. This draws actionable insights into focus.\nCreate a headline for each slide. This articulates your actionable insights and connects your slides in an easily recognisable pattern.\nUse photographs and icons. This will break up your text, making it more visually pleasing and memorable.\nDo not present superfluous data. Only present the data that directly supports your pitch.\n\nAs with any good story, a data story needs a beginning, a middle, and an end, and some actionable insights. A good data story leverages three key components: data, visuals, and narrative. As data scientists, the data component enables you to obtain and communicate actionable insights. The visual component enables you to identify and translate patterns and trends in data. Finally, the narrative component allows you to tell the data story. All combined, narrative, data and visuals generate data stories that inspire, influence, change views and drive data-driven innovation.\n\n\n\nWhen creating a plot or data visualisation which you will use in data storytelling, there are a few specific type of questions you need to ask which can guide your reader to understand the story you are trying to tell with the data and your visualisation. \n\nWhat is your story? What is the point? This determines your headline or title.\nNote: when creating a data visualisation not intended for data storytelling, the title will be more objective and lacking any interpretation, perhaps explaining what variables within the dataset are shown. \nHow can you emphasise your point in your data visualisation? Answering this question determines your decisions around plot type, specific variables used, colors, highlights, annotations, etc. \nWhat does the final plot show exactly? This is where you add things like a description, any legend or keys, and data sources. \n\nYou can read more about this approach on Lisa Charlotte Muth’s post “What Questions to Ask When Creating Charts: The Attempt of a Data Vis Worflow” \n…..",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 3: Data Storytelling"
    ]
  },
  {
    "objectID": "content/week-3/topic-3.html#examples-of-good-data-storytelling-in-health-and-social-care",
    "href": "content/week-3/topic-3.html#examples-of-good-data-storytelling-in-health-and-social-care",
    "title": "Data Storytelling",
    "section": "Examples of good data storytelling in health and social care",
    "text": "Examples of good data storytelling in health and social care\n\n\nOxfam Ireland\nOxfam Ireland is part of a global movement for change, one that empowers people to create a future that is secure, just and free from poverty. Oxfam Ireland is a not-for-profit organisation (NGO) with charitable status and is a public benefit entity. Oxfam Ireland not only collects data, but uses it to tell a story of both urgent need and impressive impact. In their annual impact report for stakeholders and the public, the team at Oxfam showcase their impact using data-driven maps and infographics, presented in an sophisticated and engaging way:\n\n\nRadio NZ\nRadio NZ is New Zealand’s public broadcaster providing comprehensive news and current affairs, specialist audio features and documentaries. One such story came from a team in Radio NZ who published and continued to update a timeline on the impact of COVID-19 in Aotearoa New Zealand. Rather than use scroll-based animation, the team embedded rich visualisations from data visualisation platform Tableau resulting in a spectacular, well-told story, interspersed with striking interactive charts and maps",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 3: Data Storytelling"
    ]
  },
  {
    "objectID": "content/week-3/topic-3.html#data-storytelling-with-r",
    "href": "content/week-3/topic-3.html#data-storytelling-with-r",
    "title": "Data Storytelling",
    "section": "Data storytelling with R",
    "text": "Data storytelling with R\nIn the video below, Olga Pierce, a reporter specialising in data-driven stories, will show you how to turn a data finding into a data story with R. Olga outlines 3 case studies in her talk\n\nSurgeon Scorecard\nColor of Debt\nMachine Bias\n\n\nHere is a Link to Olga Pierce storytelling with R. While here is a Link to the script.\n\nOlgaPierce_Storytelling_with_R.pptx\nStorytelling with R Script – Olga Pierce.docx",
    "crumbs": [
      "Content",
      "Week 3: Data Visualisation and Storytelling",
      "Topic 3: Data Storytelling"
    ]
  },
  {
    "objectID": "content/week-4/topic-3.html",
    "href": "content/week-4/topic-3.html",
    "title": "Roundup and recap",
    "section": "",
    "text": "At the beginning of this course we introduced R by highlighting one of its main attractions: reproducibility. And here we are at the end of the course still banging on about reproducibility, but that’s because it is so important.\nThere is a growing awareness of the importance of reproducibility in data processing and analysis, and thankfully the tools needed to generate reproducible outputs are more accessible than ever before, helping to make this easier to accomplish.\nThere are many ways in which you can ensure your work is reproducible. The first we would recommend, is to start using R, and the second is to become familiar with R Markdown, so you are well on your way!  \nHere is a reminder of some of the main points to bear in mind, to ensure your future self and others can easily and efficiently build on the analysis you have carried out in R:\n\nalways create an RStudio Project to work in\nuse raw data directly in R (no pre-editing with other software)\ninclude informative commenting\nensure code is easy to read with appropriate indentations and spacing\npipe  %&gt;%  with the Tidyverse\ngive your objects meaningful names\nwork in R Markdown for reporting\n\nLet’s look at these aspects in little more detail.\n\n\n\n\n\nBy now you should be familiar with the how and the why of creating Projects in RStudio as this has cropped up frequently throughout the course. Projects help to keep everything in one place, they help R to find all your files and folders easily, and they generally make your life a lot easier so you should definitely be using them. \n\n\n\n\n\nAlways use the raw data. If your data has come from a database or from a publicly updated data source, avoid the temptation of doing some quick and dirty data wrangling in Excel before importing into R. This would make your analysis less reproducible as you may not always remember exactly what steps you took during this initial phase and these steps would then need to be taken each time you wanted to repeat or carry out a similar operation. \n\n\n\n\n\nCode which is easy to decipher and understand can be revisited in the future and rerun more easily, but how many times have you looked back at scribbled notes you took in haste, or code you typed quickly, with no recollection of what it all means? By including informative commenting you can help your future self, or anyone else who might be looking at your code, understand what it is you were trying to do at each point in your script. It isn’t needed for all lines of code but should be used when there might be ambiguity, or to provide clarification, or remind yourself about something specific to do with what’s happening. The keyboard shortcut for commenting/uncommenting a line is Ctrl+Shift+C, or you can manually type  # .\n\n\n\n\n\nOn a similar theme, it is important to make your code easy to read by applying the appropriate indentations and spacing. The keyboard shortcut for quickly reformatting your code is Ctrl+I (first select your code, or use Ctrl+A to select all before doing Ctrl+I). If your script is long this is particularly important as it means you, or someone else, can scan the document more easily to find the relevant text or information. \n\n\n\n\n\nThe Tidyverse has featured heavily in this course and is another great way to make your code easier to read and understand due to the use of the  %&gt;%  operator. As we learned at the beginning of the course, the  %&gt;% reduces the need for nested functions, where each function is “nested” inside another making it potentially very difficult to debug. Instead the piped (  %&gt;% ) functions appear almost like a series of statements or instructions, which can be run line by line if needed. At the start of every R script or document, remember to include  library(tidyverse) .\n\n\n\n\n\nTo finish, just a final note about assigning your objects meaningful names. This is easily overlooked and can be surprisingly difficult but when given careful consideration, good object naming can make your life a lot easier, and again, make the code easier to understand and read. Avoid generic names such as data or plot as these can cause problems. This is particularly true if you are carrying out multiple analyses and have referred to “data” or “plot” somewhere else and not cleared your environment (Restart R often!). Also, when choosing names, no spaces or funky characters, stick to lowercase with underscores. Then be consistent, and you should be safe! \n\n\n\n\n\nR Markdown is one of the keys to producing reproducible analysis in R as it enables the code and the narrative to be easily combined, whilst providing great flexibility as to how the output might be displayed. It is certainly worth the effort becoming familiar with R Markdown which was covered in detail in Week 6: Topics 1 & 2. Creating a new R Markdown document is as simple as going to  File - New File - R Markdown - OK  then delete everything up to the first code chunk (it is useful to leave the first setup chunk) and you are ready to go!\n\n\nR Markdown is not only a powerful tool for ensuring reproducibility, but as you have already discovered, it includes many features which make it easy to create and share clear and professional looking documents for a wide range of audiences.\nHere are some things to remember when planning and designing a report with R Markdown:\n\nprovide a clear structure using Markdown section headers (#, ##, ###)\nbe conscious of the YAML, unlike R code, the YAML is very sensitive of extra spaces/formatting\nconsider the code output options depending on audience (`echo = TRUE` vs `echo = FALSE`)\nthink about the story you are trying to tell\nmake good use of figures\nuse tables appropriately\n\n\n\n\n\n\nIt is important to provide a clear structure in any report or document you are writing and R Markdown makes it easy to manage and display the structure through the use of headers. The hash symbol ( # ) is used in the Markdown sections (all the bits which aren’t code chunks or YAML) to indicate different levels of header, so how big or small the text should be. These headers also show up in the document outline which is extremely useful as a means of navigating your document when editing, helping to save you time.\n\n# global chunk options`\n\nknitr::opts_chunk$set(echo = TRUE)\n# load packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(here)\n\nhere() starts at C:/Users/karim/Documents/Github/website-ug-data-science\n\n\nThe structure of your R code chunks is important too. For example, it is good practice to have a code chunk near the top of your document (often in the setup chunk) where you keep all the packages that you are using together. It might look something like the image on the right.  The next code chunk is often where you might choose to load your data and is likely to include the code similar to:  mydata &lt;- read_csv(here(“data”, “appointments.csv”))\n\n\n\n\n\nWe’ve already mentioned that Markdown headers ( # ) make it easy to define a clear structure in your R Markdown document. Explore some of the other Markdown features, such as **bold** and *italic* or `inline R code`, which can help to give your final document a high quality finish. Remember, you can access the Markdown Quick Reference guide from the Help menu.\nIn its most basic form, the YAML which appears at the top of your R Markdown document, could simply tell R Markdown what output format you would like your document to be in, in which case it would look like this, indicating HTML output:\n---\noutput: html_document\n---\n\n\n\n\n\nHowever, you have the potential to add many attractive features via the YAML, such as table of contents, section numbering, themes, and much more. You have to be particularly careful about indentation in the YAML (unlike in R code chunks where indentation is mainly for readability) but you can start exploring YAML options easily through changing settings in the document cog and this will automatically update your YAML.\nDepending on who your intended audience is, you have a number of output options available to you. You can choose whether or not to show or run your code in the final document. Showing you code in the Markdown output document (HTML, Word, PDF) is denoted `echo = TRUE`, not showing it is denoted `echo = FALSE`. You also have control over whether to display the output of each code chunk or not too. These options can be accessed from the individual code chunk cogs or can be set for the whole document in the setup chunk.\n\n\n\n\n\nWhen creating a report or document, think about what story you are trying to tell with the data; you might be answering a specific question or perhaps highlighting a change in activity over a period of time. R Markdown makes it easy to intersperse clear and informative visual elements throughout the narrative. These could be in the form of figures and plots, nicely formatted tables or even images (e.g. JPG or PNG).\nWhen writing the code to produce a plot, remember you don’t need to assign it to an object. For example, running  myplot &lt;- mydata %&gt;% ggplot()  will result in no plot appearing in your final document, it is saved in the Environment. Instead you can simply write  mydata %&gt;% ggplot() which will ensure your plot does appear. You can also control the size of your plots and figures in the code chunk settings cog.\nWhen writing the code for a table which you want to include in your final document, remember that the function  kable()  in the knitr package helps provide consistent table formatting across various different outputs. Also note that the same principles apply concerning whether or not your table output will appear as those referred to in plotting; no need to save the table first, you can simply write  mydata %&gt;% kable() .\nAnd finally, as addictive as it is creating beautiful plots with ggplot2, try not to bombard the reader with too many tables and figures. Choose wisely the information you would like to display in order to tell your story concisely and make good use of ggplot2’s ability to provide a breadth of information in one plot, through careful use of aesthetic mappings. \n\n\n\nIn this course we have covered a whole variety of functions which you will find useful when carrying out data wrangling, plotting and analysis in R. There are however some functions which you will find yourself using more frequently than others and in the following sections we provide a quick recap on some of these more common functions.\nThe examples below are intended as a quick reference guide to remind yourself which functions to use in which settings. They can be copied and pasted straight into an R Markdown document as code chunk to test, but if you are copying multiple code chunks, you only need to include the packages once.\nNote: The examples below all use the gapminder dataset which is loaded by running the command  library(gapminder) . In your own projects you would load your data with the command read_csv(“mydata.csv”)  or similar, depending on the format and location of your data.\n\n\ndistinct() returns only distinct (unique) rows in a data frame\nIn the example below we have included the variable  continent  so that we can explore how many unique categories there are in this column.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\nWarning: package 'gapminder' was built under R version 4.4.3\n\ngapminder %&gt;%\n  distinct(continent)\n\n# A tibble: 5 × 1\n  continent\n  &lt;fct&gt;    \n1 Asia     \n2 Europe   \n3 Africa   \n4 Americas \n5 Oceania  \n\n\nUseful for:\n\nexploring categories\nspotting spelling mistakes\nlooking at date ranges\nremoving duplicate rows\nTry this out: \n\ngapminder %&gt;%\ncount(contintent)\n\n\n\nfilter()  returns a subset of rows based on a condition\nIn the example below we have specified that we only want to see results where  country  is equal to  Ghana .\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  filter(country == \"Ghana\")\n\n# A tibble: 12 × 6\n   country continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Ghana   Africa     1952    43.1  5581001      911.\n 2 Ghana   Africa     1957    44.8  6391288     1044.\n 3 Ghana   Africa     1962    46.5  7355248     1190.\n 4 Ghana   Africa     1967    48.1  8490213     1126.\n 5 Ghana   Africa     1972    49.9  9354120     1178.\n 6 Ghana   Africa     1977    51.8 10538093      993.\n 7 Ghana   Africa     1982    53.7 11400338      876.\n 8 Ghana   Africa     1987    55.7 14168101      847.\n 9 Ghana   Africa     1992    57.5 16278738      925.\n10 Ghana   Africa     1997    58.6 18418288     1005.\n11 Ghana   Africa     2002    58.5 20550751     1112.\n12 Ghana   Africa     2007    60.0 22873338     1328.\n\n\nUseful for:\n\nfiltering your data to remove unwanted rows\nfiltering your data as part of the exploratory phase\nfiltering your data prior to piping into  ggplot() \n\nRemember:\n\nthe “equal to” comparison operator needs double  == \nto use inverted commas (““) for non-numbers \n\n\n\n\nselect() lets you choose which columns to select\nIn the example below we have chosen to select only 4 variables from our dataset and we have also changed the order in which they appear.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  select(country, year, pop, lifeExp)\n\n# A tibble: 1,704 × 4\n   country      year      pop lifeExp\n   &lt;fct&gt;       &lt;int&gt;    &lt;int&gt;   &lt;dbl&gt;\n 1 Afghanistan  1952  8425333    28.8\n 2 Afghanistan  1957  9240934    30.3\n 3 Afghanistan  1962 10267083    32.0\n 4 Afghanistan  1967 11537966    34.0\n 5 Afghanistan  1972 13079460    36.1\n 6 Afghanistan  1977 14880372    38.4\n 7 Afghanistan  1982 12881816    39.9\n 8 Afghanistan  1987 13867957    40.8\n 9 Afghanistan  1992 16317921    41.7\n10 Afghanistan  1997 22227415    41.8\n# ℹ 1,694 more rows\n\n\nUseful for:\n\nremoving unwanted columns (e.g. columns with NAs, or things you’re just not going to use)\nfocusing on selected columns for further exploration\nre-ordering columns for ease of viewing\nselect() can also be used to rename columns, e.g.\n\ngapminder %&gt;% \n  select(country, year, population = pop, life_expectancy = lifeExp)\n\nAnd select() has a sister function called rename(), that can be used to rename columns without removing the ones you don’t list, e.g.:\n\ngapminder %&gt;%\n  rename(population = pop, life_expectancy = lifeExp)\n\n\n\nmutate() allows you to add a column or update an existing column\nIn the example below we have created a new column called  pop_millions  so that we can display the population as a decimal number of millions.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  mutate(pop_millions = pop/1000000)\n\n# A tibble: 1,704 × 7\n   country     continent  year lifeExp      pop gdpPercap pop_millions\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.         8.43\n 2 Afghanistan Asia       1957    30.3  9240934      821.         9.24\n 3 Afghanistan Asia       1962    32.0 10267083      853.        10.3 \n 4 Afghanistan Asia       1967    34.0 11537966      836.        11.5 \n 5 Afghanistan Asia       1972    36.1 13079460      740.        13.1 \n 6 Afghanistan Asia       1977    38.4 14880372      786.        14.9 \n 7 Afghanistan Asia       1982    39.9 12881816      978.        12.9 \n 8 Afghanistan Asia       1987    40.8 13867957      852.        13.9 \n 9 Afghanistan Asia       1992    41.7 16317921      649.        16.3 \n10 Afghanistan Asia       1997    41.8 22227415      635.        22.2 \n# ℹ 1,694 more rows\n\n\nUseful for:\n\nadding an extra column containing a calculation\ncombining with group_by()  for grouped calculations\nupdating a column which has mistakes\nconverting a column to a different data type\n\nRemember:\n\nyour new column is always added at the end of your tibble (data frame)\nthe value to the left of the =  is the new name for you column and could be called anything\nif you want to replace a column, the value to the left of the =  must be exactly the same name as the column to replace\n\n\n\n\nsummarise() returns a new tibble (data frame) containing summary statistics based on what has been specified in the function.\nIn the example below the summarise function has calculated the average life expectancy for the whole gapminder dataset by averaging all observations (or rows). On its own,  summarise()  is not so useful, but when combined with group_by()  its use becomes apparent, see later examples.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  summarise(avg_life_exp = mean(lifeExp))\n\n# A tibble: 1 × 1\n  avg_life_exp\n         &lt;dbl&gt;\n1         59.5\n\n\nUseful for:\n\ncombining with group_by()  to perform grouped calculations\n\n\n\n\ngroup_by() lets you choose how you want the variables in your dataset to be grouped so that you can perform operations (such as sum() or mean()) on these groups.\nIn the example below we have grouped our data by the variable year. Notice that the output to this chunk of code looks no different to how it would look if we just ran  gapminderwithout any grouping. This is because the effects of group_by()  are only noticeable when we start performing operations.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  group_by(year)\n\n# A tibble: 1,704 × 6\n# Groups:   year [12]\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nIn this next example we have combined group_by() with the summarise() command and now, instead of seeing an average life expectancy for the whole tibble (as in the example above), we see that life expectancy has been averaged within each year group.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\ngroup_by(year) %&gt;%\n  summarise(avg_life_exp = mean(lifeExp))\n\n# A tibble: 12 × 2\n    year avg_life_exp\n   &lt;int&gt;        &lt;dbl&gt;\n 1  1952         49.1\n 2  1957         51.5\n 3  1962         53.6\n 4  1967         55.7\n 5  1972         57.6\n 6  1977         59.6\n 7  1982         61.5\n 8  1987         63.2\n 9  1992         64.2\n10  1997         65.0\n11  2002         65.7\n12  2007         67.0\n\n\nUseful for:\n\ncombining with summarise()or mutate() for grouped calculations\npreparing summary statistics for plotting\nexploring values within groups in your data\n\nRemember:\n\nto use  ungroup()  if carrying out further data manipulations to avoid unexpected results, as by default, mutate() and summarise() silently retain the groupings.\nthe value to the left of the  =  is the new name for your column and could be called anything\n\n\n\n\nleft_join()  allows you to combine two datasets\nIn the example below we are joining the gapminder tibble to another tibble we can access directly from the gapminder package, the  country_codes data frame, in order to add the ISO country codes.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  left_join(country_codes)\n\nJoining with `by = join_by(country)`\n\n\n# A tibble: 1,704 × 8\n   country     continent  year lifeExp      pop gdpPercap iso_alpha iso_num\n   &lt;chr&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;int&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779. AFG             4\n 2 Afghanistan Asia       1957    30.3  9240934      821. AFG             4\n 3 Afghanistan Asia       1962    32.0 10267083      853. AFG             4\n 4 Afghanistan Asia       1967    34.0 11537966      836. AFG             4\n 5 Afghanistan Asia       1972    36.1 13079460      740. AFG             4\n 6 Afghanistan Asia       1977    38.4 14880372      786. AFG             4\n 7 Afghanistan Asia       1982    39.9 12881816      978. AFG             4\n 8 Afghanistan Asia       1987    40.8 13867957      852. AFG             4\n 9 Afghanistan Asia       1992    41.7 16317921      649. AFG             4\n10 Afghanistan Asia       1997    41.8 22227415      635. AFG             4\n# ℹ 1,694 more rows\n\n\nUseful for:\n\ncombining two sets of data which share an id (e.g. patient id, appointment id)\nadding in information from a lookup table (e.g. country codes, Health Board names)\n\nRemember:\n\nto watch out what happens to missing values\nto keep an eye on the number of observations in each tibble before and after joining\nto check which columns are being joined by\n\nOther functions for wrangling data and what they might be useful for:\n\n arrange()  for sorting the values within a column\n desc() for indicating descending sort order when used within arrange()\n str_replace()  for updating spelling mistakes in a column\n paste()  for joining things together and adding new text \n ymd()  for reading in dates in strange formats\n factor()  for converting a variable to factor data type\n fct_relevel()  for changing the order of factors\n pivot_longer()  for reshaping your data if you have values in column headings\n pivot_wider()  for reshaping your data if you have multiple variables in one column",
    "crumbs": [
      "Content",
      "Week 4: Analysing and Presenting Data in R",
      "Topic 3: Roundup and recap"
    ]
  },
  {
    "objectID": "content/week-4/topic-3.html#reproducibility-in-r",
    "href": "content/week-4/topic-3.html#reproducibility-in-r",
    "title": "Roundup and recap",
    "section": "",
    "text": "At the beginning of this course we introduced R by highlighting one of its main attractions: reproducibility. And here we are at the end of the course still banging on about reproducibility, but that’s because it is so important.\nThere is a growing awareness of the importance of reproducibility in data processing and analysis, and thankfully the tools needed to generate reproducible outputs are more accessible than ever before, helping to make this easier to accomplish.\nThere are many ways in which you can ensure your work is reproducible. The first we would recommend, is to start using R, and the second is to become familiar with R Markdown, so you are well on your way!  \nHere is a reminder of some of the main points to bear in mind, to ensure your future self and others can easily and efficiently build on the analysis you have carried out in R:\n\nalways create an RStudio Project to work in\nuse raw data directly in R (no pre-editing with other software)\ninclude informative commenting\nensure code is easy to read with appropriate indentations and spacing\npipe  %&gt;%  with the Tidyverse\ngive your objects meaningful names\nwork in R Markdown for reporting\n\nLet’s look at these aspects in little more detail.\n\n\n\n\n\nBy now you should be familiar with the how and the why of creating Projects in RStudio as this has cropped up frequently throughout the course. Projects help to keep everything in one place, they help R to find all your files and folders easily, and they generally make your life a lot easier so you should definitely be using them. \n\n\n\n\n\nAlways use the raw data. If your data has come from a database or from a publicly updated data source, avoid the temptation of doing some quick and dirty data wrangling in Excel before importing into R. This would make your analysis less reproducible as you may not always remember exactly what steps you took during this initial phase and these steps would then need to be taken each time you wanted to repeat or carry out a similar operation. \n\n\n\n\n\nCode which is easy to decipher and understand can be revisited in the future and rerun more easily, but how many times have you looked back at scribbled notes you took in haste, or code you typed quickly, with no recollection of what it all means? By including informative commenting you can help your future self, or anyone else who might be looking at your code, understand what it is you were trying to do at each point in your script. It isn’t needed for all lines of code but should be used when there might be ambiguity, or to provide clarification, or remind yourself about something specific to do with what’s happening. The keyboard shortcut for commenting/uncommenting a line is Ctrl+Shift+C, or you can manually type  # .\n\n\n\n\n\nOn a similar theme, it is important to make your code easy to read by applying the appropriate indentations and spacing. The keyboard shortcut for quickly reformatting your code is Ctrl+I (first select your code, or use Ctrl+A to select all before doing Ctrl+I). If your script is long this is particularly important as it means you, or someone else, can scan the document more easily to find the relevant text or information. \n\n\n\n\n\nThe Tidyverse has featured heavily in this course and is another great way to make your code easier to read and understand due to the use of the  %&gt;%  operator. As we learned at the beginning of the course, the  %&gt;% reduces the need for nested functions, where each function is “nested” inside another making it potentially very difficult to debug. Instead the piped (  %&gt;% ) functions appear almost like a series of statements or instructions, which can be run line by line if needed. At the start of every R script or document, remember to include  library(tidyverse) .\n\n\n\n\n\nTo finish, just a final note about assigning your objects meaningful names. This is easily overlooked and can be surprisingly difficult but when given careful consideration, good object naming can make your life a lot easier, and again, make the code easier to understand and read. Avoid generic names such as data or plot as these can cause problems. This is particularly true if you are carrying out multiple analyses and have referred to “data” or “plot” somewhere else and not cleared your environment (Restart R often!). Also, when choosing names, no spaces or funky characters, stick to lowercase with underscores. Then be consistent, and you should be safe! \n\n\n\n\n\nR Markdown is one of the keys to producing reproducible analysis in R as it enables the code and the narrative to be easily combined, whilst providing great flexibility as to how the output might be displayed. It is certainly worth the effort becoming familiar with R Markdown which was covered in detail in Week 6: Topics 1 & 2. Creating a new R Markdown document is as simple as going to  File - New File - R Markdown - OK  then delete everything up to the first code chunk (it is useful to leave the first setup chunk) and you are ready to go!\n\n\nR Markdown is not only a powerful tool for ensuring reproducibility, but as you have already discovered, it includes many features which make it easy to create and share clear and professional looking documents for a wide range of audiences.\nHere are some things to remember when planning and designing a report with R Markdown:\n\nprovide a clear structure using Markdown section headers (#, ##, ###)\nbe conscious of the YAML, unlike R code, the YAML is very sensitive of extra spaces/formatting\nconsider the code output options depending on audience (`echo = TRUE` vs `echo = FALSE`)\nthink about the story you are trying to tell\nmake good use of figures\nuse tables appropriately\n\n\n\n\n\n\nIt is important to provide a clear structure in any report or document you are writing and R Markdown makes it easy to manage and display the structure through the use of headers. The hash symbol ( # ) is used in the Markdown sections (all the bits which aren’t code chunks or YAML) to indicate different levels of header, so how big or small the text should be. These headers also show up in the document outline which is extremely useful as a means of navigating your document when editing, helping to save you time.\n\n# global chunk options`\n\nknitr::opts_chunk$set(echo = TRUE)\n# load packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(here)\n\nhere() starts at C:/Users/karim/Documents/Github/website-ug-data-science\n\n\nThe structure of your R code chunks is important too. For example, it is good practice to have a code chunk near the top of your document (often in the setup chunk) where you keep all the packages that you are using together. It might look something like the image on the right.  The next code chunk is often where you might choose to load your data and is likely to include the code similar to:  mydata &lt;- read_csv(here(“data”, “appointments.csv”))\n\n\n\n\n\nWe’ve already mentioned that Markdown headers ( # ) make it easy to define a clear structure in your R Markdown document. Explore some of the other Markdown features, such as **bold** and *italic* or `inline R code`, which can help to give your final document a high quality finish. Remember, you can access the Markdown Quick Reference guide from the Help menu.\nIn its most basic form, the YAML which appears at the top of your R Markdown document, could simply tell R Markdown what output format you would like your document to be in, in which case it would look like this, indicating HTML output:\n---\noutput: html_document\n---\n\n\n\n\n\nHowever, you have the potential to add many attractive features via the YAML, such as table of contents, section numbering, themes, and much more. You have to be particularly careful about indentation in the YAML (unlike in R code chunks where indentation is mainly for readability) but you can start exploring YAML options easily through changing settings in the document cog and this will automatically update your YAML.\nDepending on who your intended audience is, you have a number of output options available to you. You can choose whether or not to show or run your code in the final document. Showing you code in the Markdown output document (HTML, Word, PDF) is denoted `echo = TRUE`, not showing it is denoted `echo = FALSE`. You also have control over whether to display the output of each code chunk or not too. These options can be accessed from the individual code chunk cogs or can be set for the whole document in the setup chunk.\n\n\n\n\n\nWhen creating a report or document, think about what story you are trying to tell with the data; you might be answering a specific question or perhaps highlighting a change in activity over a period of time. R Markdown makes it easy to intersperse clear and informative visual elements throughout the narrative. These could be in the form of figures and plots, nicely formatted tables or even images (e.g. JPG or PNG).\nWhen writing the code to produce a plot, remember you don’t need to assign it to an object. For example, running  myplot &lt;- mydata %&gt;% ggplot()  will result in no plot appearing in your final document, it is saved in the Environment. Instead you can simply write  mydata %&gt;% ggplot() which will ensure your plot does appear. You can also control the size of your plots and figures in the code chunk settings cog.\nWhen writing the code for a table which you want to include in your final document, remember that the function  kable()  in the knitr package helps provide consistent table formatting across various different outputs. Also note that the same principles apply concerning whether or not your table output will appear as those referred to in plotting; no need to save the table first, you can simply write  mydata %&gt;% kable() .\nAnd finally, as addictive as it is creating beautiful plots with ggplot2, try not to bombard the reader with too many tables and figures. Choose wisely the information you would like to display in order to tell your story concisely and make good use of ggplot2’s ability to provide a breadth of information in one plot, through careful use of aesthetic mappings. \n\n\n\nIn this course we have covered a whole variety of functions which you will find useful when carrying out data wrangling, plotting and analysis in R. There are however some functions which you will find yourself using more frequently than others and in the following sections we provide a quick recap on some of these more common functions.\nThe examples below are intended as a quick reference guide to remind yourself which functions to use in which settings. They can be copied and pasted straight into an R Markdown document as code chunk to test, but if you are copying multiple code chunks, you only need to include the packages once.\nNote: The examples below all use the gapminder dataset which is loaded by running the command  library(gapminder) . In your own projects you would load your data with the command read_csv(“mydata.csv”)  or similar, depending on the format and location of your data.\n\n\ndistinct() returns only distinct (unique) rows in a data frame\nIn the example below we have included the variable  continent  so that we can explore how many unique categories there are in this column.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\nWarning: package 'gapminder' was built under R version 4.4.3\n\ngapminder %&gt;%\n  distinct(continent)\n\n# A tibble: 5 × 1\n  continent\n  &lt;fct&gt;    \n1 Asia     \n2 Europe   \n3 Africa   \n4 Americas \n5 Oceania  \n\n\nUseful for:\n\nexploring categories\nspotting spelling mistakes\nlooking at date ranges\nremoving duplicate rows\nTry this out: \n\ngapminder %&gt;%\ncount(contintent)\n\n\n\nfilter()  returns a subset of rows based on a condition\nIn the example below we have specified that we only want to see results where  country  is equal to  Ghana .\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  filter(country == \"Ghana\")\n\n# A tibble: 12 × 6\n   country continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;   &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Ghana   Africa     1952    43.1  5581001      911.\n 2 Ghana   Africa     1957    44.8  6391288     1044.\n 3 Ghana   Africa     1962    46.5  7355248     1190.\n 4 Ghana   Africa     1967    48.1  8490213     1126.\n 5 Ghana   Africa     1972    49.9  9354120     1178.\n 6 Ghana   Africa     1977    51.8 10538093      993.\n 7 Ghana   Africa     1982    53.7 11400338      876.\n 8 Ghana   Africa     1987    55.7 14168101      847.\n 9 Ghana   Africa     1992    57.5 16278738      925.\n10 Ghana   Africa     1997    58.6 18418288     1005.\n11 Ghana   Africa     2002    58.5 20550751     1112.\n12 Ghana   Africa     2007    60.0 22873338     1328.\n\n\nUseful for:\n\nfiltering your data to remove unwanted rows\nfiltering your data as part of the exploratory phase\nfiltering your data prior to piping into  ggplot() \n\nRemember:\n\nthe “equal to” comparison operator needs double  == \nto use inverted commas (““) for non-numbers \n\n\n\n\nselect() lets you choose which columns to select\nIn the example below we have chosen to select only 4 variables from our dataset and we have also changed the order in which they appear.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  select(country, year, pop, lifeExp)\n\n# A tibble: 1,704 × 4\n   country      year      pop lifeExp\n   &lt;fct&gt;       &lt;int&gt;    &lt;int&gt;   &lt;dbl&gt;\n 1 Afghanistan  1952  8425333    28.8\n 2 Afghanistan  1957  9240934    30.3\n 3 Afghanistan  1962 10267083    32.0\n 4 Afghanistan  1967 11537966    34.0\n 5 Afghanistan  1972 13079460    36.1\n 6 Afghanistan  1977 14880372    38.4\n 7 Afghanistan  1982 12881816    39.9\n 8 Afghanistan  1987 13867957    40.8\n 9 Afghanistan  1992 16317921    41.7\n10 Afghanistan  1997 22227415    41.8\n# ℹ 1,694 more rows\n\n\nUseful for:\n\nremoving unwanted columns (e.g. columns with NAs, or things you’re just not going to use)\nfocusing on selected columns for further exploration\nre-ordering columns for ease of viewing\nselect() can also be used to rename columns, e.g.\n\ngapminder %&gt;% \n  select(country, year, population = pop, life_expectancy = lifeExp)\n\nAnd select() has a sister function called rename(), that can be used to rename columns without removing the ones you don’t list, e.g.:\n\ngapminder %&gt;%\n  rename(population = pop, life_expectancy = lifeExp)\n\n\n\nmutate() allows you to add a column or update an existing column\nIn the example below we have created a new column called  pop_millions  so that we can display the population as a decimal number of millions.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  mutate(pop_millions = pop/1000000)\n\n# A tibble: 1,704 × 7\n   country     continent  year lifeExp      pop gdpPercap pop_millions\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.         8.43\n 2 Afghanistan Asia       1957    30.3  9240934      821.         9.24\n 3 Afghanistan Asia       1962    32.0 10267083      853.        10.3 \n 4 Afghanistan Asia       1967    34.0 11537966      836.        11.5 \n 5 Afghanistan Asia       1972    36.1 13079460      740.        13.1 \n 6 Afghanistan Asia       1977    38.4 14880372      786.        14.9 \n 7 Afghanistan Asia       1982    39.9 12881816      978.        12.9 \n 8 Afghanistan Asia       1987    40.8 13867957      852.        13.9 \n 9 Afghanistan Asia       1992    41.7 16317921      649.        16.3 \n10 Afghanistan Asia       1997    41.8 22227415      635.        22.2 \n# ℹ 1,694 more rows\n\n\nUseful for:\n\nadding an extra column containing a calculation\ncombining with group_by()  for grouped calculations\nupdating a column which has mistakes\nconverting a column to a different data type\n\nRemember:\n\nyour new column is always added at the end of your tibble (data frame)\nthe value to the left of the =  is the new name for you column and could be called anything\nif you want to replace a column, the value to the left of the =  must be exactly the same name as the column to replace\n\n\n\n\nsummarise() returns a new tibble (data frame) containing summary statistics based on what has been specified in the function.\nIn the example below the summarise function has calculated the average life expectancy for the whole gapminder dataset by averaging all observations (or rows). On its own,  summarise()  is not so useful, but when combined with group_by()  its use becomes apparent, see later examples.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  summarise(avg_life_exp = mean(lifeExp))\n\n# A tibble: 1 × 1\n  avg_life_exp\n         &lt;dbl&gt;\n1         59.5\n\n\nUseful for:\n\ncombining with group_by()  to perform grouped calculations\n\n\n\n\ngroup_by() lets you choose how you want the variables in your dataset to be grouped so that you can perform operations (such as sum() or mean()) on these groups.\nIn the example below we have grouped our data by the variable year. Notice that the output to this chunk of code looks no different to how it would look if we just ran  gapminderwithout any grouping. This is because the effects of group_by()  are only noticeable when we start performing operations.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  group_by(year)\n\n# A tibble: 1,704 × 6\n# Groups:   year [12]\n   country     continent  year lifeExp      pop gdpPercap\n   &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ℹ 1,694 more rows\n\n\nIn this next example we have combined group_by() with the summarise() command and now, instead of seeing an average life expectancy for the whole tibble (as in the example above), we see that life expectancy has been averaged within each year group.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\ngroup_by(year) %&gt;%\n  summarise(avg_life_exp = mean(lifeExp))\n\n# A tibble: 12 × 2\n    year avg_life_exp\n   &lt;int&gt;        &lt;dbl&gt;\n 1  1952         49.1\n 2  1957         51.5\n 3  1962         53.6\n 4  1967         55.7\n 5  1972         57.6\n 6  1977         59.6\n 7  1982         61.5\n 8  1987         63.2\n 9  1992         64.2\n10  1997         65.0\n11  2002         65.7\n12  2007         67.0\n\n\nUseful for:\n\ncombining with summarise()or mutate() for grouped calculations\npreparing summary statistics for plotting\nexploring values within groups in your data\n\nRemember:\n\nto use  ungroup()  if carrying out further data manipulations to avoid unexpected results, as by default, mutate() and summarise() silently retain the groupings.\nthe value to the left of the  =  is the new name for your column and could be called anything\n\n\n\n\nleft_join()  allows you to combine two datasets\nIn the example below we are joining the gapminder tibble to another tibble we can access directly from the gapminder package, the  country_codes data frame, in order to add the ISO country codes.\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  left_join(country_codes)\n\nJoining with `by = join_by(country)`\n\n\n# A tibble: 1,704 × 8\n   country     continent  year lifeExp      pop gdpPercap iso_alpha iso_num\n   &lt;chr&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;int&gt;\n 1 Afghanistan Asia       1952    28.8  8425333      779. AFG             4\n 2 Afghanistan Asia       1957    30.3  9240934      821. AFG             4\n 3 Afghanistan Asia       1962    32.0 10267083      853. AFG             4\n 4 Afghanistan Asia       1967    34.0 11537966      836. AFG             4\n 5 Afghanistan Asia       1972    36.1 13079460      740. AFG             4\n 6 Afghanistan Asia       1977    38.4 14880372      786. AFG             4\n 7 Afghanistan Asia       1982    39.9 12881816      978. AFG             4\n 8 Afghanistan Asia       1987    40.8 13867957      852. AFG             4\n 9 Afghanistan Asia       1992    41.7 16317921      649. AFG             4\n10 Afghanistan Asia       1997    41.8 22227415      635. AFG             4\n# ℹ 1,694 more rows\n\n\nUseful for:\n\ncombining two sets of data which share an id (e.g. patient id, appointment id)\nadding in information from a lookup table (e.g. country codes, Health Board names)\n\nRemember:\n\nto watch out what happens to missing values\nto keep an eye on the number of observations in each tibble before and after joining\nto check which columns are being joined by\n\nOther functions for wrangling data and what they might be useful for:\n\n arrange()  for sorting the values within a column\n desc() for indicating descending sort order when used within arrange()\n str_replace()  for updating spelling mistakes in a column\n paste()  for joining things together and adding new text \n ymd()  for reading in dates in strange formats\n factor()  for converting a variable to factor data type\n fct_relevel()  for changing the order of factors\n pivot_longer()  for reshaping your data if you have values in column headings\n pivot_wider()  for reshaping your data if you have multiple variables in one column",
    "crumbs": [
      "Content",
      "Week 4: Analysing and Presenting Data in R",
      "Topic 3: Roundup and recap"
    ]
  },
  {
    "objectID": "content/week-4/topic-3.html#plotting-and-tables-recap",
    "href": "content/week-4/topic-3.html#plotting-and-tables-recap",
    "title": "Roundup and recap",
    "section": "Plotting and tables recap",
    "text": "Plotting and tables recap\nIn the examples below we provide a recap on how to use ggplot2 to build up layers on a plot. Included are examples of argument options which you might want to experiment with in your own plots. I rarely remember any of the argument specifics and always fine-tune my plots by pressing `F1` on the various functions I’m using and exploring the documentation (F1, or fn+F1 on some computers opens up the Help tab). Then it’s a process of trial and error.  \nComments are included before each new line or layer to describe what is happening.\n\nThe blank canvas\n\nlibrary(tidyverse)\nlibrary(gapminder)\n     \n    # Data we are sending to ggplot()\ngapminder %&gt;%\n    # lets create a plot\n ggplot(aes(    # specify the axes\n      x = year,\n      y = lifeExp))\n\n\n\n\n\n\n\n\nRemember:\n\nthat everything from now on needs  + at end of line instead of  %&gt;% \n\n\n\nPlot type\n\nlibrary(tidyverse)\nlibrary(gapminder)\n     \ngapminder %&gt;%\n  ggplot(aes(x = year, y = lifeExp)) +\n  # specify type of plot: scatter with jitter\n  geom_jitter(\n    # colour life expectancy based on values\n    aes(colour = lifeExp),\n    # set \"alpha\" making points same transparency\n    alpha = 0.6,\n    # specify width of \"jitter\"\n    width = 0.5)\n\n\n\n\n\n\n\n\nRemember:\n\ncolour is specified inside the  aes() function because we want it to be dependent on the values in the data\n alpha and  width are specified outside the  aes() function because we want to specify a fixed value for these arguments\n\n\n\nFaceting\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  ggplot(aes(x = year, y = lifeExp)) +\n  geom_jitter(aes(colour = lifeExp),\n              alpha = 0.6,\n              width = 0.5) +\n  # create muti-panle plot\n  facet_wrap(\n    # based on continent grouping\n    ~ continent)\n\n\n\n\n\n\n\n\nUseful for:\n\ncomparing values between different categories in your dataset\n\n\n\nIncluding a second geom, e.g., geom_boxplot()\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  ggplot(aes(x = year, y = lifeExp)) +\n  geom_jitter(aes(colour = lifeExp),\n              alpha = 0.6,\n              width = 0.5) +\n  facet_wrap(~ continent) +\n  # specify type of plot: boxplot\n  geom_boxplot(\n    # group by year (discrete) as currently \"int\" data type (continuous)\n    aes(group = year),\n    # set \"alpha\" so we can see jitter points underneath\n    alpha = 0.3,\n    # remove boxplot outliers as already present in \"jitter\" layer\n    outlier.shape = NA)\n\n\n\n\n\n\n\n\nRemember:\n\nDepending on whether your variable is stored as a “number” or a “character” or a “factor” will determine how ggplot2 deals with it\n\n\n\nStandard themes\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\ngapminder %&gt;%\n  ggplot(aes(x = year, y = lifeExp)) +\n  geom_jitter(aes(colour = lifeExp),\n              alpha = 0.6,\n              width = 0.5) +\n  facet_wrap(~ continent) +\n  geom_boxplot(aes(group = year),\n               alpha = 0.3,\n               outlier.shape = NA) +\n  # choose a standard theme: dark\n  theme_dark()\n\n\n\n\n\n\n\n\nUseful for:\n\nquickly changing how your plot looks by applying one of the built in themes:  theme_bw() ,  theme_classic() ,  theme_dark() etc.",
    "crumbs": [
      "Content",
      "Week 4: Analysing and Presenting Data in R",
      "Topic 3: Roundup and recap"
    ]
  },
  {
    "objectID": "content/week-4/topic-3.html#making-tables",
    "href": "content/week-4/topic-3.html#making-tables",
    "title": "Roundup and recap",
    "section": "Making Tables",
    "text": "Making Tables\nTables are another important tool to have in your data analysis toolbox for communcating. As a general rule, when deciding between tables for graphs/plots:\n\nuse tables when the display will be used to look up individual values or you want to compare individual values \nuse graphs when the display will be used to reveal relatonships among whole sets of values or between variables \n\nTo make tables, we worked with the kable function and kableExtra package in R. \nThe general gt workflow includes 3 key steps: \n\nwrangle your data for the table by either piping %&gt;% the data or creating a new data object (tabledata &lt;- data %&gt;% mutate(…)) \npass the wrangled data to the kable function %&gt;% kable()\nformat the table object for presentation \n\n\nStep 1: Wrangle data for the table\nLets say we are interested in average life expectancy in Africa, Asia, and Oceania in the years 1952, 1972, 1992, and 2002 \n\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(gapminder)\n\ntabledata &lt;- gapminder %&gt;%\ngroup_by(year, continent) %&gt;%\nsummarise(avg_life_exp = mean(lifeExp)) %&gt;% \nfilter(continent %in% c(\"Africa\", \"Asia\", \"Oceania\"),\nyear %in% c(1952, 1972, 1992, 2002))\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n\n\nStep 2: Create the kable table object\n\ntabledata %&gt;% kable()\n\n\n\n\nyear\ncontinent\navg_life_exp\n\n\n\n\n1952\nAfrica\n39.13550\n\n\n1952\nAsia\n46.31439\n\n\n1952\nOceania\n69.25500\n\n\n1972\nAfrica\n47.45094\n\n\n1972\nAsia\n57.31927\n\n\n1972\nOceania\n71.91000\n\n\n1992\nAfrica\n53.62958\n\n\n1992\nAsia\n66.53721\n\n\n1992\nOceania\n76.94500\n\n\n2002\nAfrica\n53.32523\n\n\n2002\nAsia\n69.23388\n\n\n2002\nOceania\n79.74000\n\n\n\n\n\n\n\nStep 3: Format the table for presentation\nHow you decide to format and customise your table visually is a stylistic and personal choice. However, there are a few key principles to keep in mind: \n\nyou should always change the name of your variables to be meaingful to stakeholders who are not familiar with your data - that is, do not keep column names the same as they are called in your dataset \ngenerally, it is a good idea to have a title on each table \nfor reproducibility, it is a good idea to include the data source if you are using an openly available dataset \nbe consistent: use capital or lower case letters in a consistent way\nformat numbers in an interpretable way (generally speaking, you do not need to include more than 2 or 3 decimal places)\n\nBelow is one way you could format the table for presentation: \n\ntabledata %&gt;%\n  kable(col.names = c(\"Year\", \"Continent\", \"Average Life Expectancy\"),\n        align = \"clc\",\n        digits = 2,\n        caption = \"Life Expectancy in Africa, Asia, and Oceania: Data from 1952, 1972, 1992, and 2002\") %&gt;%\n  kable_styling(\"striped\", full_width = FALSE) %&gt;%\n  footnote(general = \"Data from the gapminder dataset\")\n\n\nLife Expectancy in Africa, Asia, and Oceania: Data from 1952, 1972, 1992, and 2002\n\n\nYear\nContinent\nAverage Life Expectancy\n\n\n\n\n1952\nAfrica\n39.14\n\n\n1952\nAsia\n46.31\n\n\n1952\nOceania\n69.25\n\n\n1972\nAfrica\n47.45\n\n\n1972\nAsia\n57.32\n\n\n1972\nOceania\n71.91\n\n\n1992\nAfrica\n53.63\n\n\n1992\nAsia\n66.54\n\n\n1992\nOceania\n76.94\n\n\n2002\nAfrica\n53.33\n\n\n2002\nAsia\n69.23\n\n\n2002\nOceania\n79.74\n\n\n\nNote: \n\n\n\n\n Data from the gapminder dataset",
    "crumbs": [
      "Content",
      "Week 4: Analysing and Presenting Data in R",
      "Topic 3: Roundup and recap"
    ]
  },
  {
    "objectID": "content/week-4/topic-3.html#combined-example",
    "href": "content/week-4/topic-3.html#combined-example",
    "title": "Roundup and recap",
    "section": "Combined Example",
    "text": "Combined Example\nIn the example below you can see a simple R Markdown document without any narrative text added yet, just the YAML then a series of code chunks outlining the various stages in the data analysis process.\n\n\n\n\n\nWhich when knitted to a pdf document looks like this:\nCombined example\nThe next step would be to add in narrative text, creating a comprehensive analysis.",
    "crumbs": [
      "Content",
      "Week 4: Analysing and Presenting Data in R",
      "Topic 3: Roundup and recap"
    ]
  },
  {
    "objectID": "content/week-4/topic-3.html#optional-data-engagement-data-controversies",
    "href": "content/week-4/topic-3.html#optional-data-engagement-data-controversies",
    "title": "Roundup and recap",
    "section": "Optional: Data Engagement & Data Controversies",
    "text": "Optional: Data Engagement & Data Controversies\nCritically review the arguments posed in this article:\nMittelstadt B (2019) Principles alone cannot guarantee ethical AI. Nat Mach Intell 1, 501–507.",
    "crumbs": [
      "Content",
      "Week 4: Analysing and Presenting Data in R",
      "Topic 3: Roundup and recap"
    ]
  }
]